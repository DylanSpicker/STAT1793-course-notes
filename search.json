[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 1793: Course Notes",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-course",
    "href": "index.html#what-is-this-course",
    "title": "STAT 1793: Course Notes",
    "section": "What is this Course?",
    "text": "What is this Course?\nTODO: Write this.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#using-these-notes",
    "href": "index.html#using-these-notes",
    "title": "STAT 1793: Course Notes",
    "section": "Using These Notes",
    "text": "Using These Notes\nTODO: Write this.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#some-mathematical-background",
    "href": "index.html#some-mathematical-background",
    "title": "STAT 1793: Course Notes",
    "section": "Some Mathematical Background",
    "text": "Some Mathematical Background",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "STAT 1793: Course Notes",
    "section": "Additional Resources",
    "text": "Additional Resources",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "notes/chapter1.html#what-is-probability",
    "href": "notes/chapter1.html#what-is-probability",
    "title": "1  Introduction",
    "section": "1.1 What is Probability?",
    "text": "1.1 What is Probability?\nAt its core, statistics is the study of uncertainty. Uncertainty permeates the world around us, and in order to make sense of the world, we need to make sense of uncertainty. The language of uncertainty is probability. Probability is a concept which we all likely have some intuitive sense of. If there was a 90% probability of rain today, you likely considered grabbing an umbrella. You are not likely to wager your life savings on a game that has only a 1% probability of paying out. We have a sense that probability provides a measure of likelihood. Defining probability mathematically is a non-trivial task, and there have been many attempts to formalize it throughout history. While we will spend a good deal of time formalizing notions of probability in this course, we first pause to emphasize the familiarity with probability that you are likely starting with.\nSuppose that two friends, Charles and Sadie, meet for coffee once a week. During their meetings they have wide-ranging, deep, philosophical conversations spanning across many important topics.1 Beyond making progress on some of the most pressing issues of our time, Charles and Sadie each adore probability. As a result, at the end of each of their meetings, they play a game to decide who will pay. The game proceeds by having them flip a coin three times. If two or more heads come up Charles pays, and otherwise Sadie pays.\nWe can think about the strategy that they are using here and feel that this is going to be “fair”. With two or more heads, Charles pays. With two or more tails, Sadie pays. There always has to be either two or more heads or two or more tails, and each is equally likely to come up2. The outcome of their game is uncertain before it begins, but we know that in the long run neither of the friends is going to be disadvantaged relative to the other. We can say that the probability that either of them pays is equal. It’s 50-50. Everything is balanced.\nNow imagine that one day, in the middle of their game, Charles gets a very important phone call 3 and he leaves abruptly after the first coin has been tossed. The first coin toss showed a heads. Sadie, recognizing the gravity of the phone call, pays for the both of them, but she also realizes that Charles was well on the way to having to pay.\n\nExample 1.1 (Basic Probability Enumeration) What is the probability that Sadie would have had to pay in the aforementioned scenario? That is, assuming that the first coin shows a head, what is the probability that at least two heads are shown on the first three coin tosses?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSadie figures that any coin toss is equally likely to show heads or tails. because the first coin showed heads, then there are four possible sequences that could have shown up:\n\n\\(H,H,H\\);\n\\(H,H,T\\);\n\\(H,T,H\\);\n\\(H,T,T\\).\n\nIn three of these situations ((1) \\(H,H,H\\), (2) \\(H,H,T\\), and (3) \\(H,T,H\\)) there are two heads and so Charles would have to pay. In one of them there are two tails, and so Sadie would have to pay. As a result, Charles would have to pay in \\(3\\) of the \\(4\\) (with probability \\(0.75\\)) and Sadie in \\(1\\) of the \\(4\\) (with probability \\(0.25\\)).\n\n\n\n\nLooking at Example 1.1, we can see that Sadie should likely have not paid. Only one out of every four times would she have had to pay, given the first coin being heads. However, we can not be certain that, had all three tosses been observed, Sadie would not have paid. It is possible that we would have observed two tails, making her responsible for the bill. This possibility happens one time out of four, which is more likely than the probability of rolling a four on a six-sided die4. Fours are rolled on six-sided dice quite frequently5, and so it is not all together unreasonable for her to have paid.\nThis seemingly simple concept is the core of probability. Probability serves as a method for quantifying uncertainty. It allows us to make numeric statements regarding the set of outcomes that we can observe, by quantifying the frequency with which we expect to observe those outcomes. Probability does not remove the uncertainty. We still need to flip the coin or roll the die to know what will happen. All probability gives us is a set of tools to quantify this uncertainty. These tools are critical for decision making in the face of the ever-present uncertainty around us.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "notes/chapter1.html#how-to-interpret-probabilities-like-a-frequentist",
    "href": "notes/chapter1.html#how-to-interpret-probabilities-like-a-frequentist",
    "title": "1  Introduction",
    "section": "1.2 How to Interpret Probabilities (like a Frequentist)",
    "text": "1.2 How to Interpret Probabilities (like a Frequentist)\nWe indicated that, intuitively, probability is a measure of the frequency with which a particular outcome occurs. This intuition can be codified exactly with the Frequentist interpretation of probability. According to the Frequentist interpretation (or frequentism, as it is often called), probabilities correspond to the proportion of time any event of interest is6 actually occurs in the long run. For a Frequentist, you imagine yourself performing the experiment you are running over, and over, and over, and over, and over again. Each time you answer “did the event happen?” and you count up those occurrences. As you do this more and more and more, wherever that proportion lands corresponds to the probability.\n\n\n\n\n\nFigure 1.1: This plot simulates the repeated tossing of a coin. The x-axis represents the number of coins being tossed, and the y-axis plots the proportion of times that heads has shown up cumulatively over the tosses thus far. We can see in the long run that this proportion tends towards \\(0.5\\).\n\n\n\n\n\n\n\n\nTo formalize this mathematically, we first define several important terms.\n\nDefinition 1.1 (Experiment) Any action to be performed whose outcome is not (or cannot) be known with certainty, before it is performed.\n\n\nDefinition 1.2 (Event [Informally]) A specified result that may or may not occur when an experiment is performed.\n\nSuppose that an experiment is able to be performed as many times as one likes, limited only by your boredom. If you take \\(k_N\\) to represent the number of times that the event of interest occurs when you perform the experiment \\(N\\) times, then a Frequentist would define the probability of the event as \\[\\text{probability} = \\lim_{N\\to\\infty}\\frac{k_N}{N}.\\] This course does not assume that you have any familiarity with calculus, and yet, this definition relies on limits, a concept taken directly from calculus. However, we will not actually require the ability to work with limits for this course. Instead, when you see a statement of the form \\[\\lim_{x\\to\\infty} f(x),\\] simply think “what is happening to the function \\(f(x)\\) as \\(x\\) grows and grows (off to \\(\\infty\\))?”\nIn practice this means that, in order to interpret probabilities, we think about repeating an experiment many, many times over. As we do that, we observe the proportion of times that any particular outcome occurs, and take that to be the defining relation for probabilities. The reason that we say the probability of flipping a heads is \\(0.5\\) is because if we were to sit around and flip a coin7 over, and over, and over again, then in the long-run we would observe a head8 in \\(0.5\\) of cases.\n\nExample 1.2 (Probability Interpretation) How do we interpret the statement “the probability that Sadie would have had to pay, given a head on the first toss, is \\(0.25\\)”? Recall that in the game, they toss three coins, and Sadie pays if two of them show tails.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis statement means that, if Sadie were to repeatedly be in the situation where one head has shown and there are two coins left to toss, then in \\(0.25\\) of these situations (in the limit, as this is repeated an infinite number of times) will end up showing two tails.\n\n\n\n\nMany situations in the real world are not able to be run over and over again. Think about, for instance, the probability that a particular candidate wins in a particular election. There is uncertainty there, of course, but the election can only be run once. What then? There are several ways through these types of events.\nFirst, we can rely on the power of imagination. There is nothing stopping us from envisioning the hypothetical possibility of running the election over, and over, and over, and over again. If we step outside of reality for a moment, we can ask “if we could play the day of the election many, many, many times, what proportion of those days would end with the candidate being elected?” If we say that the candidate has a 75% chance of being elected, then we mean that in \\(0.75\\) of those imagined worlds, the candidate wins. It is crucial to stress that in our imagination here, we need to be thinking about the exact same day over and over again. We cannot imagine a different path leading to the election, different speeches being given in advance, or different opposition candidates. If we start from the same place, and play it out many times over, what happens in each of those worlds?\nThis repeated imagining is not for everyone. As a result, alternative proposals to the interpretation of probability have been made. Most popularly, the Bayesian interpretation has recently become prominent. To Bayesians, probability is a measure of subjective belief. To say that there is a \\(50\\%\\) chance of a coin coming up is a statement about one’s knowledge of the world. Typically, coins show up heads half the time, so that’s our belief about heads. The Bayesian view, built around subjective confidence in the state of the world, can be formalized mathematically as well. A Bayesian considers the prior evidence that they have about the world9 and combine this with current observations in order to update their subject beliefs, balancing these two sources of information.\n\nRemark (Bayesian Probabilities and Belief Updating). Suppose that a Bayesian is flipping a coin. Before any flips have been made the Bayesian understandably believes that the coin will come up heads \\(50\\%\\) of the time. However, when the coin starts to be flipped, the observations are a string of tails in a row.\nAfter having flipped the coin five times, the individual has observed five tails. Of course, it is totally possible to flip a fair coin five times and see five tails, but there is a level of skepticism growing.\nAfter 10 flips, the Bayesian has still not seen a head. At this point, the subjective belief is that there is likely something unfair about this coin. Even though the experiment started with a baseline assumption that the coin was fair the Bayesian no longer believes that the next flip will be a head.\nAs this goes on, you can imagine the Bayesian continuing to update their view of the world. To them, the probability is an evolving concept, capturing what was believed and what has been observed.\n\nFor the election example, the Bayesian interpretation is somewhat easier to think through. To say that a candidate has a \\(75\\%\\) chance of winning the election means that “based on everything that has been observed, and any prior beliefs about the viability of the candidates, the subjective likelihood that the candidate wins the election is \\(0.75\\)”. If we disagree about prior beliefs, or have experienced different pieces of information, then we may disagree on the probability. That is okay.\nIn this course, we focus on the Frequentist interpretation. This is, in part, because Frequentist probability is an easier introduction to the concepts that are necessary for grasping uncertainty. Additionally, there is some research to suggest that Frequentist interpretations are fairly well understood by the general public 10. However, it is important to know and recognize that there is a world beyond Frequentist Probability and Statistics, one which can be very powerful once it is unlocked. 11\nIf probability measures the long term proportion of times that a particular event occurs, how can we go about computing probabilities? Do we require to perform an experiment over and over again? Fortunately, the answer is no. The tools of probability we will cover allow us to make concrete statements about the probabilities of events without the need for repeated experimental runs. However, before dismissing the idea of repeatedly running an experiment at face value, it is worth considering a tool we have at our disposal that renders this more possible than it has ever been: computers.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "notes/chapter1.html#r-programming-for-probability-and-statistics",
    "href": "notes/chapter1.html#r-programming-for-probability-and-statistics",
    "title": "1  Introduction",
    "section": "1.3 R Programming for Probability and Statistics",
    "text": "1.3 R Programming for Probability and Statistics\nThroughout this course we will make use of a programming language for statistical computing, called R. Classically, introductory statistics courses involved heavy computation of particular quantities by hand. The use of a programming language (like R) frees us from the tedium of these calculations, allowing for a deeper focus on understanding, explanation, decision-making, and complex problem solving. This is not to say we will never do problems by hand12, however, we will emphasize the use of statistical computing often. While you will not be expected to write R programs on your own, you will be expected to read simple scripts, make basic modifications, and to run code that is given to you.\nThroughout these course notes, where relevant, R code will be provided to demonstrate the ideas being discussed. When viewing the notes on the web, you will have the capacity to actually run the code, directly in your browser, and play around with the values that it gives. The code running will be somewhat limited, and so there may be cases where you are better copying the code onto your own computer, but this provides a really excellent method for getting used to working with R code directly. In this section we will cover some of the very basics of using R, and reading R code. If you are interested, there are plenty of resources to becoming a more proficient R programmer. This is a skill that will benefit you not only in this course, but in many courses to come, and far beyond your university training. If you have any programming in your background, R is a fairly simple language to learn; if not, R can be quite beginner friendly.\n\n1.3.1 Basic Introduction to R Programming\nWhen programming, the basic idea is that we are going to write instructions in a script which we will tell our computer to execute. These instructions are13 performed one-by-one, from the top to the bottom of the script. We can have instructions which operate on their own, or which interact with previous (or future) instructions to add to the complexity. The trick with programming then is to determine which actions you need the computer to perform, in which order, to accomplish the task that you are setting out to do.\nTo begin, we may consider a very simple R program, one which uses the programming language as a basic calculator.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHere, we ask the computer to perform some simple arithmetic operations. We use + for addition, - for subtraction, * for multiplication, / for division, and ^ for exponentiation. With the use of parentheses, any expressions relating to these basic operations can be performed. Note that here the result is simply output after it is computed. Try modifying the exact expressions being computed, allowing you to feel comfortable with these types of mathematical operations.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHere, we have two lines of math running with simple operations. Each is simply output when it is computed. These two results have no ability to interact with one another, and if we were to add more and more lines beneath, the same would continue to happen. If we want different commands to be able to interact with one another, we need a method for storing the results. To do so, we can define variables. In R, to define a variable, we use the syntax variable_name &lt;- variable_value. We can choose almost anything that we want for the variable name14 and the variable value can also be of many types.15 The arrow between the two is the assignment operator and it simply tells R to assign the variable_value to be accessible from the variable_name.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this code we assign the variable my_5 to contain the value 5, and the variable my_8 to contain the value 8. We can output these as expressions themselves, simply by typing the variable name. Simply outputting these variables is not of particular interest, however, we can use the variables in later statements by simply including the variable name in them.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHere, instead of simply outputting the variables, we multiply them together. We could have used 5 * 8 in this case for the same result, however, we are afforded a lot more flexibility with this approach. Much of this flexibility comes from our capacity to change the values of variables over time. Consider the following script, and try to understand why the output is the way that it is.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAt the top point in the script, before the first my_5*my_8 call, The variable my_5 has the value 5. However, after this is called, the value is updated to be 10. Then, when we call my_5*my_8 again, this is now simplified to 10 * 8, giving the result. Perhaps more importantly, we can take the value of an expression and assign that to a variable itself.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHere, we define another variable. This time, result now contains the result of multiplying our previous two variables. Thus, when we output it, we get the same value. Take a moment to read through the following script, and try to understand what will happen at the end.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe result is 1600. Why? We can read through this step-by-step in order to understand this. First we set our variables to have the values of 5 and 8. Then, result is made to be the product of our two variables, which in this case is 40. After that, we set the value of my_5 to be the same as the value of result, which gives my_5 equal to 40. At this point, result equals 40, my_5 equals 40, and my_8 is equal to 8. The next line updates my_8 to be the same as the value of my_5, which we just clarified was 40. As a result, all of the variables we have defined now take on the value of 40. The final line before output, result &lt;- my_5 * my_8 updates the value of result to be the product of the two variables again, this time giving 40 * 40 which gives 1600.\n\n\n1.3.2 Function Calls in R\nUp until this point we have simply used numerical operations and variable assignment. While this allows R to serve as a very powerful calculator, we often want computers to do much more than arithmetic. As a result, we need to explore functions in R. A function is a piece of code which takes in various arguments and outputs some value (or values). Most of the way that we will use R in this course is through the use of function calls.16 This is exactly analogous to a mathematical function: it is simply some rule which maps input to output. In fact, some of the most basic functions in R are functions which relate to mathematical functions.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe basic format of a function call will always be function_name(param1, param2, ...). Each of these functions required only a single parameter, however, there are some functions which take more than one parameter. If we have decimal numbers, for instance, we may wish to round them. To do so, we can use the round function in R, which takes two parameters: the number we wish to round, and how many digits we wish to keep.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThere are a few things to note about this sequence of function calls. First, note that we assign the output of a function to a new variable. This behaves exactly as we saw above with simple numeric calculations. Next, consider that the output of the function17 has a value of \\(3.142\\). That is: we rounded the value to \\(3\\) decimal points, exactly as would be expected. The final part to note is that the second parameter passed to the function call is named. That is, instead of simply calling round(unrounded_value, 3), we have round(unrounded_value, digits = 3). If you had made the first call this would have worked perfectly.18 However, R also provides the ability to pass in parameter names alongside the parameter values with the syntax function_name(param_name = param_value, ...). The benefit to doing this is two-fold. First, it is easier to read what is happening, especially for function calls that you have never seen before. Second, it removes the need to have the parameters ordered correctly. It is best practice to always include parameter names where you can.\nNow, you may be wondering “how do you know what the parameter names should be?” The names are built-in to the different functions that you are working with and so overtime you will become quite familiar with them. However, at any point you can also run the command ?function_name, replacing function_name with the name of a function you are interested in, to open documentation about that function. There you will see not only the names of the different parameters, but useful information regarding what the function does, examples of how to call it, and so forth.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen we run this code, we are given a lot of information. We can see the function name, the details, how it’s called, and so forth. In the Arguments section we get a list of all of the arguments we can pass to the function, along with a description of them. In this case we see that round can take a parameter called x for the number to be rounded, and digits (which we have previously seen).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis code produces the exact same output, where now both parameters are named. Specifically, we could read this function call as “round the value of x to have digits decimal points.” If you had instead written round(3, unrounded_value), you would get the value 3, since now we are rounding 3 to 3.141592 decimal points.\n\n\n1.3.3 Moving Beyond Numeric Data\nSo far, everything that we have looked at has been numeric data. We have seen integers, and decimals. You can have negative results, say by taking my_var &lt;- -5. And while numbers are frequently useful, we will require further types of data to write useful computer programs. For this course we will focus on three additional data types: textual data which (referred to as strings), true and false binary data (referred to as booleans), and lists of the same data type (referred to as vectors). They will behave in much the same way as numeric data, with different functions and techniques which can be applied to them.\nTo define a string of text, we simply encapsulate the text that we are interested in within quotation marks (either single ' or double \" quotation marks will work).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTwo commonly used functions which rely on strings are paste and print. Each will take in strings as input, and they do not need to be named. The function paste can take in as many strings as you would like. It will “paste” together all the strings provided, creating a longer string out of these. The function print will display the string that is passed as output. Until now we have been running these programs in a way where all calls are displayed as output: this will not always be the case, and so print can come in handy there.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNote that paste has several additional options which can be investigated in the documentation for paste. This is only the simplest use case for it. In general, strings are particularly helpful when we wish to have output from the computer that will be human readable. Where strings are largely for humans, booleans are largely for computers.\nMuch of what computer programming entails is checking whether certain conditions hold, and then taking different actions depending on what is found. In order to do this, the computer needs a way to represent true and false statements. In R, these are codified with the values TRUE and FALSE. Note, the capital letters here make a difference. You cannot use true or True or any other combination thereof. More important than simply being able to specify the values TRUE and FALSE directly is the ability to detect whether certain statements are TRUE or FALSE. For this we require comparison operators.\nIf we think of mathematical comparisons we can state whether two things are equal, or not equal, and whether one thing is less than (or equal to) or greater than (or equal to) another. We can run all of these same checks in R.\n\nTo check whether two quantities are equal you use quantity_1 == quantity_2. This statement will be TRUE if quantity_1 and quantity_2 are exactly the same, and will be FALSE otherwise.\nTo check whether two quantities are not equal, you can use quantity_1 != quantity_2. This statement will be TRUE if the quantities differ from one another.\nTo check whether one quantity is larger than another, you can use quantity_1 &gt; quantity_2. If you want to know whether it is greater than or equal to, you can use quantity_1 &gt;= quantity_2.\nTo check whether one quantity is smaller than another, you can use quantity_1 &lt; quantity_2. If you want to know whether it is less than or equal to, you can use quantity_1 &lt;= quantity_2.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThere are two key points to note beyond this. First, we will of course not normally compare two constants to one another. We already know that 5==5 and so we would not need to check it. We can, however, plug-in variables, perhaps with unknown values, and have the same types of statements being made. Second, the checks for equality and inequality also work with other data types (like strings).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe final checks may be slightly odd. Here we are comparing across different types of data. When we do this R will automatically try to convert from one type to the other. With strings and numbers this is not too challenging. If they can be converted nicely between types, then they are and the values are compared. Otherwise, R will conclude they are not equal by default. For booleans, it is important to note that TRUE == 1 and FALSE == 0. We will often use these values interchangeably.\nThe final data type that we will consider are vectors. Vectors store many different values, of the same type, in a single object. Thus, we may have a vector of numeric data, or a vector of strings, or a vector of booleans. The vectors will always contain the same type throughout, but they are stored in a single object (and as such, for instance, can be stored in a single variable). To define a vector we call c(...), where the ... contains the set of objects we want to store in the vector. The c stands for concatenate, as we are concatenating together the set of items into a single container.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe see that each of these vectors holds one type of object. Vectors can be of arbitrary and different lengths. It is also possible to combine multiple vectors of the same type into one, by using the c function again.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHere we combine different numeric vectors together. We also show, when forming combined_v3, how numeric vectors can have single items added onto them. That is, if you have a single number, it can be treated as a vector with one element in it. This becomes very useful when building up vectors within code. In addition to combining multiple vectors together, we can also select elements out of a vector. To do this, we use a set of square brackets after the vector’s name, with a number within those square brackets specifying the index of the vector we are interested in. The index is simply the element position starting at 119 and running to the length of the vector. We can include a vector of indices to select multiple elements at once.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNote that, each element is selectable individually, giving a single item of that type (in this case, strings). If you select multiple of the elements together, it will create a vector of that type (in this case, a string vector). Note that in addition to selecting elements in this way by their indices, you can also update the elements in the same way.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this example we are changing the last element of the vector. Sometimes we may not know how long the vector actually is, if for instance, it is being built-up as our code runs. If we ever want to check the length of a vector, we can simply call the function length which takes as input only one vector, and outputs the numeric value of its length.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n1.3.4 Program Control Flow\nWe have seen different types of data, different ways of manipulating data, functions, and variables so far. In order to bring all of these concepts together into useful programs we need some way to control the flow of our programs. We have seen that, by default, programs execute from the top until the bottom. However, it will often be the case that we want want to have certain code running only if certain conditions hold, or that we want to repeat some piece of code many times over. To accomplish these tasks we require control flow statements. We will consider only two types of control flow statements now, which will serve well enough to read most of what needs to be read for this course.\nThe first type of statement is the conditional statement. Conditional statements execute only when certain conditions hold. The simplest conditional statement is an if statement. The format to define an if statement if if (condition){ ... }, where condition is some logical condition to be evaluated. If the condition is TRUE then the code contained in { … } is evaluated. If the condition is FALSE it is not.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this case, these simple conditional statement check to see whether the number we entered is larger than zero and whether the number we entered is smaller than zero, respectively. When run, notice that only one of the statements is executed.20 In this case, we know that only one (or neither) of these statements can be true. When that is the case it may make sense to make use of else clauses in our conditional logic.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHere, if the number is greater than 0, then we run the first block of code, otherwise we run the second block of code. Thus, whenever a positive number is entered, we see “My number is a positive”, and whenever a non-positive number is entered, we see “My number is not a positive.” We can extend else blocks to be else if blocks, where further conditions can be specified.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn these case we can simply pass through each conditional statement in order. First, is the number larger than 50? If so, print the statement, otherwise we check the next condition, is the number greater than 0? Note that if we are checking this condition we know that the number is less than or equal to 50 since it failed the first check. We continue through the rest of this procedure down until the last else block. This block is run only when all of the other conditions fail: that is, our value is not larger than 50, or larger than 0, or smaller than -50, or smaller than 0. The only value that satisfies this is 0 itself.\nSometimes, we wish to check compound conditions. That is, we want to know whether multiple conditions hold, or perhaps, whether at least one of many conditions hold. These statements can be converted into “and” and “or” statements, respectively. To denote “and” statements we use && and to denote “or” statements we use ||. Thus, the check my_val &gt; 0 && my_val &lt; 100 returns true only if my_val is both above 0 and below 100. The check my_val &lt; -50 || my_val &gt; 50 returns true whenever either my_val is less than -50 or my_val is greater than 50.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConditional statements can grow to be very complex, however, with these rules you can read through them top-to-bottom, substituting for “and” and “or” where necessary. It is also possible, where required, to place one conditional statement inside the code block for another, and to combine them with any of the other techniques that we have learned thus far.\nThe final piece of control flow that we will consider for now is the for loop. The idea with a for loop is that we want to repeat the same action either a certain number of times, or for every item in a set of items. To do so, we use the syntax for(x in vector){...}, where the code in ... will be performed once for every single item in the vector. Within the code block specified by ..., the value x will take on the current value in the loop.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNotice that three values are printed, in order, 1 then 2 then 3. The loop code is run three different times, one for each element in the list. Each time the loop is running the next value from the list gets assigned to the variable x. The first time it runs it gets the first element, and so forth. As a result, we can use these values in our calculations in whatever way we need to.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhenever we are trying to form a numeric vector with consecutive elements, as we are in c(1,2,3), we can make this easier on ourselves by simply specifying the upper and lower bounds of the range, separated by a colon. That is c(1,2,3) == 1:3. This is often useful when specify a loop as we very often want to repeat something a set number of times. Note that we do not ever need to use the value of the looping variable. Sometimes, we just want things to repeat, and so the loop is a convenient way to do that.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n1.3.5 Reading Through a More Complex R Program\nTake a moment to read through the following R program and try to understand what is happening exactly. There are comments throughout which will assist in the parsing of the script. We have seen comments up until now, without drawing explicit attention to them. In R, anything placed after a # on the line is considered a comment. The programming language ignores these and so they are only there to help other individuals who may be reading through. It is good practice to comment your code to help others, and also to help yourself whenever you return to it in the future. In the course notes code will typically be commented. If you are reading the PDF version of the notes often these comments will be annotations beside the code (numbering certain lines) with the comments provided below, for the sake of legibility.\nNote, this combines everything that we have learned, and it is entirely understandable if it takes some time to process. Fortunately, you can always try running the script yourself, and playing with different components of it. Remember, if you do not know what a function call does, you can use the documentation21 and try playing with it some yourself. To help with the interpretation here, note that this is an implementation of the game that Charles and Sadie have been playing.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n1.3.6 R Programming for Probability Interpretations\nRecall that the motivation for the discussion of R was the Frequentist interpretation of probability. One task that computers are very effective at is repeatedly performing some action. As a result, we can use computers to mimic the idea of repeatedly performing an experiment. Consider the simple case of flipping a coin over and over again.\nWe can use sample(x, size) as a function to select size realizations from the set contained in x. Thus, if we take sample(x = c(\"H\",\"T\"), size=1) we can view this as flipping a coin one time. If we use the loop structure we talked before, then we can simulate the experience of repeatedly flipping a coin. Consider the following R code. Note, any time that we are doing something which is randomized in R (such as drawing random samples) we also will make use of the set.seed() function. This function takes in an integer value as an argument, and by providing the same integer value we can make sure to always get the same random numbers generated.22 This helps to ensure the repeatability of any R analysis, and it is good practice to do. To see what happens without seeding, try modifying the following code without a seed, and running it several times. Then, set the seed (to any number you like) and do the same process.\nConsider the following code block and this time actually see if you can update the sample sizes, or change the generation process at all. There is no harm in trying: anything you do can be undone simply be reloading the page!\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIt is worth adjusting some of the parameters within the simulation, and seeing what happens. What if you ran the experiment only 5 times? Ten thousand times? What if instead of counting the number of heads, we wanted to count the number of tails? What if we wanted to count the number of times that a six-sided die rolled a 4? All of these settings can be investigated with simple modifications to the provided script.\n\n\nReferences\n\n\n\n\nBartoš, František, Alexandra Sarafoglou, Henrik R. Godmann, Amir Sahrani, David Klein Leunk, Pierre Y. Gui, David Voss, et al. 2023. “Fair Coins Tend to Land on the Same Side They Started: Evidence from 350,757 Flips.” https://arxiv.org/abs/2310.04153.\n\n\nGigerenzer, Gerd, Ralph Hertwig, Eva Van Den Broek, Barbara Fasolo, and Konstantinos V Katsikopoulos. 2005. “‘A 30% Chance of Rain Tomorrow’: How Does the Public Understand Probabilistic Weather Forecasts?” Risk Analysis: An International Journal 25 (3): 623–29.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "notes/chapter1.html#footnotes",
    "href": "notes/chapter1.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "For instance they ask “do we all really see green as the same colour?” or “why is it that ‘q’ comes as early in the alphabet as it does? it deserves to be with ‘UVWXYZ’?”↩︎\nThere has been some recent literature, see Bartoš et al. (2023), which may suggest that a coin is ever so slightly more likely to land on the same side it started on, perhaps undermining this assertion.↩︎\nSomeone has just pointed out the irony in the fact that there is no synonym for synonym. Technically, there are the words metonym and poecilonym and polyonym, but these are rarely used and Charles would wager that there is a very high probability you have never seen them.↩︎\nThis event happens one time out of every six.↩︎\nAgain, about one out of every six rolls↩︎\nBe that flipping a heads, rolling a four, or observing rain on a given day.↩︎\nOur experiment.↩︎\nOur event.↩︎\nAny relevant evidence that has previously been collected.↩︎\nSee, for instance Gigerenzer et al. (2005), where they study how people interpret the statement “there is a \\(30\\%\\) chance of rain tomorrow.” Interestingly, most people can convert this into a frequency statement (\\(3\\) out of \\(10\\), say), even if the specific meaning is sometimes lost. They conclude that there are other issues in attempting to understanding this statement, issues which we will address later on.↩︎\nMore on this in later years, if you so desire! Come have a chat, if that sounds interesting.↩︎\nTo the contrary, some amount of by-hand problem solving helps to solidify these concepts.↩︎\nTypically. There are some exceptions to this, but if this is your first time programming, you need not worry about that!↩︎\nThe variable name must start with a letter and can be a combination of letters, numbers, periods, and underscore.↩︎\nmore on this later↩︎\nNote: when you begin to write programs for yourself, a lot of your time will be spent writing custom functions. If this is of interest to you, I suggest looking into programming more! For this course we will not need to define our own functions, except perhaps ones that will be defined for you.↩︎\nWhich we have stored in the variable rounded_value↩︎\nTry it out to convince yourself!↩︎\nIf you have programmed in the past there is a good chance the language you have learned is “0 indexed” rather than “1 indexed”. In R, all vectors start at position 1 and count up, which is not the case in many languages. Be careful of this.↩︎\nIn fact, if my_number is set to 0 then none of the statements are executed.↩︎\nThe internet is also a wonderful resource, one which even very experienced developers make frequent use of.↩︎\nTechnically, we cannot use a computer to generate random numbers. We can only generate pseudo random numbers, which are close enough for most purposes.↩︎",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "notes/chapter2.html#the-sample-space-and-events",
    "href": "notes/chapter2.html#the-sample-space-and-events",
    "title": "2  The Mathematical Foundations of Statistical Experiments",
    "section": "2.1 The Sample Space and Events",
    "text": "2.1 The Sample Space and Events\nWhile we gave the mathematical formulation for the Frequentist interpretation of probability, we will typically require a more detailed mathematical model to work with probabilities. We want a description, framed in terms of mathematical objects, which will allow us to work out the probabilities of interest. In general, to form such a probability model we need both a list of all possible outcomes that the experiment can produce, as well as the probabilities of these outcomes.\nWe call the list of outcomes that can occur from an experiment the sample space of the experiment. The sample space is denoted \\(\\mathcal{S}\\), and is defined as the set of all possible outcomes from the experiment. For instance, if the experiment is flipping a coin we have \\(\\mathcal{S} = \\{\\text{H}, \\text{T}\\}\\). If the experiment is rolling a six-sided die then \\(\\mathcal{S} = \\{1,2,3,4,5,6\\}\\).\n\nDefinition 2.1 (Sample Space) The sample space of a statistical experiment is the set of all possible outcomes that can be realized from that experiment. The sample space is typically denoted \\(\\mathcal{S}\\), or with similar script letters.\n\n\nExample 2.1 (Enumerating Sample Spaces) Write down the complete sample space, \\(\\mathcal{S}\\) for the game that Sadie and Charles play, based on flipping and observing a coin three times in sequence.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor Sadie and Charles their experiment involves tossing a coin three times in sequence. As a result each outcome is a three-dimensional list of values, given for instance by \\((\\text{H},\\text{H},\\text{H})\\). As a result, we can write down the full sample space as \\[\\mathcal{S} = \\{(\\text{H},\\text{H},\\text{H}), (\\text{H},\\text{H},\\text{T}), (\\text{H},\\text{T},\\text{H}), (\\text{H},\\text{T},\\text{T}), (\\text{T},\\text{H},\\text{H}), (\\text{T},\\text{H},\\text{T}), (\\text{T},\\text{T},\\text{H}), (\\text{T},\\text{T},\\text{T})\\}.\\]\n\n\n\n\nWith the sample space formally defined, we can revisit Definition 1.2, and formally define the concept of an event.\n\nDefinition 2.2 (Event) An event is any collection of outcomes from a sample space for a statistical experiment. Mathematically, an event, \\(E\\), is a subset of \\(\\mathcal{S}\\), and we write \\(E\\subset\\mathcal{S}\\).\n\nTake for instance the experiment of a single coin. In this case, \\(E_1 = \\{\\text{H}\\}\\), \\(E_2 = \\{\\text{T}\\}\\), and \\(E_3 = \\{\\text{H},\\text{T}\\}\\) are examples of possible events. Here, \\(E_1\\) corresponds to the event that a head is observed, \\(E_2\\) corresponds to the event that a tail is observed, and \\(E_3\\) corresponds to the event that either a tails or a heads was observed. Note that for each event we have \\(E_1 \\subset \\mathcal{S}\\), \\(E_2 \\subset \\mathcal{S}\\), and \\(E_3 \\subseteq \\mathcal{S}\\).\n\nExample 2.2 (Basic Event Listing) List several events from the game that Charles and Sadie are playing. Indicate why these are events.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall that an event is any subset of the sample space. In Example 2.1 we define \\(\\mathcal{S}\\) for this game. As a result we can take sets which contain any combinations of these elements. For instance \\(E_1 = \\{(\\text{H},\\text{H},\\text{H})\\}\\), or \\(E_2 = \\{(\\text{H},\\text{H},\\text{T}), (\\text{H},\\text{T},\\text{H})\\}\\), or \\[E_3 = \\{(\\text{H},\\text{H},\\text{H}), (\\text{H},\\text{H},\\text{T}), (\\text{H},\\text{T},\\text{H}), (\\text{H},\\text{T},\\text{T}), (\\text{T},\\text{H},\\text{H}), (\\text{T},\\text{H},\\text{T}), (\\text{T},\\text{T},\\text{H}), (\\text{T},\\text{T},\\text{T})\\}.\\] These are all events since \\(E_1 \\subset \\mathcal{S}\\), \\(E_2 \\subset \\mathcal{S}\\), and \\(E_3 \\subset \\mathcal{S}\\).\n\n\n\n\n\nExample 2.3 (Event Identification) Is “Charles has to pay” an event from the game that Charles and Sadie are playing? Why?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo. “Charles has to pay” is not an event as it is not a subset of the sample space. This could plausibly be seen as a real-world description of a possible event, but it is not itself an event. 1\n\n\n\n\n\nExample 2.4 (Defining Events from Real-World Descriptions) What event corresponds to the description “Charles has to pay” in the game that Charles and Sadie are playing? Recall that they flip a coin three times, and Charles will pay if at least two heads come up, while Sadie will pay if at least two tails come up.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCharles will have to pay whenever there are two or more heads. As a result we can enumerate the possible outcomes that leads to Charles paying. We have\n\n\\((\\text{H},\\text{H},\\text{H})\\);\n\\((\\text{T},\\text{H},\\text{H})\\);\n\\((\\text{H},\\text{T},\\text{H})\\);\n\\((\\text{H},\\text{H},\\text{T})\\).\n\nAny other outcome will have fewer than two heads, and as a result, Charles will not have to pay. Thus, to form an event, we consider the set with each of these outcomes in it. This gives \\[E = \\{(\\text{H},\\text{H},\\text{H}), (\\text{T},\\text{H},\\text{H}), (\\text{H},\\text{T},\\text{H}), (\\text{H},\\text{H},\\text{T})\\}.\\]\n\n\n\n\nWhile the events \\(E_1 = \\{\\text{H}\\}\\) and \\(E_2 = \\{\\text{T}\\}\\) each correspond to a simple outcome from the sample space, \\(E_3 = \\{\\text{H},\\text{T}\\}\\) corresponds to a combined event. We call direct outcomes simple events and more complex outcomes like \\(E_3\\) compound events.\n\nDefinition 2.3 (Simple Event) A simple event is any event which corresponds to exactly one outcome from the sample space. A simple event only has one way of occurring. The size of the set for a simple event will be \\(1\\). The sample space, in turn, is made up of a collection of simple events.\n\n\nDefinition 2.4 (Compound Event) A compound event is any event which corresponds to more than one outcome from the sample space. A compound event can occur in multiple different ways. The size of the set for the compound event will be greater than \\(1\\).\n\nIf we consider rolling a six-sided die, then an example of a simple event is that a four shows up, denoted \\(\\{4\\}\\). A compound event could be that an even number is rolled, \\(\\{2,4,6\\}\\), or that a number greater than or equal to four is rolled, \\(\\{4, 5, 6\\}\\).\n\nExample 2.5 (Identifying Simple and Compound Events) List an example of (at least) one simple and one compound event from the game that Charles and Sadie are playing.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAn example of a simple event would be \\(E_1 = \\{(\\text{H},\\text{H},\\text{H})\\}\\) since it is comprised of exactly one outcome. If three heads are rolled, this event occurs, there is no other way for it to occur. An example of a compound event would be \\[E_2 = \\{(\\text{H},\\text{H},\\text{H}), (\\text{T},\\text{H},\\text{H}), (\\text{H},\\text{T},\\text{H}), (\\text{H},\\text{H},\\text{T})\\}.\\] Here there are four outcomes that correspond to this event, and if any of those outcomes are observed the event occurs.\n\n\n\n\nWe say that an event “occurs” if any of the outcomes comprising the event occur. As a result we can have more than one event occurring as the result of a run of a statistical experimental. Suppose that we are rolling a fair, six-sided die. Consider the events “an even number was rolled” and “a number greater than or equal to four was rolled.” If a four or a six are rolled, both of these events happen simultaneously. Our goal when working with probability will be to assign probability values to different events. We will talk about how likely, or unlikely, events of interest are, given the underlying statistical experiment.\nAbove, we defined \\(E_3\\) to be equal to \\(\\mathcal{S}\\). As a result, we can say that \\(\\mathcal{S}\\) is an event since \\(\\mathcal{S} \\subseteq \\mathcal{S}\\). This is the event that any outcome is observed, which is certain to happen. Since it is certain to happen, we know it happens with probability \\(1\\). There is another “special” event which is important to consider. We call this the null event. Denote \\(\\emptyset\\), the null event is an event that corresponds to “nothing in the sample space”. We know that every time an experiment is run something in the sample space occurs, and so the null event is assigned probability zero.\n\nDefinition 2.5 (Null Event) The null event, denoted \\(\\emptyset\\) or \\(\\{\\}\\), is an event from a statistical experiment which corresponds to nothing within the sample space. The null event has probability zero, and it is impossible to observe. Note that, no matter the sample space, \\(\\emptyset\\subset\\mathcal{S}\\).",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Mathematical Foundations of Statistical Experiments</span>"
    ]
  },
  {
    "objectID": "notes/chapter2.html#set-operations-for-event-manipulation",
    "href": "notes/chapter2.html#set-operations-for-event-manipulation",
    "title": "2  The Mathematical Foundations of Statistical Experiments",
    "section": "2.2 Set Operations for Event Manipulation",
    "text": "2.2 Set Operations for Event Manipulation\nUltimately, we think of all events as being sets. These sets are subsets of the sample space, and can contain single or multiple outcomes. Every quantity that we are interested in can be expressed as some set of outcomes of interest. In building up these sets it is common to construct through the use of “and”, “or”, and “not” statements. That is, we may say that our event occurs if some outcome OR another outcome occurs, or perhaps our outcomes occurs if some outcome does NOT occur.2\nConsider the example of drawing cards from a standard 52-card deck. In such a deck there are 13 card ranks, and four card suits, with one of each combination present. If we draw a single card we can think of the outcomes of the experiment as being any of the 52 possible combinations of rank and suit. We are often interested in an event such as “the card is red”, which is the same as saying “the card is a heart or the card is a diamond.” Perhaps we want to know whether the card was an ace through ten, this is the same as saying “the card is not a Jack or a Queen or a King.” If we are interested in the event that the ace of spades was drawn, this can be expressed by saying that “the card was a spade and the card was an ace.”\nAs you begin to pay attention to the linguistic representation of events that we use, you will notice more and more the use of these words to form compound events in particular. As a result, we give each of them a mathematical operation which allow us to quickly and compactly express these quantities in notation.\n\nExample 2.6 (Description of Events) Describe “Charles has to pay”, based on the game Charles and Sadie are playing, using language revolving around “or”, “and”, and “not” each. That is, describe observing at least two heads, on three flips of a coin, one time using “or”, one time using “and”, and one time using “not”.3\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are plausibly many possible ways of doing so. Consider the following:\n\nOR: Two heads are observed OR three heads are observed.\nAND: Not two tails are observed AND not three tails are observed.\nNOT: Not more than one tails are observed.\n\n\n\n\n\nWe define mathematical operations to encapsulate the use of or, and, and not. These operations apply to any mathematical sets, whether they refer to events or not.\n\nDefinition 2.6 (Union) The union encodes the use of “or” in reference to two or more sets. Formally, with two sets \\(A\\) and \\(B\\), the union of \\(A\\) and \\(B\\) is the set of all elements that are contained in \\(A\\), or \\(B\\), or both \\(A\\) and \\(B\\). We write \\(A \\cup B\\) and read that as \\(A\\) union \\(B\\). When we wish to take the union of many sets, \\(A_1,A_2,\\dots,A_n\\), we write this as \\[\\bigcup_{i=1}^n A_i.\\]\n\n\nDefinition 2.7 (Intersection) The intersection captures the use of “and” in reference to two or more sets. Formally, the intersection of two sets, \\(A\\), and \\(B\\), is the set that contains all elements that belong to both \\(A\\) and \\(B\\). We write \\(A \\cap B\\), and say “\\(A\\) intersect \\(B\\).” When we wish to take the union of many sets, \\(A_1,A_2,\\dots,A_n\\), we write \\[\\bigcap_{i=1}^n A_i.\\]\n\n\nDefinition 2.8 (Complement) The complement makes formal the concept of “not.” The complement of a set to be the set of all elements which occur in the sample space but are not in the given set. We write this as \\(A^C\\) and say “\\(A\\) complement.” When dealing with a sample space, \\(\\mathcal{S}\\), the complement of \\(A\\) is the set of all elements in \\(\\mathcal{S}\\) that are not in \\(A\\).\n\n\nExample 2.7 (Basic Set Operations) For the game being played by Charles and Sadie, take \\(E_1 = \\{(\\text{H}, \\text{H}, \\text{H})\\}\\), \\(E_2 = \\{(\\text{H}, \\text{H}, \\text{H}), (\\text{T}, \\text{H}, \\text{H}), (\\text{H}, \\text{T}, \\text{H})\\}\\), and \\(E_3 = \\{(\\text{H}, \\text{H}, \\text{T})\\}\\). Express the following events.\n\n\\(E_1 \\cup E_2\\);\n\\(E_1 \\cap E_2\\);\n\\(E_2^C\\);\n\\(E_2 \\cap E_3\\);\n\\(E_1 \\cup E_2 \\cup E_3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nDirectly from definitions we can write down each of the following sets:\n\n\\[E_1 \\cup E_2 =  \\{(\\text{H}, \\text{H}, \\text{H}), (\\text{T}, \\text{H}, \\text{H}), (\\text{H}, \\text{T}, \\text{H})\\} = E_2.\\] As a result, the union of \\(E_1\\) and \\(E_2\\) is simply \\(E_2\\).4\n\\[E_1 \\cap E_2 = \\{(\\text{H}, \\text{H}, \\text{H})\\} = E_1.\\] As a result, the intersection of \\(E_1\\) and \\(E_2\\) is simply \\(E_1\\).5\n\\[E_2^C =  \\{(\\text{H},\\text{H},\\text{T}), (\\text{H},\\text{T},\\text{T}), (\\text{T},\\text{H},\\text{T}), (\\text{T},\\text{T},\\text{H}), (\\text{T},\\text{T},\\text{T})\\}.\\]\nFor \\(E_2\\cap E_3\\) note that they share no elements. As a result, the intersection will be empty since there are no elements common to both of them. This gives \\(E_2\\cap E_3 = \\emptyset\\).\n\\[E_1 \\cup E_2 \\cup E_3 = \\{(\\text{H}, \\text{H}, \\text{H}), (\\text{T}, \\text{H}, \\text{H}), (\\text{H}, \\text{T}, \\text{H}), (\\text{H}, \\text{H}, \\text{T})\\}.\\]\n\n\n\n\n\n\nDefinition 2.9 (Disjoint Events) To events, \\(E_1\\) and \\(E_2\\) are said to be disjoint whenever their intersection is the null event. That is, if \\(E_1 \\cap E_2 = \\emptyset\\) then \\(E_1\\) and \\(E_2\\) are disjoint events.\n\nThese concepts allow us to more compactly express sets of interest, and in particular, will be quite useful when it comes to assigning probability. The more times you work with the set operations, the more familiar they will become, and as a result, practice is always useful. Considering rolling a 6-sided die, and take \\(A\\) to be the event that a \\(6\\) is rolled, \\(B\\) to be the event that the roll was at least \\(5\\), \\(C\\) to be the event that the roll was less than \\(4\\), and \\(D\\) to be the event that the roll was odd.\n\nIf we consider \\(D^C\\) this is the event that the roll was even;\n\\(A \\cup C\\) is the event that a \\(6\\) was rolled or that a number less than 44$ was rolled, which is to say anything other than a \\(4\\) or a \\(5\\);\nIf we take \\(A \\cup B\\) then this will be the same as \\(B\\), and \\(A\\cap B\\) will be \\(A\\).\nIf we take the event \\(A\\cap C\\), notice that no outcomes satisfy both conditions, and so \\(A \\cap C = \\emptyset\\).\nWe can also join together multiple operations. \\(D^C \\cap C\\) gives us even numbers than less than \\(4\\), which is to say the outcome \\(2\\).\nSimilarly, \\((A \\cap B)^C\\) would represent the event that a number less than \\(6\\) is rolled.\n\n\nExample 2.8 (Set Operations with Decks of Cards) Charles and Sadie are tiring of flipping their coin, and so they wish to start using decks of cards sometimes instead. Before they formalize a game based on decks of cards, they want to make sure that they are both very comfortable working with these. Suppose that the sample space is defined to be the set of \\(52\\) standard cards that may be drawn on a single draw. Describe how set operations can be used to form events corresponding to:\n\nA red card is observed.\nAny card between an ace and a ten is observed.\nThe ace of spades is observed.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we define several events. Note, these can be defined in shorthand to prevent needing to write out many different cards. we take \\(D\\) to be the event that a diamond is observed, we take \\(H\\) to be the event that a heart is observed, take \\(S\\) to be the event that a spade is observed6 and then take \\(A\\) to be the event that an ace is observed, \\(J\\) to be the event that a Jack is observed, \\(Q\\) to be the event that a Queen is observed, and \\(K\\) to be the event that a King is observed7 then we can use unions, intersections, and complements to express the previously mentioned scenarios.\n\nTo represent outcomes corresponding to “the card is red”, we can use \\(D \\cup H\\).\nTo represent outcomes corresponding to “an ace through ten”, we can use \\((J \\cup Q \\cup K)^C\\).\nTo represent the outcome “the ace of spades”, we may use \\(A \\cap S\\).\n\n\n\n\n\nWorking with these basic set operations should eventually become second nature. There are often very many ways of expressing the same event using these different operations, and finding the most useful method of representing a particular event can often be the key to solving challenging probability questions. The first step in making sure that these tools are available to you is in ensuring that the basic operations are fully understood, and this comes via practice. Remember, unions represent “ors”, intersections represent “ands”, and complements represent “nots”.\n\n2.2.1 Using R To Represent Sample Spaces and Events and Performing Set Operations\nWe have seen how R can encode sets of elements using vectors. For instance, we may take sample_space &lt;- 1:6 to represent the sample space of rolling a six-sided die. We can form events by taking subsets of the relevant quantities, selecting via indices. Fortunately, there are also all of the basic set operations implemented in R. We can use union(x, y) to perform the union of x and y, intersect(x, y) to perform the intersection of x and y, and setdiff(x = sample_space, y) to perform the complement of y (assuming that sample_space contains the full sample space).8\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Mathematical Foundations of Statistical Experiments</span>"
    ]
  },
  {
    "objectID": "notes/chapter2.html#venn-diagrams",
    "href": "notes/chapter2.html#venn-diagrams",
    "title": "2  The Mathematical Foundations of Statistical Experiments",
    "section": "2.3 Venn Diagrams",
    "text": "2.3 Venn Diagrams\nThe sample space is partitioned into outcomes, and the outcomes can be grouped together into events. These events are sets and can be manipulated via basic set operations. Sometimes it is convenient to represent this process graphically through the use of Venn diagrams. In a Venn diagram, the sample space is represented by a rectangle with the possible outcomes placed inside, and events are drawn inside of this as circles containing the relevant outcomes.\n\n\n\nFigure 2.1: A basic Venn diagram, representing the sample space and two different events. In practice, the sample space would have the possible outcomes written into the rectangle, and the circled events would end up containing the relevant outcomes for those events.\n\n\n\n\n\n\nOn the Venn diagram then, the overlap between circles represents their intersection, the combined area of two (or more) circles represents their union, and everything outside of a given circle represents the complement of a set. This can be a fairly useful method for representing sample spaces, and for visualizing the basic set operations that we use to manipulate events inside the sample spaces. A word of caution: Venn diagrams are useful tools, but they are not suitable as proofs themselves. It is possible to convince yourself of false truths if the wrong diagrams are used, and as a result, Venn diagrams should be thought of as aids to understanding, rather than asa rigorous tool in and of themselves.9\n\n\n\nFigure 2.2: Union: The union of events A and B is shaded here in red. The union of two sets is all of the contents of both sets, including the overlap between the two.\n\n\n\n\n\n\n\n\n\nFigure 2.3: Intersection: The intersection of events A and B is shaded here in red. The intersection of two sets is all of the content shared by both sets, given by the overlapping area of the two circles.\n\n\n\n\n\n\n\n\n\nFigure 2.4: Complement: The complement of event A is shaded here in red. The complement of a sets is all of area inside of the sample space, not inside of the set. Here we show the complement of Event A, though Event B would be similar.\n\n\n\n\n\n\n\n\nExample 2.9 (Venn Diagram with Defined Events) Draw a Venn diagram representing the original game that Charles and Sadie played. On the diagram draw the events corresponding to “At least one head and one tail are observed”, and “Sadie won the game”. Recall that three coins are tossed, and Sadie wins if at least two of them show heads.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample space contains the eight possible options. Only \\((\\text{T}, \\text{T}, \\text{T})\\) does not belong to at least one of the events. Both events share \\((\\text{H},\\text{H},\\text{T})\\), \\((\\text{H},\\text{T},\\text{H})\\), and \\((\\text{T},\\text{H},\\text{H})\\).\n\n\n\n\n\nSample spaces, events, and the manipulation of these quantities forms a critical component of understanding probability models. In particular, they describe the complete set of occurrences in a statistical experiment that we could be interested in assigning probability values to. To formalize a probability model, however, we also need some rule for assigning probability values.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Mathematical Foundations of Statistical Experiments</span>"
    ]
  },
  {
    "objectID": "notes/chapter2.html#footnotes",
    "href": "notes/chapter2.html#footnotes",
    "title": "2  The Mathematical Foundations of Statistical Experiments",
    "section": "",
    "text": "As time goes on we will become less strict about this language. When speaking to a statistician, they would understand “Charles has to pay” as an event that can occur based on the defined sample space, by simply transforming it into the language of the sample space. However, the distinction is important to make: events are always subsets of the sample space. Once this is second nature, it is a rule that can be loosened, as the knowledge can always be fallen back on when needed. Simply put: you need to know the rules in order to break them!↩︎\nWhile both or and not language is likely clear from the examples we have seen so far, and language may be slightly less obvious. While we will explore this in more depth shortly, note that you could not have two simple events occurring simultaneously. If \\(E_1\\) and \\(E_2\\) are both simple events, then you can have \\(E_1\\) or \\(E_2\\), and you can have not \\(E_1\\), but you cannot have \\(E_1\\) and \\(E_2\\). This is not true for compound events.↩︎\nIt may be helpful to notice that you can mix and match these terms to your hearts content!↩︎\nNote that whenever we have two events, \\(A\\) and \\(B\\), with \\(A\\subset B\\), then \\(A\\cup B = B\\).↩︎\nNote that whenever we have two events, \\(A\\) and \\(B\\), with \\(A\\subset B\\) then \\(A \\cap B = A\\).↩︎\nNote, these three are compound events with \\(13\\) different outcomes contained within them.↩︎\nCompound events with four different options.↩︎\nR does not implement complements directly, and instead implements the set difference operation. The set difference function, setdiff(x, y) returns the set of all elements in x which are not in y, a sort of subtracting of sets. The complement of a set is defined to be \\(A^C = \\text{setdiff}(\\mathcal{S}, A)\\), indicating why this works!↩︎\nThis is a general principle in mathematics. Coming up with one example that makes something seem true does not form an argument demonstrating that it is true. Venn Diagrams should largely be thought of as specific examples of the underlying phenomena, which are great if you’re a visual learner!↩︎",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Mathematical Foundations of Statistical Experiments</span>"
    ]
  },
  {
    "objectID": "notes/chapter3.html#assigning-probabilities-and-the-equally-likely-outcome-model",
    "href": "notes/chapter3.html#assigning-probabilities-and-the-equally-likely-outcome-model",
    "title": "3  The Core Concepts of Probability",
    "section": "3.1 Assigning Probabilities (and The Equally Likely Outcome Model)",
    "text": "3.1 Assigning Probabilities (and The Equally Likely Outcome Model)\nThere are a plethora of ways to assign probabilities to different events. At the most basic level any rule that maps from the space of possible events to real numbers between \\(0\\) and \\(1\\) can be used as rules for probability assignment. That is, probability assignment is simply a set of rules which says “for this event assign this probability.”\n\nExample 3.1 (Coin Toss Probabilities) Suppose that the fair coin used by Charles and Sadie is tossed one time. Write down the probability assignments relating to this experiment.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn this case we have \\(\\mathcal{S} = \\{\\text{H},\\text{T}\\}\\). Thus, the possible events for which we need to assign probabilities are \\(\\emptyset\\), \\(\\{\\text{H}\\}\\), \\(\\{\\text{T}\\}\\), and \\(\\{\\text{H},\\text{T}\\} = \\mathcal{S}\\). For any probability model we have \\(P(\\emptyset) = 0\\) and \\(P(\\mathcal{S}) = 1\\). When we say that a coin is “fair” we are saying that \\(P(\\text{T}) = P(\\text{H})\\), and since these are the only two possible outcomes in the sample space, we must have that they each have probability \\(0.5\\).\n\n\n\n\nNot every assignment of probability values is going to be valid. Suppose, for instance, that we have a six-sided die, each side labelled with a number from one to six. If I told you that there was a probability of \\(0.5\\) that it comes up \\(1\\), \\(0.5\\) that it comes up \\(2\\), \\(0.5\\) that it comes up \\(3\\), \\(0.5\\) that it comes up \\(4\\), \\(0.5\\) that it comes up \\(5\\), and \\(0.5\\) that it comes up \\(6\\), you would probably call me a liar.1 If, as we have previously seen, probabilities represent the long run proportion of time that a particular event is observed, we cannot have \\(6\\) different outcomes each occurring in half of all cases.\nBeyond the requirements that we impose on what constitutes a “valid” probability rule, we have another concern: scalability. It is perfectly acceptable to indicate that in an experiment with \\(3\\) outcomes, the first has a probability of \\(0.25\\), the second of \\(0.3\\), and the third of \\(0.45\\). What if the experiment has \\(100\\) possible outcomes? Or \\(1000\\)? It quickly becomes apparent that enumerating the probabilities of each event in the sample space is an efficient way of assigning probabilities in practice. A core focus of our study of probability will be finding techniques that allow us to efficiently encode probability information into manageable objects. Once we have done this we will be in a position where we can manipulate these (comparatively) simple mathematical quantities in order to make statements and conclusions about any of the events of interest, even if they have never been explicitly outlined as having an assigned probability.\nWhile we will consider myriad methods for accomplishing these goals throughout our study of probability, we begin with a very useful model which simplifies probability assignment, without any added complexity, and creates a solid foundation for us to explore the properties of probability models. We start by considering equally likely outcomes. As the name suggests, the probability model considering equally likely outcomes assigns an equal probability to every possible outcome of the experiment. This is a probability model that we are already distinctly familiar with: flipping a coin, rolling a die, or drawing a card are all examples of experiments which rely on the equally likely outcomes framework.\n\nRemark (Statisticians and Urn Models). In statistics and probability courses and books you will often have instructors or authors using fairly simple models to illustrate probability concepts. There will often be questions relating to coin tosses, and dice, and decks of cards, and everyone’s favourite: urns. It will very frequently be the case that a statistics question will state that there is an urn with some combination of coloured balls within it, from which you will be selecting some number either with or without replacement. The frequency of these types of examples and questions often feels disconnected from the refrain that “uncertainty is all around us” and that “statistics is relevant to every aspect of our world!”2 Why is it that we seldom see questions or examples that are directly tied to these wide spread applications of the lessons and techniques being taught?\nIn part these simple experiments are cleaner to handle than “real world” situations. We can easily assume that a die is fair and that takes care of any unsuspecting wrinkles that will necessarily come along with the “real world”. This is not dissimilar to working under the assumption of frictionless surfaces in introductory physics, or assuming that human beings are rational in economics. Another key point is that most of us have deep familiarity with dice, and coins, and cards.3 The same is not going to be true of stories that are derived from different use cases in the real world. A final important point, and this will be something we see in depth in the coming chapters, is that from a statistical point of view: there is no difference. Once we have the tools to work with these quantities, we have the tools to work with any of the quantities. This actually distinguishes the use of these types of examples in statistics and probability from those for other subjects: at no point is anything that we are learning incorrect, or overly simple - we are just focusing on the raw probabilistic nature of the phenomenon. As a result, we will continue to see these simple models in these notes. I would encourage you, whenever possible, to hold a topic in mind that matters more to you and start trying to draw the parallels between rolling dice, and whatever it is that you may care about.\nWhy urns, specifically? Well, whether it be coin flipping or dice rolling or card selection, we can model this equivalently using an urn (with \\(2\\), \\(6\\), and \\(52\\) items, respectively). The urn becomes more flexible to exactly dictate what the probability of any selection will be, which is a useful way of moving from equally likely models (each ball is equally likely to be selected) to arbitrary models (we can have however many identical balls in the urn as we would like).\n\nIf we have an experiment with a sample space \\(\\mathcal{S}\\) which has \\(|\\mathcal{S}| = k\\) total elements4, then each element of the sample space occurs with probability \\(\\frac{1}{k}\\). In the case of the coin toss example, \\(\\mathcal{S} = \\{\\text{H}, \\text{T}\\}\\), and so \\(k=2\\) and each outcome occurs with probability \\(\\frac{1}{2}\\). In the case of drawing a card at random, there are \\(52\\) different outcomes, and so \\(k=52\\), and the probability of drawing any particular card is \\(\\frac{1}{52}\\).\nIt is critically important to recognize that the equal probability model assigns equal likelihood to the possible outcomes of an experiment, not the possible events of interest. It will not be the case that all events have the same probability. To make this concrete, consider the events \\(A\\) “the ace of spades is drawn” and \\(B\\) “any spade is drawn”. It is clear that \\(B\\) happens more frequently than \\(A\\), even though we have said that this is an experiment with equally likely outcomes. Remember: an outcome is an observation from a single experimental run, an event is any collection of these possible outcomes.\nA core goal is then bridging the gap between the probability of an outcome5 and the probability of an event. In order to do so, we will next consider the rules of probability, introducing properties that are required for valid probability assignments, and the techniques for manipulating probabilities to calculate the probabilities of quantities of interest.\n\n3.1.1 Using R for the Equally Likely Probability Model\nIn the previous chapter we saw how we can codify sample spaces and events using vectors in R. In the introduction we actually saw how can sample from a sample space using the equally likely outcome framework. Specifically, an application of the sample function will draw a set number of values from a sample space, giving each value an equal probability to be drawn.6\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Core Concepts of Probability</span>"
    ]
  },
  {
    "objectID": "notes/chapter3.html#the-axioms-of-probability",
    "href": "notes/chapter3.html#the-axioms-of-probability",
    "title": "3  The Core Concepts of Probability",
    "section": "3.2 The Axioms of Probability",
    "text": "3.2 The Axioms of Probability\nWe have previously seen that not every probability assignment can be valid. For instance, assigning \\(0.5\\) probability to each outcome on a die leads to a nonsensical scenario. With just a little imagination, we can conjure equally nonsensical scenarios in other ways. For instance, it would make very little sense to discuss the probability of an event being a negative value. What would it mean for an event to occur in a negative proportion of experimental runs? Alternatively, we can consider two events that are nested in one another: say event \\(A\\) is that we draw the ace of spades, and event \\(B\\) is that we draw any spade. Every single time that \\(A\\) happens, we know that \\(B\\) also happens. But there are ways that \\(B\\) can occur where \\(A\\) does not.7 If I told you the probability of \\(A\\) was \\(0.5\\) and the probability of \\(B\\) was \\(0.2\\), this would violate our base instincts. How can it be more likely to draw the ace of spades than it would be to draw any spade at all?8\nOften in mathematics when we have an intuitive set of rules9 that particular quantities must obey, we work to add formality through defining properties of these concepts. To this end, we can define the key properties that probabilities must obey in order to be well-defined, valid probabilities. With three fairly basic properties, we can completely specify what must be true in order for a set of probabilities to be “valid”, and to in turn match with our intuitions.\n\n\n\n\n\n\nThe Axioms of Probability\n\n\n\n\nUnitary: Every valid set of probabilities must assign a probability of \\(1\\) to the full sample space. That is, \\(P(\\mathcal{S}) = 1\\). This is an intuitive requirement as every time the experiment is run we observe an outcome in the sample space. As a result, in every experimental run the event \\(\\mathcal{S}\\) occurs.\nNon-negative: We require that every probability is non-negative. We can have probabilities of \\(0\\), but we can never have a probability less than zero. Again, this is sensible10 but is important to include in our formalization. Specifically, for every event \\(E\\), we must have \\(P(E) \\geq 0\\).\nAdditivity: the final property requires slightly more parsing on first pass. Suppose that we define a sequence of events, \\(E_1, E_2, E_3, \\dots\\) such that no two events have any overlap. That is, \\(E_j \\cap E_\\ell = \\emptyset\\) for all \\(\\ell\\neq j\\). Then, the final property we require for probabilities is that \\[P\\left(\\bigcup_i E_i\\right) = \\sum_i P(E_i).\\] That is, the probability of the union of disjoint events is the summation of the probability of these events.\n\n\n\nIt is worth dwelling slightly on axiom 3. Consider the case of drawing a card at random from a deck of \\(52\\) cards. Using the equally likely outcome model for probability we know that the probability that any card is drawn is given by \\(\\frac{1}{52}\\). If I were to ask “what is the probability you draw that ace of spades?” under this model you can respond, immediately, with \\(\\frac{1}{52}\\). Now, if I were to ask “what is the probability that you draw the ace of spades or the two of spades?” then intuitively you likely figure that this will be \\(\\frac{2}{52}\\). Note that the event \\(E_1\\), “draw the ace of spades” and the event \\(E_2\\) “draw the two of spades”, are disjoint events. Moreover, recall that the union is the “or” and so \\(E_1\\cup E_2\\) is the same as \\(E_1\\) or \\(E_2\\). Taken together then, \\[P(E_1\\cup E_2) = P(E_1) + P(E_2).\\] The axiom of additivity simply extends this intuition to an arbitrary number of events.\n\nExample 3.2 (Basic Additivity) Still unsure of how best to go about using cards to replace their coin game, Charles and Sadie are considering various different events and trying to understand their probabilistic behaviour. They take \\(S\\), \\(C\\), \\(H\\), and \\(D\\) to be the events that a spade, club, heart, or diamond are drawn from a standard deck of cards, respectively. Further, they take \\(C_j\\) to be the event that a card with denomination \\(j\\) is drawn (\\(j\\) ranging from ace with \\(1\\) through King with \\(13\\)). If they consider the union of any two (or more) of these events when can they leverage properties of additivity? When can’t they?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn order to use the properties of additivity it is required that the two events are disjoint. Note that taking any two (or more) of \\(S\\), \\(C\\), \\(H\\), and \\(D\\) will lead to disjoint events. There is no way to draw a card which has two suits on it at once. Similarly, taking any two (or more) of \\(C_j\\) will lead to disjoint events. However, mixing any of the suited events (\\(S\\), \\(C\\), \\(H\\), and \\(D\\)) with any \\(C_j\\) will not be disjoint.\nConsider \\(S\\cap C_1\\). The ace of spades is in \\(S\\) since it is a spade and it is in \\(C_1\\) since it is an ace. As a result, \\(S\\cap C_1 = \\{\\text{Ace of Spades}\\}\\). Because of this we are not able to say that \\(P(A \\cup C_1) = P(A) + P(C_1)\\). However, we can say that \\[P(S\\cup C\\cup H\\cup D) = P(S) + P(C) + P(H) + P(D),\\] and could do the same with any subset of these sets. Similarly, we can take \\[P\\left(\\bigcup_{j=1}^{13} C_j\\right) = \\sum_{j=1}^{13} P(C_j),\\] or any of the subsets there.\n\n\n\n\nThese three axioms fully define valid probabilities. Any mechanism that assigns probability values to events which conforms to these rules will assign valid probabilities. While it may seem counterintuitive that such basic rules fully define our notion of a probability, these rules readily give rise to many other properties that are indispensible when working with probabilities.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Core Concepts of Probability</span>"
    ]
  },
  {
    "objectID": "notes/chapter3.html#secondary-properties-of-probabilities",
    "href": "notes/chapter3.html#secondary-properties-of-probabilities",
    "title": "3  The Core Concepts of Probability",
    "section": "3.3 Secondary Properties of Probabilities",
    "text": "3.3 Secondary Properties of Probabilities\nUsing the previously indicated axioms of probability we are able to derive many useful secondary properties. These properties will frequently be used to actually compute different probabilities, and are helpful to become familiar with. All of the following properties follow directly from the axioms, though, some are more clear than others. For the following we take \\(E\\) and \\(E_1,E_2,E_3,\\dots\\) to be arbitrary events on some well defined sample space.\n\n\\(P(E^C) = 1-P(E)\\), and equivalently, \\(P(E) = 1 - P(E^C)\\).\n\\(P(\\emptyset) = 0\\).\n\\(P(E_1 \\cup E_2) = P(E_1) + P(E_2) - P(E_1\\cap E_2)\\).\n\\[P(E_1 \\cup E_2 \\cup E_3) = P(E_1) + P(E_2) + P(E_3) - P(E_1\\cap E_2) - P(E_1 \\cap E_3) - P(E_2 \\cap E_3) + P(E_1 \\cap E_2 \\cap E_3).\\]\nIf \\(E_1 \\subset E_2\\) then \\(P(E_1) \\leq P(E_2)\\).\n\n\n\n\n\n\n\nProofs of the Secondary Properties of Probability\n\n\n\n\n\nIt may be instructive to see how these properties are derived. Doing so generates added familiarity with manipulating probability expressions and helps to encourage deeper understanding.\n\nNote that, for any event \\(E\\), by definition we have \\(E \\cup E^C = \\mathcal{S}\\) and \\(E \\cap E^C = \\emptyset\\). As a result, we can apply additivity to the sets \\(E\\) and \\(E^C\\) giving \\(P(E \\cup E^C) = P(E) + P(E^C)\\). However, since \\(E\\cup E^C = \\mathcal{S}\\), then we know that \\(P(E \\cup E^C) = P(\\mathcal{S}) = 1\\) by the unitary property. Taken together this tells us that \\(1 = P(E) + P(E^C)\\), and rearranging gives \\(P(E^C) = 1 - P(E)\\), or \\(P(E) = 1 - P(E^C)\\), as required.\nWe know that \\(\\mathcal{S}^C = \\emptyset\\). Using secondary property (1), \\(P(E) = 1 - P(E^C)\\). Taking \\(E = \\emptyset\\) gives \\(P(\\emptyset) = 1 - P(\\mathcal{S}) = 1 - 1 = 0\\), by the unitary property.\nHere note that \\(E_1 \\cup E_2\\) can be written as \\(E_1 \\cup E_2'\\) where \\(E_2' = E_2\\cap E_1^C\\). That is, \\(E_2'\\) contains the outcomes from \\(E_2\\) which were not shared by \\(E_1\\). Then \\(E_1 \\cap E_2' = \\emptyset\\) so we can write \\(P(E_1 \\cup E_2) = P(E_1 \\cup E_2') = P(E_1) + P(E_2')\\), by additivity. Now, if we define \\(E_2^* = E_2\\cap E_1\\) then \\(E_2 = E_2' \\cup E_2^*\\), and \\(E_2'\\cap E_2^* = \\emptyset\\). Thus, \\(P(E_2) = P(E_2'\\cup E_2^*) = P(E_2') + P(E_2^*)\\). Rearranging this gives \\(P(E_2') = P(E_2) - P(E_2^*)\\), and we know that \\(P(E_2^*) = P(E_1 \\cap E_2)\\). Thus, plugging into what we found before we get \\[P(E_1 \\cup E_2) = P(E_1) + P(E_2') = P(E_1) + P(E_2) - P(E_1 \\cap E_2).\\]\nThis follows exactly from the argument for (3). To see, first consider \\(E_2 \\cup E_3\\) to be an event itself, say \\(E_4\\). Then we can apply the above result to \\(E_1 \\cup E_4\\). And then we need only repeat the process for the remaining terms.\nWe can rewrite \\(E_2\\) as \\(E_1 \\cup (E_2 \\cap E_1^C)\\). These two sets are disjoint, since the one is \\(E_1\\) and the other must contain only elements in \\(E_1^C\\). Then, \\(P(E_2) = P(E_1) + P(E_2 \\cap E_1^C)\\) by additivity. Then, through the non-negative property we know that \\(P(E_2 \\cap E_1^C) \\geq 0\\), and so rearranging we have \\(P(E_1) = P(E_2) - P(E_2\\cap E_1^C) \\leq P(E_2)\\).\n\n\n\n\nThese properties are immensely useful when computing probabilities. In fact, these secondary properties will be used with more frequency than the basic axioms when manipulating probabilities in practice. It is worth building comfort with these properties, early and often, as they will assist in manipulating all probability expressions in the future.\nWhile these properties hold in general for all probability models, it is instructive to focus on the equal probability model to begin building familiarity with probability. These properties allow us to take events – whether compound or simple – and combine, rewrite, and manipulate expressions to assist in the handling of the computations. Eventually, however, we require the ability to assign numerical values to these probabilities.\nConsider a simple event, \\(A\\). Recall that a simple event is defined as a possible outcome of an experiment, and so in this case, \\(A\\) corresponds directly to an event that may be observed. If our sample space is \\(k\\) elements large, then \\(P(A) = \\frac{1}{k}\\) in this framework. For instance, if \\(A\\) is the event that a two is rolled on a six-sided fair die, then \\(P(A) = \\frac{1}{6}\\).\nNow, suppose that a compound event is defined, \\(B\\). By definition, a compound event can be expressed as a set of possible outcomes from the experiment. Suppose that we enumerate these possible events as \\(b_1, b_2, \\dots, b_\\ell\\). Then we know that \\(B\\) occurs if any of \\(b_1,b_2,\\dots,b_\\ell\\) occur. Each \\(b_j\\) are elements of the sample space and correspond to possible outcomes of the experiment. As a result, we know that \\(P(b_j) = \\frac{1}{k}\\), based on the equal probability assumption. Now, if we take any two distinct events, say \\(b_i\\) and \\(b_j\\), we know that they must be disjoint: \\(b_i \\cap b_j = \\emptyset\\). This is because in an experiment run only one outcome can occur. Moreover, we can say that \\(B = b_1 \\cup b_2 \\cup\\cdots\\cup b_\\ell\\).\nUsing the axioms of probability outlined above we therefore know that \\(P(B) = \\sum_{j=1}^\\ell P(b_j) = \\sum_{j=1}^\\ell \\frac{1}{k} = \\frac{\\ell}{k}\\). This holds in general for any compound event in this setting. If we take \\(B\\) to be the event that an even number is rolled on a six-sided die, then we would have \\(b_1\\) is the event that a two is rolled, \\(b_2\\) is the event that a four is rolled, and \\(b_3\\) is the event that a six is rolled. There are three such events, and so the probability that an even number is rolled must be \\(\\frac{3}{6} = 0.5\\), which matches our intuition.\nIf we consider what this process is doing at its core, we can reframe the calculation as counting up the number of ways that event can happen and dividing by the total number of events. In our previous discussion, there were \\(\\ell\\) ways of \\(B\\) occurring, a total of \\(k\\) outcomes, and so the probability becomes \\(\\frac{\\ell}{k}\\). In the equal probability model, this will always be the case. The probability of any event \\(A\\) occurring is given by \\[P(A) = \\frac{N_A}{k},\\] where \\(N_A\\) is the number of unique ways that \\(A\\) can occur. In other words, \\(N_A\\) is the size of the set \\(A\\), \\(|A|\\).\nAs a result of this, computing probabilities largely relies on the counting of possible outcomes corresponding to different events. If we can determine \\(N_A = |A|\\), and the count of the total number of occurrences, \\(k\\), then we can determine the probability of \\(A\\). This study of counting is known as combinatorics, and it is where we will turn our attention next.\n\nExample 3.3 (Unmatched Six-Sided Dice) Charles and Sadie, not all together content with the progress through decks of cards, are considering games with dice. Suppose that they have two, fair, six-sided dice. They are interested in the probability that the two dice show different numbers when they are rolled. What is this probability?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere, the key is to realize that the probability is easier to solve when considering the complement rather than the event itself. Notably, rolling two, fair, six-sided dice gives a total of \\(36\\) possible outcomes. Of these, exactly \\(6\\) have equal numbers showing on both dice. Thus, the probability that the two dice show the same number is going to be \\(\\frac{6}{36} = \\frac{1}{6}\\). Then, using the fact that \\(P(E) = 1 - P(E^C)\\), and that taking \\(E\\) to refer to the event where the two dice show different numbers, then \\(E^C\\) refers to the event that the two dice show the same number. As a result, the probability that we want is \\(P(E) = 1 - \\frac{1}{6} = \\frac{5}{6}\\).\n\n\n\n\n\nExample 3.4 (Unmatched Arbitrary Dice) Charles and Sadie, working from their intrigue about dice, have decided that instead of using two six-sided dice, they wish to take two dice of possibly different sizes. Suppose that the first die has \\(d_1\\) sides and the second has \\(d_2\\) sides, and that both dice are otherwise fair. They are interested in the probability that the two dice show different numbers when they are rolled. What is this probability?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis problem is conceptually no different from Example 3.3. There will be a total of \\(d_1\\times d_2\\) possible combinations of the two dice to be rolled.11 Of these, the dice will match in \\(\\min\\{d_1, d_2\\}\\) events.12 Then, with the same complement trick discussed above, we get \\[P(E) = 1 - P(E^C) = 1 - \\frac{\\min\\{d_1,d_2\\}}{d_1\\times d_2}.\\]",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Core Concepts of Probability</span>"
    ]
  },
  {
    "objectID": "notes/chapter3.html#combinatorics",
    "href": "notes/chapter3.html#combinatorics",
    "title": "3  The Core Concepts of Probability",
    "section": "3.4 Combinatorics",
    "text": "3.4 Combinatorics\n\n3.4.1 The Product Rule\nFundamentally, counting is a matter of assessing the size of a collection of items. Sometimes, this is very straightforward. If you want to count the number of students in a classroom, you start at \\(1\\) and enumerate upwards through the integers. To count the number of days until the next Holiday, you do the same thing. If you really need to sleep, perhaps you will count imaginary sleep until you drift off. There is not much to this type of counting, and it is certainly deeply familiar to you all. However, it is also quite limited in its utility.\nImagine that you are interested in determining how many possible ways there are of arranging a deck of \\(52\\) cards. You could of course arrange them in a particular order, then count each of those. That would take a tremendous amount of time, so perhaps instead of using an actual deck you just write down the combinations. Still, each combination is going to be \\(52\\) cards long, and keeping track of that all will be a tremendous challenge. This seems like an approachable question, and yet, it illustrates how complicated (and large) these types of “counting” problems can become very quickly.\nFortunately for us there are some strategies for simplifying these problems down, some of which you are likely already familiar with. Think about trying to form an outfit where you have \\(4\\) different sweaters, \\(3\\) pairs of pants, and \\(2\\) options for your shoes. Suppose that any combination of these will work well. How many total outfits are there? Well, if you have already picked your sweater and pants, then there are going to be \\(2\\) different outfits using these: one with each of the pairs of shoes. This is true for each possible sweater-pant combination, and so we can count \\(2\\) for each one these. In other words, to get the total number of outfits we multiply the number of sweater-pant combinations by the number of shoe options. The same rationale can be applied to count the total number of sweater-pant combinations. For each sweater, there are \\(3\\) pairs of possible pants, and so to get the total number we can take \\(3\\) for each possible sweater, or in other words, \\(3\\times 4\\). Taken together then we have \\(4\\times 3\\times 2 = 24\\) total possible outfits.\nAnother way of framing this is that we have to make three sequential decisions: which of the \\(4\\) sweaters, which of the \\(3\\) pants, and which of the \\(2\\) shoes are to be worn? When we do this we multiply through the number of alternatives at each decision point to get the total number of combinations. This is known as the product rule for counting.\n\nDefinition 3.1 (Product Rule for Counting) The product rule for counting states that, when there are a sequence of \\(k\\) decisions to be made, and for each decision \\(j=1,\\dots,k\\), there are \\(n_j\\) options, then the total number of combinations will be \\[N = n_1\\times n_2\\times\\cdots\\times n_k.\\]\n\n\nExample 3.5 (Counting Coffee Orders) When Charles and Sadie are out for coffee, Sadie enjoys ordering the same thing each time: a black coffee and vegan chocolate chip cookie. Charles, on the other hand, has decided to work through the entire menu of the local coffee shop, each day ordering a drink, with one add-in, and a snack. If there are \\(10\\) different drinks, \\(8\\) possible add-ins, and \\(12\\) different snacks, how many trips to the coffee shop will it take until Charles has tried it all?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis necessitates an application of the product rule for counting. Specifically, we can view this as three sequential decisions, where the first decision is which drink (with \\(n_1 = 10\\)), the second decision is which add-in (with \\(n_2 = 8\\)), and the third decision is which snack (with \\(n_3 = 12\\)). Taking the product gives the total number of combinations as \\(10\\times 8\\times 12 = 960\\). As a result, it will take \\(960\\) visits (assuming that nothing on the menu changes!) to try all combinations.\n\n\n\n\n\nExample 3.6 (Sequence of Dice Rolls) Charles and Sadie have been enjoying playing with dice, but they lost one of the two they had. As a result, they are trying to come up with games revolving around rolling a single die. They decide to try a game called “six is lava”, where they roll a single six-sided die \\(10\\) times in a row. If they get \\(1\\) or more sixes, they lose the game. They are not sure if \\(10\\) is the correct number of rolls to use. What is the probability that they lose on any given set of \\(10\\) rolls of the die in this game?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOnce again this is a scenario where using the complement simplifies the problem. If we asked “what is the probability that no 6’s are rolled, on \\(10\\) rolls of the die” then we can count the number of possibilities through an application of the product rule. In particular, there are going to be \\(5\\) options which are not \\(6\\) at each possible step. We can view this as have \\(n_1 = n_2 = \\cdots = n_{10} = 5\\). Thus, the total number of ways of rolling no sixes is \\(5\\times5\\times5\\times\\cdots\\times5 = 5^{10}\\).\nEssentially the same process can be used to count the total number of possible rolls, replacing \\(5\\) with \\(6\\) to get the denominator. This means that there are \\(6^{10}\\) total sequences of \\(10\\) rolls, and \\(5^{10}\\) which contain no sixes. As a result, taking \\(E\\) to represent the probability that we observe no sixes on \\(10\\) rolls of the die, we would get \\[P(E) = \\frac{5^{10}}{6^{10}}.\\] The question asks for \\(E^C\\) and so we take \\[P(E^C) = 1 - P(E) = 1 - \\left(\\frac{5}{6}\\right)^{10}.\\] This is approximately 0.8385.\nNote that if instead of \\(10\\) flips they had \\(n\\) flips, the probability would be \\(1 - \\left(5/6\\right)^n\\). We can plot this over various values of \\(n\\), to see how the number of flips impacts this probability. The probability of \\(0.5\\) is marked and we can see that taking \\(4\\) tosses gives an ever so slightly greater than \\(0.5\\) probability of losing (0.5177).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Tree Diagrams\nSometimes it is helpful to express counting rules graphically. To do so we rely on tree diagrams.\n\nDefinition 3.2 (Tree Diagram) A tree diagram is a graphical representation for the product rule of counting. Specifically, a tree diagram puts each of the decisions in sequence, and draws a branch for each separate option, starting from the branches drawn at the previous decision step.\n\nTo draw a tree diagram, you start with the first choice, drawing one branch for each of the \\(n_1\\) alternatives, labelling each. Then, at the second choice, you do the same process at the end of each of the branches you drew for choice \\(1\\), this time drawing \\(n_2\\) branches there (so you will have just drawn \\(n_1\\times n_2\\) branches). Then for each of those you draw the \\(n_3\\) further branches, and so on and so forth until the end.\nIf you want to know the total number of choices, you simply count the end points at the very end of the diagram. Each branch corresponds to a single option. To determine which combination of choices it corresponds to, you simply read off the branch labels at each branch you take. If you want to know how many possible combinations come with certain options selected, you can look at only those branches which are downstream from the choices that you care about.\n\n\n\nFigure 3.1: A generic tree diagram. Here the first choice has three different options and the second choice has two. We can see the six total combinations, labelled on the right of the diagram, and can trace the choices required to get there.\n\n\n\n\n\n\nWhile tree diagrams can be quite useful for visualizing a problem, they often grow to be overly complex. As a result, we need to fall back on the numerical representation afforded to us through the product rule for counting. Counting problems, in general, can very quickly become tremendously large and complex. For this reason, we have several tools to assist us in reducing this complexity based on common types of problems that we would like to count.\n\n\n3.4.3 The Factorial\nThe first useful tool for simplifying these problems is the factorial. The factorial of an integer, denoted by \\(x!\\) is given by the product of all integers from \\(x\\) to \\(1\\).13 That is, \\[x! = x(x-1)(x-2)\\cdots(2)(1).\\] If we consider the product rule for counting then note that if \\(n_1=1\\), \\(n_2=2\\), … , \\(n_k = k\\), then the total number of options is \\(k\\times(k-1)\\times\\cdots\\times 1 = k!\\). The most common reason that this comes up is when we want to order a collection of items. Suppose that you have \\(10\\) books that you want to place on a shelf. You can view this as making \\(10\\) sequential decisions: what book goes first, second, third, and so on. There are \\(10\\) options for the first book, then \\(9\\) for the second (any except for the first one), and then \\(8\\) for the third (any except for the first \\(2\\)). This continues down to the last book, and so we conclude that there are \\(10\\times9\\times8\\times\\cdots\\times1 = 10!\\) ways of arranging these books.\n\nExample 3.7 (Seating in a (Full) Coffee Shop) One day Charles and Sadie walk into the coffee shop and find that it is completely full. There are ten seats and ten people sitting in them. They are disappointed that they do not have room to sit themselves, however, they are never ones to pass up an interesting probability question.\n\nHow many different ways could these ten people have sat in these ten seats?\nIf there are ten drinks that have been made, and one is to be passed out to each seat, how many different ways can these ten people sit in these ten seats, with each of these ten drinks?\nAlongside the ten drinks, there are ten snacks to be served up as well. How many different ways can these ten people sit in these ten seats, with each of these ten drinks, and each of these ten snacks?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nHere, we can think about lining up the ten seats in a row, each number \\(1\\) through \\(10\\). Then, we want to place one patron into each of the seats. This is no different from ordering \\(10\\) books, and so the total is \\(10!\\) which is \\(3,628,800\\).\nNote that we still have to make the seat choices from part (a), so there are \\(10!\\) ways of getting the \\(10\\) people sat in the \\(10\\) chairs. Once there, we can think of handing out the drinks to each of the numbered combinations of person-chair. This is no different from passing out the people as well, giving \\(10!\\) ways of doing this. We use the product rule to combine these two choices, with \\(10!\\times 10!\\) total combinations of people-chair-drink. This is \\(13,168,189,440,000\\).\nExtending the same logic before, there are \\(10!\\times 10!\\) ways of getting each person sat in a chair with a drink. Then, there will be an additional \\(10!\\) ways of passing out the snacks to these people. Taken together this gives \\(10!\\times 10!\\times 10!\\) which is \\(47,784,725,839,872,000,000\\).14\n\n\n\n\n\n\nRemark (0!). Depending on how factorials are thought of, some trouble can come up around a quantity like \\(0!\\). On one hand, if we view factorials as multiplying each number between \\(n\\) and \\(1\\) together then \\(0! = 0\\times 1\\) and we get \\(0! = 0\\). On the other hand, if we view factorials as counting the number of ways which we can order a set of \\(n\\) items, then \\(0!\\) is the number of ways we can order \\(0\\) items, which is \\(1\\).15 So, which is it?\nWe take \\(0!\\) to be equal to \\(1\\). The ordering argument is perhaps the most convincing. However, if you are algebraically minded you may wonder how we get around the tricky issue of using our algebraic definition. The key insight is to not define \\(n!\\) as the product of the numbers from \\(n\\) to \\(1\\), but rather, to define \\[(n-1)! = \\frac{n!}{n},\\] and specify that \\(1! = 1\\). Then in this case we get all of the usual requirements for how we have discussed factorials, but we also get that \\(0! = \\frac{(0+1)!}{(0+1)} = 1\\). As a result, we will take \\(0! = 1\\).16\n\n\n\n3.4.4 Permutations and Combinations\nSometimes, we want to order items from a collection, but we want to only take a subset of these times. That is, suppose that you have \\(20\\) books, only \\(9\\) of them will fit on the self, and you want to know “how many ways can you put \\(10\\) books on the shelf, in order, from your collection of \\(20\\)?” Using the product rule of counting for this directly, we recognize that there are \\(20\\) options for the first, then \\(19\\), then \\(18\\), and so on until there are \\(12\\) choices for the \\(10\\)th book to place. We can write this out in a seemingly strange way. \\[\\begin{align*}\n  &\\ \\frac{(20)(19)(18)(17)(16)(15)(14)(13)(12)(11)(10)(9)(8)(7)(6)(5)(4)(3)(2)(1)}{(11)(10)(9)(8)(7)(6)(5)(4)(3)(2)(1)} \\\\\n  &= \\frac{(20)(19)(18)(17)(16)(15)(14)(13)(12)\\cancel{(11)}\\cancel{(10)}\\cancel{(9)}\\cancel{(8)}\\cancel{(7)}\\cancel{(6)}\\cancel{(5)}\\cancel{(4)}\\cancel{(3)}\\cancel{(2)}\\cancel{(1)}}{\\cancel{(11)}\\cancel{(10)}\\cancel{(9)}\\cancel{(8)}\\cancel{(7)}\\cancel{(6)}\\cancel{(5)}\\cancel{(4)}\\cancel{(3)}\\cancel{(2)}\\cancel{(1)}}\n\\end{align*}\\]\nThis expression is \\(20!\\) divided by \\(11!\\), and gives the same as our argument from the product rule for counting directly. This is a more general result than our example with books would suggest. If we have \\(n\\) items, and we want to choose \\(k\\) of them taking into account the order those choices, it will always be \\(n!\\) divided by \\((n-k)!\\). We call this a permutation.\n\nDefinition 3.3 (Permutations) If we wish to select \\(k\\) items from a collection of \\(n\\) items, where the ordering of these selections matters, then the total number is referred to as a permutation. Mathematically, \\[P_{n,k} = \\frac{n!}{(n-k)!}.\\]\n\nPermutations arise when we select ordered subsets from a collection. We often, in combinatorial problems, talk about ordering, though sometimes what we mean by this is slightly more abstract. Suppose that you want to form a committee with \\(5\\) different people, each of which occupies a different role: the president, vice president, treasurer, note taker, and critic. If there are \\(30\\) people to select for this then there are \\(P_{30,5}\\) total possible committees that can be formed. While there is not a sequential order here, we talk about this as being “ordered” since we can differentiate between the five roles. Instead of labelling them with their names, we could label them \\(1\\) through \\(5\\) and make the ordering more explicit.\n\nExample 3.8 (Seating in a (Not Full) Coffee Shop) Still haunted by that time when the coffee shop was full, Sadie and Charles enter the coffee shop at a later date and find that, including themselves, there are only \\(7\\) patrons in the store, and still the \\(10\\) seats to choose from.\n\nHow many different ways can the \\(7\\) people sit in the \\(10\\) different chairs?\nIf there are \\(10\\) drinks on the menu, how many different ways can each person choose a chair and a drink?\nIf the coffee shop can make only one of each drink, how does the previous total change?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nIn this case we are looking to order subsets from a total collection. If we line up the \\(7\\) patrons we then need to select \\(7\\) chairs to go with them, and keep these ordered. This is simply \\[P_{10,7} = \\frac{10!}{3!} = 604800.\\]\nIn this part of the question we know that the \\(7\\) people have \\(P_{10,7}\\) ways of sitting into seats, we can view this as decision one. Then, for each of these \\(7\\) people there is a decision of what drink they will order. For these \\(7\\) decisions, \\(n_2 = \\cdots = n_8 = 10\\). As a result, we get the total number is \\[P_{10,7}\\times10\\times10\\times\\cdots\\times10 = P_{10,7}\\times 10^{7} = 6,048,000,000,000.\\]\nThis setting, like part (b) starts with a first decision involving \\(P_{10,7}\\) choices. Then, instead of there being \\(7\\) more decisions with \\(10\\) choices each, we can either view this as \\(7\\) more choices with a descending number of options \\(10\\) for the first, then \\(9\\), and so on, or we can view this as a single choice where we need to select \\(7\\) ordered options from the \\(10\\) drinks available. This gives \\[P_{10,7} \\times P_{10,7} = 365,783,040,000,\\] total choices.\n\n\n\n\n\nFactorials compute the number of orderings for a set of objects, and permutations compute the number of ordered subsets from a collection of objects. What about when we do not wish to differentiate the order of subsets? Suppose that you still need to form a \\(5\\) person committee, but you do not have explicit roles for the different members of the committee. Here we cannot use a permutation directly, as we know that this takes into account the order.\nTo determine the number of unordered subsets, we will consider a different approach for taking ordered subsets. Suppose that we formulate the ordered committee as a two step procedure. First, we select \\(5\\) people without concern for their order. Then we choose which order they will have. If \\(M\\) represents the number of unordered sets of \\(5\\) from this population, the product rule for counting tells us that the total number of ordered committees will be \\(M\\times 5!\\), since there are \\(5!\\) arrangements of the \\(5\\) people. Thus, we can write this down as \\[P_{30,5} = \\frac{30!}{25!} = M\\times 5! \\implies M = \\frac{30!}{25!5!}.\\]\nThis will be true far more broadly than our committee example. If we want to select \\(k\\) items from a collection of \\(n\\), we will have \\(n!\\) divided by the product of \\(k!\\) and \\((n-k)!\\). We refer to these as combinations.\n\nDefinition 3.4 (Combinations) If we wish to select \\(k\\) items from a collection of \\(n\\) items, where the ordering of these selections does not matter, then the total number is referred to as a combination. Mathematically, \\[\\binom{n}{k} = \\frac{n!}{k!(n-k)!}.\\]\n\nWe read \\(\\binom{n}{k}\\) as “\\(n\\) choose \\(k\\)”, which translates to “select \\(k\\) items from a population of \\(n\\) total options, without concern for their order.”\nTo summarize: factorials allow us to order a complete collection, permutations allow us to select a subset with consideration of the ordering, and combinations allow us to select a subset from the collection without regard to the order. These three techniques can be used in combination with the product rule for counting to allow us to have very complex total summations.\n\nExample 3.9 (Changing the Seating in the Coffee Shop) Some nights, the coffee shop hosts local music acts. Because of the added equipment, the coffee shop owners only keep out the number of seats that are going to be required based on the number of tickets that were sold.\n\nIf there are \\(8\\) tickets sold, how many different combinations of the \\(10\\) chairs can get left out?\nSuppose that only \\(6\\) people end up showing up. How many different ways can the \\(6\\) people sit in the \\(8\\) chairs that are being selected from the \\(10\\) total possibilities?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nHere, there is no ordering for the \\(8\\) chairs that are to be selected. As a result, we are simply looking for how many chairs can be selected from a group of \\(10\\) of them. This is \\[\\binom{10}{8} = \\frac{10!}{8!\\cdot 2!} = 45.\\]\nWith the first choice having \\(45\\) possible options, the second choice that needs to be made is how \\(6\\) people sit into \\(8\\) chairs. Here the ordering does matter, since the chairs are distinguishable. As a result, this is a permutations question, with there being a total of \\(P_{8,6} = \\frac{8!}{2!}\\) ways of having the people select their seats. In total then there are \\[\\binom{10}{8}\\times P_{8,6} = \\frac{10!}{8!\\cdot 2!}\\cdot\\frac{8!}{2!} = 907,200.\\]\n\n\n\n\n\n\n\n3.4.5 Less Common Counting Techniques\nWhile most of the problems we address will revolve around permutations and combinations (with heavy use of the product rule), there are additional techniques which are important to know (and recognize when to use). In particular, combinations and permutations each assume that we are sampling from our set without replacement. That is, each time you select an item, it is removed from the population. These are the most common situations in these combinatorial problems, however, there are some situations which arise where we need to count the number of ordered or unordered subsets with replacement.\n\n3.4.5.1 Ordered Subsets with Replacement\nConsider, for instance, forming a password using only lowercase numbers and letters. If you decide on a fixed length for the password, then there are going to be \\(36\\) choices at each decision point, and you want to take an ordered subset of these. This is forming an ordered subset with replacement, and to count how many different ways there are of doing this, we can simply use the product rule. That is, you have \\(36\\) choices at each decision point, and so there are \\(36\\times 36\\times\\cdots\\times 36 = 36^k\\) total decisions, where \\(k\\) is the number of items to select.\nIn general, if you have \\(n\\) total items and you want to make an ordered set of \\(k\\) of these items with replacement you will have \\(n^k\\) total ways of doing this.\n\n\n3.4.5.2 Unordered Subsets with Replacement\nForming unordered sets with replacement is slightly less intuitive. Consider rolling \\(k\\) dice which are not distinguishable from one another. We know that there are \\(6\\) total sides that can show up on each of these dice, but how many different combinations of numbers can show up overall? If the dice can be distinguished we would say that there are \\(6^k\\) possible ways of doing this. However, some of these combinations are going to be equivalent in the unordered world. Take the simple case of \\(k=2\\). Here we have the following possibilities: \\[\\begin{align*}\n(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6)&\\\\\n(2, 2), (2, 3), (2, 4), (2, 5), (2, 6)&\\\\\n(3, 3), (3, 4), (3, 5), (3, 6)&\\\\\n(4, 4), (4, 5), (4, 6)&\\\\\n(5, 5), (5, 6)&\\\\\n(6,6)&\n\\end{align*}\\]\nThis gives a total of \\(21\\) possible combinations, rather than \\(36\\).\nIn general, if we want to find the way of selecting \\(k\\) elements with replacement from a total of \\(n\\), then the number of ways of doing this will be \\[\\binom{n+k-1}{k}.\\] In our example this gives \\(\\binom{6+2-1}{2} = 21\\).\n\n\n3.4.5.3 Permutations with Identical Objects\nFinally, it is worth understanding how to handle identical objects in combinatorial problems. Suppose that, of the \\(10\\) books that we wish to place on a shelf, we have \\(2\\) copies of one of them, \\(3\\) copies of another one, and the other \\(5\\) have one copy each. Supposing that there is no way to tell these identical objects apart, how many ways can we arrange the bookshelf?\nFirst, if we pretend that all of the items are actually able to be differentiated then there are \\(10!\\) ways of placing these books. Now, in any of these permutations, had we swapped the order of the first book (with \\(2\\) copies) the ordering would have been indistinguishable. As a result, for every ordering of the \\(8\\) other books, we counted that permutation twice (when it should have only been counted once!). So to address the two repeated copies we need to take \\(10!/2\\). Now, a similar argument is going to hold for the book with \\(3\\) repeated copies. However, instead of there being \\(2\\) permutations which are identical, there are going to be \\(3! = 6\\) permutations which are identical. This is because we can reorder the \\(3\\) copies of the book in anyway we choose, and still wind up with the same overall permutation.17 As a result, the total number is going to be \\[\\frac{10!}{2!\\cdot3!} = 302400.\\]\nWe can see this same result through an alternative construction. First, we select which of the \\(10\\) slots should have the first book. We do not care about the order, and so there are \\(\\binom{10}{2}\\) ways of doing this. Next, we can select which of the \\(8\\) remaining slots should have the second book. Like the first one there will be \\(\\binom{8}{3}\\) ways of placing these. Now, there are \\(5\\) slots remaining, and \\(5\\) books to place, so as a result, we can order those in \\(5!\\) different ways, and then slot them into the remaining places in order. This gives, in total \\[\\binom{10}{2}\\binom{8}{3}(5!) = \\frac{10!}{2!\\cancel{8!}}\\cdot\\frac{\\cancel{8!}}{3!\\cancel{5!}}\\cdot\\cancel{5!} = \\frac{10!}{2!3!}.\\]\nTo generalize this, if we want to order \\(n\\) elements, such that there are \\(k\\) distinguishable elements with \\(n_1\\) of the first type, \\(n_2\\) of the second, and so forth until \\(n_k\\) of the last type (\\(n = n_1 + n_2 + \\cdots + n_k\\)), then the total number of orderings will be \\[\\frac{n!}{n_1!\\cdot n_2!\\cdots n_k!}.\\]",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Core Concepts of Probability</span>"
    ]
  },
  {
    "objectID": "notes/chapter3.html#from-combinatorics-to-probability",
    "href": "notes/chapter3.html#from-combinatorics-to-probability",
    "title": "3  The Core Concepts of Probability",
    "section": "3.5 From Combinatorics to Probability",
    "text": "3.5 From Combinatorics to Probability\nWhile combinatorics is a field of study on its own, with many intriguing tools and developments surrounding the enumeration of objects, for the purposes of simple probability models these tools will suffice. Ultimately, we care about counting since in the equal probability model, the probability of any event can be determined by counting the number of ways that the event can occur and dividing by the total number of outcomes that are possible. That is, we use these tools to derive \\(N_A\\), the total number of ways that \\(A\\) can occur, and \\(N\\), the total number of experimental outcomes, and then we conclude that \\[P(A) = \\frac{N_A}{N}.\\]\n\nExample 3.10 (Poker Hand Counts) During one of their conversations, Charles and Sadie were remarking how they never really played poker. As they understand it, in poker you are dealt a hand of \\(5\\) cards and you want to use these \\(5\\) cards to try to match certain sets of cards, some of which are more rare than others. Charles and Sadie start to get hung-up on discussions regarding “straights” and “flushes”.\nA straight is any sequence of \\(5\\) cards in ascending order (where aces can be low, or high). For instance, \\(7, 8, 9, 10, \\text{J}\\) of any suit. A flush, is any set of \\(5\\) cards belonging to the same suit. Charles just feels that straights have to be more rare than flushes.\n\nHow many different straights are there from a standard deck of cards?\nHow many different flushes are there from a standard deck of cards?\nIf dealt \\(5\\) cards at random, what is the probability of a flush? What is the probability of a straight?\nA straight flush occurs when you have \\(5\\) cards in order, of the same suit. What is the probability of a straight flush?\nIf straight flushes were not counted as flushes, and not counted as straights, how do the probabilities of either hand change?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nA straight necessitates drawing five cards in order, with each of any suit. Just as with the straight flush, there are \\(10\\) possible starting values for the straight. Once we have selected the starting value, then for each of the five cards we can pick any of the four suits, resulting in \\(4\\) choices each. That gives \\[N_A = 10\\times 4\\times 4\\times 4\\times 4\\times 4 = 10\\times 4^5 = 10240.\\]\nA flush necessitates drawing any five cards from the same suit. If we had a suit fixed, there would be \\({13\\choose 5}\\) ways of doing this, since we do not care about ordering. If we think about first choosing the suit, we have \\(4\\) ways of doing that, resulting in \\[N_A = 4 \\times {13 \\choose 5} = 5148.\\]\nTo find the probabilities of each of these, we need to know the total number of \\(5\\) card hands. We do not consider order, and so \\(N = \\binom{52}{5} = 2598960\\). Then, the probability is simply the number of combinations (calculated above) divided by the number of hands. This gives, for straights, \\[P(A) = \\frac{10240}{2598960} = \\frac{128}{32487} \\approx 0.00394,\\] and for flushes, \\[P(A) = \\frac{5148}{2598960} = \\frac{33}{16660} \\approx 0.00198.\\] As a result, we see that straights are roughly twice as common as flushes are.18\nNote that to form a straight flush, we first have to fix a suit. There are \\({4\\choose 1}=4\\) total ways of doing this. Next, we need to pick which starting value we will use. Once a card has been selected as a starting value, the remaining cards are fixed. The start value ranges from A through to \\(10\\). Correspondingly, we have \\[N_A = {4\\choose 1}{10 \\choose 1} = 4\\times10 = 40.\\] As a result, we get that \\[P(A) = \\frac{40}{2598960} = \\frac{1}{64974}.\\]\nFrom (d) exactly \\(40\\) of the straights and \\(40\\) of the flushes are also straight flushes. We can thus remove \\(40\\) from the totals of each of these, giving \\(\\frac{10200}{2598960}\\) for straights and \\(\\frac{5108}{2598960}\\) for flushes.\n\n\n\n\n\n\nReferences\n\n\n\n\nKahneman, Daniel, and Amos Tversky. 1972. “Subjective Probability: A Judgment of Representativeness.” Cognitive Psychology 3 (3): 430–54.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Core Concepts of Probability</span>"
    ]
  },
  {
    "objectID": "notes/chapter3.html#footnotes",
    "href": "notes/chapter3.html#footnotes",
    "title": "3  The Core Concepts of Probability",
    "section": "",
    "text": "Or else conclude that I was mistaken and maybe should not be teaching probability.↩︎\nOne of the most famous quotes from a statistician was a thought shared by John Tukey, stating “The best thing about being a statistician is that you get to play in everyone’s backyard.” This is a common refrain, and one rooted in truth. Statistics is everywhere, across every field of human inquiry, and can help us make sense of everything from the trivial to the deeply important.↩︎\nThis does not help to explain why we use urns so much, of course. When was the last time any of us drew a ball from an urn?↩︎\nNote that, when we have a set, using the absolute value symbols \\(|\\cdot|\\) stands for the cardinality of the set. Cardinality is just a fancy way of saying the size or the number of elements that the set has in it.↩︎\nA quantity which in the equally likely outcome framework, we know exactly.↩︎\nThe sample function can also be used without equally likely events by specifying a vector of probabilities, however, this is a less common use case.↩︎\nFor instance, the Queen of spades being drawn.↩︎\nThis is actually a scenario where our instincts may lead us awry in some situations. Consider the following from Kahneman and Tversky (1972): Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? (a) Linda is a bank teller, or (b) Linda is a bank teller and is active in the feminist movement. A majority of respondents rate (b) as being more probable, even though (a) is contained in (b).↩︎\nThese rules, which we call “properties” are formally known as “axioms”.↩︎\nWhat would it mean to have a negative probability? It is perhaps a more interesting question than it seems at first glance. It is a topic that has come up in some pretty strange places and, while it is not presently sensible to call them “probabilities” in a traditional sense, there are interesting results which follow.↩︎\nNote: if this is not yet clear to you, that’s okay! In the very next section we begin to discuss how to count the possible combinations in these types of scenarios.↩︎\nSuppose that \\(d_1 = 2\\) and \\(d_2 = 4\\). Then here, the dice can match when they show either \\(1\\) or \\(2\\), but if the second die shows \\(3\\) or \\(4\\) there is no possibility of having a match at all.↩︎\nFactorials are exciting because they always look like they are shouting!↩︎\nIt is somewhat interesting to note how large these values get relatively quickly. This is a comparatively small question: only \\(10\\) people with \\(10\\) drinks and \\(10\\) snacks. If you consider any counting problem with a larger number of items, these problems quickly grow to be intensely complex. For instance, a count of my main home book collection reveals \\(376\\) of them. To order these would give \\(376!\\) possible orderings. In decimal representation this is \\[4.992244775852435618292576458782762114148884082811840265632\\dots\\times10^{806}.\\] This is an \\(807\\) digit long number. This is an incomprehensibly large number. This number is \\(8×10^{726}\\) times larger than the number of atoms in the universe. That is, if every atom in the universe were given some arrangements of books to hold onto, they would need to hold \\(8×10^{726}\\) of them in order for all of the arrangements to be held. I point this out because combinatorics explodes in this way. Even simple problems grow out of hand very, very quickly. This is where comfort with the algebraic tools is required, rather than a reliance on intuition. There is simply no way to have intuition regarding the scope of these numbers, at least, not without a lot of practice.↩︎\nImagine I am placing books on my shelf. With \\(3\\) books there are \\(3! = 6\\) ways my shelf can look at the end. With \\(2\\) books there are \\(2! = 2\\) ways my shelf can look at the end. With \\(1\\) book there are \\(1! = 1\\) ways my shelf can look at the end. With \\(0\\) books there are \\(0! = 1\\) ways my shelf can look at the end.↩︎\nNote, this does not help us with the factorials of negative numbers, nor of fractional numbers. Factorials can be extended to these in sensible ways, but these are not for combinatorial purposes and are no longer “factorials” exactly.↩︎\nIn fact, the reason that there are \\(2\\) ways of doing this with the book with \\(2\\) copies is since \\(2! = 2\\).↩︎\nThis is still a deeply unintuitive result to me. To me it feels like it should be harder to get a nice ordering of cards all in a row than it is to find ones of the same suit. And yet, it is about twice as likely to have the straights. Now, if you think about this deeply, it makes a lot of sense: there is a lot more leeway in selecting the straight than the flush. However, my brain refuses to accept this as intuitive. This is something that can often occur in probability questions where the true results can be far from what we would expect. An interesting question is how long does a straight have to be to be less likely than a flush of the same length. Note that the number of ways of choosing a flush will always be \\(4\\times\\binom{13}{k}\\), and the number of ways of forming a straight will always be \\(10\\times 4^k\\). If we consider \\(k\\) from \\(1\\) to \\(13\\), we can take the ratios of these to see just how much more (or less) likely a straight is. Doing this gives: k=1 gives 0.769. k=2 gives 0.513. k=3 gives 0.559. k=4 gives 0.895. k=5 gives 1.989. k=6 gives 5.967. k=7 gives 23.869. k=8 gives 127.304. k=9 gives 916.587. k=10 gives 9165.874. k=11 gives 134432.821. k=12 gives 3226387.692. k=13 gives 167772160. Up to (and including) \\(4\\) cards the straight is indeed harder to achieve. However, this does not last long!↩︎",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Core Concepts of Probability</span>"
    ]
  },
  {
    "objectID": "notes/chapter4.html#marginal-and-joint-probabilities",
    "href": "notes/chapter4.html#marginal-and-joint-probabilities",
    "title": "4  Probabilities with More than One Event",
    "section": "4.1 Marginal and Joint Probabilities",
    "text": "4.1 Marginal and Joint Probabilities\nUp until this point we have primarily focused on assigning probabilities to particular events. If we have some event of interest, \\(A\\), then \\(P(A)\\) is the probability that \\(A\\) occurs in any manner. If we are using our equally likely probability model, then \\(P(A) = \\frac{N_A}{N}\\). This is the probability of the event \\(A\\) where nothing else is known at all. If we smooth over anything which could alter the likelihood, if we have no additional information, if we want the best guess for the likelihood of occurrence in a vacuum, this is the probability of interest. We refer to such quantities as marginal probabilities.\n\nDefinition 4.1 (Marginal Probability) The marginal probability of a single event, \\(A\\), is the probability that the event happens without considering any information. This is simply the probability that the event happens, denoted \\(P(A)\\).\n\nIt is useful to specifically call these marginal probabilities to differentiate them from probabilities which depend on two or more events. Specifically, we can think about taking the intersection of two events, say \\(A \\cap B\\). In words this is the event that \\(A\\) occurs and \\(B\\) occurs. Until this point we have thought about solving for probabilities related to intersections as a two-step procedure: first, find an event \\(C = A \\cap B\\), and then second solve for the marginal probability of \\(C\\). While this is a useful technique for calculation, sometimes it is more useful to think of the probability of the intersection as the probability of \\(A\\) and \\(B\\) occurring simultaneously. We call this the joint probability of \\(A\\) and \\(B\\).\n\nDefinition 4.2 (Joint Probability) The joint probability of two events, \\(A\\) and \\(B\\), is given by the probability of their intersection. That is, \\(P(A\\cap B)\\). This is sometimes denoted \\(P(A,B)\\) and corresponds to the probability that both \\(A\\) and \\(B\\) occur. The joint probability of more than two events extends in the same way. Suppose there exists a sequence of events, \\(A_1, \\dots, A_n\\), then the joint probability of these events is \\[P(A_1,A_2,\\dots,A_n) = P\\left(\\bigcap_{i=1}^n A_i\\right).\\]",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilities with More than One Event</span>"
    ]
  },
  {
    "objectID": "notes/chapter4.html#what-are-conditional-probabilities",
    "href": "notes/chapter4.html#what-are-conditional-probabilities",
    "title": "4  Probabilities with More than One Event",
    "section": "4.2 What are Conditional Probabilities?",
    "text": "4.2 What are Conditional Probabilities?\nMarginal probabilities are often of interest, and frequently are the best tool for summarizing the overall state of the world, or our knowledge regarding the state of the world. Joint probabilities can be useful when working with compound events, or thinking of complex outcomes that we wish to consider. However, there are many scenarios which are not covered by either the joint or marginal probabilities we have seen. In practice we know that sometimes information that we have will change our understanding of the probability of an event. Suppose the event \\(A\\) corresponds to the event that it snows tomorrow, in some particular city. It is possible to think about how often it snows on average, and report a value related to that as \\(P(A)\\). Now, what if we know that it is currently the middle of summer? In this case, while \\(P(A)\\) does not shrink to \\(0\\), it becomes far less likely than if we did not have that information. Similarly, if we know that it is winter, the likelihood that it snows tomorrow increases. In order to formally capture this we can introduce the idea of conditional probability.\n\nDefinition 4.3 (Conditional Probability) The conditional probability of an event \\(A\\) given an event \\(B\\), is the probability that \\(A\\) occurs assuming that we know that \\(B\\) occurs. The conditional probability takes into account additional information codified through the occurrence of an additional event. We write this quantity as \\(P(A|B)\\), and will read this as “the probability of \\(A\\) given \\(B\\).”\n\nUnlike in the case of marginal probabilities, conditional probabilities allow us to condition on extra pieces of information. Instead of asking “what is the probability of this event”, we instead ask, “given that we know this piece of information, what is the probability of this event?” Joint probabilities ask “what is the probability that both of these events occur simultaneously” which is distinct in that we do not have any additional information about the state of the world when working with joint probabilities. The subtle distinction becomes quite powerful, both in terms of manipulating and working with probabilities, but also in terms of expressing the correct events of interest for ourselves.\nTo make use of conditional probabilities, we will think of the process of conditioning on one or more events. We will talk of the probability of \\(A\\) conditional on \\(B\\), where \\(A\\) and \\(B\\) are two events of interest. Intuitively, this it the probability of \\(A\\) happening, supposing that we know that \\(B\\) has already happened.\n\nExample 4.1 (Six from the Sum) Charles and Sadie are playing a new game using two dice. In the game they each take turns rolling the two dice into a container that the other player cannot see. They add up the dice and then report this sum to the other player. The other player then has to guess whether or not there is a six showing.\n\nOn one round, Charles does not hear the sum that Sadie reports as he was not paying attention. Sadie is strict and insists that she will not repeat herself. What should Charles guess?\nDetermine a strategy which optimizes the likelihood that the guessing player will be correct.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nIn this case we want the marginal probability of a six showing up on the roll of two dice. Take \\(E\\) to represent the event wherein at least one six is showing on the roll of two dice. Thus, \\(E^C\\) is the event that no sixes are showing. There are \\(5\\times 5=25\\) (using the product rule for counting) ways of not rolling a \\(6\\), meaning that $\\(P(E^C) = \\frac{25}{36} \\implies P(E) = 1 - \\frac{25}{36} = \\frac{11}{36}\\). Thus, Charles should guess that there is no \\(6\\) as the probability is only \\(11/36 \\approx 0.31\\).\nHere we wish to determine conditional probabilities. We take \\(E\\) to be the event that at least one \\(6\\) is showing, and then \\(S\\) to be a variable representing the sum of the two dice. For \\(s= 1, \\dots, 12\\) we wish to find \\(P(E|\\{S=s\\})\\). Notice that for \\(s=2,\\dots,6\\) \\(P(E|S=s) = 0\\). If the sum is \\(2\\) we know that there could not have been a \\(6\\). Moreover, for \\(s=11,12\\) we know that \\(P(E|S=s) = 1\\) since the only way to form \\(11\\) is a five and a six, and the only way to form \\(12\\) is to have two sixes. This leaves \\(s = 7,\\dots,10\\) to check. The following table gives the set of values, the possible combinations to reach the value, and then the combinations that end up involving a \\(6\\).\n\n\n\nValue\nCombinations\nInvolving 6\n\n\n\n\n7\n(1,6) (2,5) (3, 4) (4, 3) (5, 2) (6, 1)\n(1, 6) (6, 1)\n\n\n8\n(2,6) (3,5) (4, 4) (5, 3) (6, 2)\n(2, 6) (6, 2)\n\n\n9\n(3,6) (4,5) (5, 4) (6, 3)\n(2, 6) (6, 2)\n\n\n10\n(4, 6) (5, 5) (6, 4)\n(4, 6) (6, 4)\n\n\n\nReferencing from this table we can read off the following probabilities \\[\\begin{align*}\nP(E|S=7) &= \\frac{2}{6} = \\frac{1}{3}\\\\\nP(E|S=8) &= \\frac{2}{5} \\\\\nP(E|S=9) &= \\frac{2}{4} \\\\\nP(E|S=10) &= \\frac{2}{3}.\n\\end{align*}\\] As a result, the best strategy is to guess “yes” when a \\(10\\), \\(11\\), or \\(12\\) is rolled and to be indifferent to the guess when a \\(9\\) is rolled. Otherwise, guess “no.”\n\n\n\n\n\nRecall that \\(A\\) and \\(B\\), as events, are merely subsets of the sample space, \\(\\mathcal{S}\\). Each item in either \\(A\\) or \\(B\\) is one of the possible outcomes from the experiment or process that we are observing. Suppose that we know that \\(B\\) has occurred. What this means is that, one of the outcomes in the set \\(B\\) was the observed outcome from the experiment. Now, if we want to know \\(P(A|B)\\), we want to know the probability, working from the assumption that \\(B\\) has happened, that \\(A\\) also happens. That is, knowing that \\(B\\) has happened, what is the probability that \\(A\\) and \\(B\\) both happen.\nThe event that \\(A\\) and \\(B\\) both happen is denoted by the intersection, \\(A \\cap B\\). This corresponds to the set of events inside the set \\(B\\) which also belong to the set \\(A\\). Now, instead of considering the joint probability directly, we need to acknowledge that for \\(A|B\\), only the events in \\(B\\) were possible. That is, instead of being divided by the whole space, we can only divide by the space of \\(B\\). In some sense, we can view conditioning on \\(B\\) as treating \\(B\\) as though it is the full sample space, and finding probabilities within that. In general, \\(B\\) will be smaller than \\(\\mathcal{S}\\), and so \\(P(B) &lt; 1\\). Instead of the conditional probability being “out of” \\(1\\), it will instead be “out of” \\(P(B)\\), which gives \\(P(A|B) = \\frac{P(A\\cap B)}{P(B)}\\).\n\n\n\n\n\n\nComputing Conditional Probabilities\n\n\n\nFor an event \\(A\\), and an event \\(B\\) with probability \\(P(B) &gt; 0\\), the conditional probability of \\(A\\) given \\(B\\) is \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}.\\]\n\n\nTo make this more clear, let’s consider a simple example. Suppose that we take \\(A\\) to be the event that a \\(2\\) is rolled on a fair, six-sided die, and \\(B\\) to be the event that an even number was rolled. This is an equal probability model, and so each outcome gets \\(\\frac{1}{6}\\) probability. The original sample space is \\(\\mathcal{S} = \\{1,2,3,4,5,6\\}\\), the event \\(A\\) is \\(\\{2\\}\\), and the event \\(B\\) is \\(\\{2,4,6\\}\\). In order for both \\(A\\) and \\(B\\) to occur, we note that we need \\(A \\cap B = \\{2\\}\\). If we know that \\(B\\) has occurred, then we know that either a \\(2\\), \\(4\\), or \\(6\\) has been rolled, with equal probability for each. Thus, intuitively, we can view \\(B\\) as the new sample space, and say that rolling a \\(2\\) has a \\(\\frac{1}{3}\\) probability, given that there are \\(3\\) outcomes and \\(1\\) of them is the event of interest. Another way to consider this is to note that \\(P(B) = \\frac{1}{2}\\), and so we need to scale each event by \\(\\frac{1}{1/2}\\) in order to make sure that the total probability of our reduced sample space equals \\(1\\). Then \\(P(A\\cap B) = \\frac{1}{6}\\), so \\[P(A|B) = \\frac{1/6}{1/2} = \\frac{1}{3}.\\]\nSuppose that, instead of a fair die, it was weighted so that \\(6\\) comes up more frequently than the other options. Consider the probability of observing a six to be \\(0.5\\), with the other five values each coming up with probability \\(0.10\\). If \\(A\\) and \\(B\\) are the same events as above, then \\(P(B) = 0.7\\). If we know that \\(B\\) has occurred then the new sample space is \\(\\{2,4,6\\}\\) where \\(P(2) = \\frac{0.1}{0.7} = \\frac{1}{7}\\), \\(P(4) = \\frac{0.1}{0.7}\\), and \\(P(6) = \\frac{0.5}{0.7} = \\frac{5}{7}\\). Note that these three probabilities sum to \\(1\\) still, which constitutes a valid probability model, and so \\(P(A|B) = \\frac{1}{7}\\).\n\nExample 4.2 (Six from the Sum - Revisited) Sadie sees the solution worked out for the best strategy in the dice game above, but is having trouble understanding it in the context of conditional probability more broadly. For the sums \\(7, 8, 9\\), and \\(10\\), describe the process of limiting the sample space to get the correct conditional probability, and use the formula to show that the probabilities worked out in Example 4.1 are correct.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe relevant table of solutions is copied from above.\n\n\n\nValue\nCombinations\nInvolving 6\n\n\n\n\n7\n(1,6) (2,5) (3, 4) (4, 3) (5, 2) (6, 1)\n(1, 6) (6, 1)\n\n\n8\n(2,6) (3,5) (4, 4) (5, 3) (6, 2)\n(2, 6) (6, 2)\n\n\n9\n(3,6) (4,5) (5, 4) (6, 3)\n(2, 6) (6, 2)\n\n\n10\n(4, 6) (5, 5) (6, 4)\n(4, 6) (6, 4)\n\n\n\nHere, given a sum, we know that one of the events listed under “combinations” has occurred. As a result, once we have conditioned on what the sum is, we are able to treat the “combinations” column as the full sample space of possible outcomes. Each of these in this case is equally likely, and so we can divide the number which contain a \\(6\\), by the total number in the reduced sample space.\nTo do this algebraically, we first note that \\[\\begin{align*}\n    P(S=7) &= \\frac{6}{36} \\\\\n    P(S=8) &= \\frac{5}{36} \\\\\n    P(S=9) &= \\frac{4}{36} \\\\\n    P(S=10) &= \\frac{3}{36}.\n\\end{align*}\\] Now, suppose we consider the event \\(E\\cap\\{S=7\\}\\). This is the set \\(\\{(1,6),(6,1)\\}\\) and so \\(P(E\\cap\\{S=7\\}) = 2/36\\). In fact, the same holds true for all of the joint events. As a result, we get \\[\\begin{align*}\n    P(E|S=7) &= \\frac{\\frac{2}{36}}{\\frac{6}{36}} = \\frac{2}{6} \\\\\n    P(E|S=8) &= \\frac{\\frac{2}{36}}{\\frac{5}{36}} = \\frac{2}{5} \\\\\n    P(E|S=9) &= \\frac{\\frac{2}{36}}{\\frac{4}{36}} = \\frac{2}{4} \\\\\n    P(E|S=10) &= \\frac{\\frac{2}{36}}{\\frac{3}{36}} = \\frac{2}{3}.\n\\end{align*}\\] This corresponds exactly to the values we found before.\n\n\n\n\nSometimes, we wish to condition on more than one event. To do so, the same process extends naturally. For instance, suppose we want to know the probability of \\(A\\) given \\(B\\) and \\(C\\). This would be written \\[P(A|B,C) = \\frac{P(A\\cap B\\cap C)}{P(B \\cap C)} = \\frac{P(A,B,C)}{P(B,C)}.\\] Moving beyond two events occurs in the expected way.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilities with More than One Event</span>"
    ]
  },
  {
    "objectID": "notes/chapter4.html#using-conditional-probabilities",
    "href": "notes/chapter4.html#using-conditional-probabilities",
    "title": "4  Probabilities with More than One Event",
    "section": "4.3 Using Conditional Probabilities",
    "text": "4.3 Using Conditional Probabilities\nConditional probability is a mechanism for capturing our knowledge of the world, and using that to update our sense of the uncertainties at play. For instance, suppose that we are interested in drawing a random card from a deck of \\(52\\), and we want to know the probability that it is a heart. Without any additional knowledge, the probability of this event is \\(\\frac{1}{4}\\). Now, suppose that you know that it is a red card. In this case, we now know that it is either a heart or a diamond, and there are equal numbers of each, meaning that the new probability is \\(0.5\\). We can work this out directly \\[P(\\text{Heart}|\\text{Red}) = \\frac{P(\\text{Heart},\\text{Red})}{P(\\text{Red})} = \\frac{P(\\text{Heart})}{1/2} = \\frac{1/4}{1/2} = 0.5.\\] Suppose instead that we had been told that the card was an ace. Here we now know that there are four possible outcomes that correspond to an ace, and only one of these is a heart, meaning the probability is \\(\\frac{1}{4}\\). In this case, \\(P(A|B) = P(A)\\), and our beliefs did not update.\nWhat if instead we had considered the second event to be “the card was a spade.” In this case if we want to know \\(P(A|B)\\) then, given a spade being drawn, we know that the probability of drawing a heart is \\(0\\).\n\nExample 4.3 (Charles’s Mismatched Urn Mishap) Charles’s love of probability prompts a spontaneous decision: buy an urn and some coloured balls to fill it up with. Unfortunately, the supplier of the balls misunderstood the request and sent over an assortment of different shapes rather than just spheres. There are some spheres, some cubes, some pyramids, and some cones. There are also five different colours present, red, blue, green, yellow, and black. Charles is slightly dismayed as, when reaching into the urn, it is very easy to feel what shape you are pulling out before you see the object, and the distribution of colour-shape combinations is not even. Despite the dismay, Charles shows Sadie, who is deeply excited, explaining how this mismatched urn is perfect for understanding conditional probabilities deeply. The distribution of shapes and colours is presented in the following table.\n\n\n\nColour\nSphere\nCube\nPyramid\nCone\n\n\n\n\nRed\n2\n3\n2\n0\n\n\nBlue\n1\n0\n0\n6\n\n\nGreen\n2\n2\n1\n2\n\n\nYellow\n0\n4\n2\n1\n\n\nBlack\n3\n1\n1\n2\n\n\n\n\nWhat is the probability of drawing each colour, if items are selected at random, without knowledge of the shape?\nAssuming that the shape is known, what is the probability of selecting each colour?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNote that there are \\(2+3+2=7\\) red, \\(1+6=7\\) blue, \\(2+2+1+2=7\\) green, \\(4+2+1=7\\) yellow, and \\(3+1+1+2=7\\) black objects. As a result, each colour is going to be equally likely, with a probability of \\(0.2\\).\nTake \\(S\\), \\(C\\), \\(P\\), \\(Cn\\) to be the events that a sphere, cube, pyramid, or cone are drawn, respectively. Take \\(R\\), \\(B\\), \\(G\\), \\(Y\\), \\(Bk\\) to be the events that a red, blue, green, yellow, or black object is drawn, respectively. We want the probability of each colour, given the corresponding shape. There are a total of \\(20\\) conditional probabilities to solve for here. We walk through, in full, the first conditional probability calculation. Afterwards, the same process follows to get the (provided) answer.\nFirst note that there are \\(8\\) spheres, \\(10\\) cubes, \\(6\\) pyramids, and \\(11\\) cones. Thus, the marginal probabilities are \\(P(S) = 8/35\\), \\(P(C) = 10/35\\), \\(P(P) = 6/35\\), and \\(P(Cn) = 11/35\\). Note further that the joint probability between any colour-shape combination is going to be given by the number of that combination that exist, divided by \\(35\\). Thus, suppose we want \\(P(R|S)\\). There are \\(2\\) red spheres, so \\(P(R \\cap S) = 2/35\\). The marginal probability \\(P(S) = 8/35\\), so taken together this gives \\[P(R|S) = \\frac{2/35}{8/35} = \\frac{2}{8} = \\frac{1}{4}.\\]\nApplying the same process gives the following probabilities, reported in the table for convenience.\n\n\n\n\n\n\n\n\n\n\nColour\nSphere\nCube\nPyramid\nCone\n\n\n\n\nRed\n\\(\\frac{2}{8} = \\frac{1}{4}\\)\n\\(\\frac{3}{10}\\)\n\\(\\frac{1}{3}\\)\n\\(0\\)\n\n\nBlue\n\\(\\frac{1}{8}\\)\n\\(0\\)\n\\(0\\)\n\\(\\frac{6}{11}\\)\n\n\nGreen\n\\(\\frac{2}{8} = \\frac{1}{4}\\)\n\\(\\frac{1}{5}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{2}{11}\\)\n\n\nYellow\n\\(0\\)\n\\(\\frac{2}{5}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{11}\\)\n\n\nBlack\n\\(\\frac{3}{8}\\)\n\\(\\frac{1}{10}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{2}{11}\\)\n\n\n\n\n\n\n\n\n\n4.3.1 The Multiplication Rule\nWhile sometimes we will want to work out the conditional probability using our knowledge of the joint and marginal probabilities, there are other times where it is easier to determine the conditional probability directly. In these settings we may wish to understand the marginal or joint probabilities. That is, we may know \\(P(A|B)\\), but we want to make statements regarding \\(P(A)\\) or \\(P(A,B)\\).\nTo do so, we can simply rearrange the defining relationship of conditional probability, to solve for the quantities of interest. Because of the importance of this procedure, we actually give this mostly straightforward rearrangement a special name.\n\nDefinition 4.4 (Multiplication Rule) The multiplication rule states that, for two events \\(A\\) and \\(B\\) where \\(P(B) &gt; 0\\), \\[P(A\\cap B) = P(A|B)P(B).\\]\n\nNote that, by multiplying both sides of the definition of \\(P(A|B)\\) by \\(P(B)\\) gives the result. In words it states that we can solve for the joint probability of \\(A\\) and \\(B\\) by multiplying the conditional probability of \\(A\\) given \\(B\\), by the marginal probability of \\(B\\). This is symmetric in \\(A\\) and \\(B\\) so that \\[P(A\\cap B) = P(B|A)P(A).\\] This is useful as sometimes it is easier to determine \\(B\\) given \\(A\\).\n\nExample 4.4 (Package Delivery Times) To thank Sadie for the help in seeing the silver lining with the urn mishap, Charles decides to order a small gift online, sending it direct to Sadie. Unfortunately, the website does not list which delivery company each package is sent out with. After some sleuthing, Charles determines that there are two different companies that it may have been sent with. Looking at online reviews it appears as though company \\(A\\) is late \\(75\\%\\) of the time while company \\(B\\) is late \\(15\\%\\) of the time.\nIf the store that Charles ordered from sends out \\(10\\%\\) of packages with company \\(A\\), and the rest with company \\(B\\), which is more likely: that the package is late and was sent with \\(A\\), or that the package was late and sent with \\(B\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe will take \\(L\\) to correspond to the events where the package is late, \\(A\\) to the events where the package was sent with \\(A\\) and \\(B\\) to be the events where the package was sent with \\(B\\). We wish to compare \\(P(L, A)\\) and \\(P(L, B)\\). We know that \\(P(A) = 0.1\\) and \\(P(B) = 0.9\\), and we know that \\(P(L|A) = 0.75\\) and \\(P(L|B) = 0.15\\). As a result, we can use the multiplication rule to find the joint probabilities. First, \\(P(L, A) = P(L|A)P(A) = (0.75)(0.1) = 0.075\\). Additionally, \\(P(L, B) = P(L|B)P(B) = (0.15)(0.9) = 0.135\\). As a result it is more likely to have the package late and sent with \\(B\\) then the package late and sent with \\(A\\).1\n\n\n\n\n\n\n4.3.2 Partitions and the Law of Total Probability\nWhile the multiplication rule gives us the capacity to solve for joint probabilities, often we wish to make statements regarding marginal probabilities. Fortunately, we can extend this process outlined in the multiplication rule to solve directly for marginal probabilities as well. To do so, we first introduce the concept of a partition.\n\nDefinition 4.5 (Partition) A partition is a collection of sets which divide up the sample space such that all of the sets are disjoint from one another, and the sample space is given by the union of all of the sets. That is, \\(A_1,\\dots,A_n\\) is a partition of \\(\\mathcal{S}\\) if:\n\n\\(A_i \\cap A_j = \\emptyset\\) for all \\(i \\neq j\\), and\n\\[\\bigcup_{i=1}^n A_i = \\mathcal{S}.\\]\n\n\nFor instance, if the sample space were all the positive integers, we could partition this space into all the even numbers as one set and all the odd numbers as a second. We could also partition this into the set of numbers which are less than \\(10\\), the set of numbers that are greater than \\(10\\), and then \\(10\\). In both examples we have sets whose union forms the full sample space with no overlap. Note that we could not partition the set into multiples of \\(2\\) and multiples of \\(3\\), since (i) not all values are contained between these two sets,2 and (ii) there is overlap between these two sets.3\n\nExample 4.5 (Partitions of the Coin Game) Charles and Sadie are thinking back with fondness to their original coin flipping game, where they would toss a coin three times in a row. If two or more heads showed up, Charles would pay. Otherwise, Sadie would.\nTo help them reminisce, Write down three different partitions of the sample space, each with a different number of partitioning sets. Describe your partitions using words.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are many possible partitions to write down. The following are examples.\n\nWe can partition the space into games where Charles pays and games where Sadie pays. This gives \\[\\begin{align*}\nB_1 &= \\{(\\text{H},\\text{H},\\text{H}), (\\text{H},\\text{H},\\text{T}), (\\text{H},\\text{T},\\text{H}), (\\text{T},\\text{H},\\text{H})\\}\\\\\nB_2 &= \\{(\\text{T},\\text{T},\\text{T}), (\\text{T},\\text{T},\\text{H}), (\\text{T},\\text{H},\\text{T}), (\\text{H},\\text{T},\\text{T})\\}.\\end{align*}\\]\nWe can partition the space into those with \\(0\\), \\(1\\), \\(2\\), or \\(3\\) heads. This gives \\[\\begin{align*}\nB_1 &= \\{(\\text{H},\\text{H},\\text{H})\\}\\\\\nB_2 &= \\{(\\text{T},\\text{T},\\text{H}), (\\text{T},\\text{H},\\text{T}), (\\text{H},\\text{T},\\text{T})\\}\\\\\nB_3 &= \\{(\\text{H},\\text{H},\\text{T}), (\\text{H},\\text{T},\\text{H}), (\\text{T},\\text{H},\\text{H})\\}\\\\\nB_4 &= \\{(\\text{H},\\text{H},\\text{H})\\}.\n\\end{align*}\\]\nWe can partition the space into the number of times the sequence switches between heads and tails. There will be either \\(0\\) switches, \\(1\\) switch, or \\(2\\) switches. This gives \\[\\begin{align*}\nB_1 &= \\{(\\text{H},\\text{H},\\text{H}), (\\text{H},\\text{H},\\text{H})\\}\\\\\nB_2 &= \\{(\\text{T},\\text{T},\\text{H}), (\\text{H},\\text{T},\\text{T}), (\\text{H},\\text{H},\\text{T}), (\\text{T},\\text{H},\\text{H})\\}\\\\\nB_3 &= \\{(\\text{T},\\text{H},\\text{T}),  (\\text{H},\\text{T},\\text{H})\\}.\n\\end{align*}\\]\n\n\n\n\n\nPartitions allow us to move from discussions regarding the joint probability of events to the marginal probability of an event. Suppose that we have a partition given by \\(B_1, B_2, \\dots\\). This means that our full sample space can be cut up into these various non-overlapping sets, and every single outcome belongs to exactly one of them. Now, suppose we are interested in some other event \\(A\\). We can ask: how can \\(A\\) occur, in terms of the events \\(B_1, B_2, \\dots\\)? Since every single event in the sample space belongs to exactly one of our partitioning sets, then it must be the case that every single event in \\(A\\) belongs to exactly one of our partitioning sets. This means that if we consider \\(A\\cap B_j\\), for all \\(j\\), then every single event in \\(A\\) must belong to exactly one of these. In other words, it must be the case that \\[A = \\bigcup_{j} A\\cap B_j,\\] for any partition \\(B_1, B_2,\\dots\\). Moreover, every single \\(A\\cap B_j\\) is disjoint from every other \\(A \\cap B_\\ell\\), whenever \\(\\ell \\neq j\\). This means that we can use the axiom of additivity to give \\[P(A) = P\\left(\\bigcup_{j} A\\cap B_j\\right) = \\sum_{j} P(A \\cap B_j).\\] In other words: the marginal probability of \\(A\\) can be found by summing over all joint probabilities between \\(A\\) and sets that form a partition. This argument gives the law of total probability.\n\n\n\nFigure 4.1: This graphic shows the argument that summing over the joint probabilities between an event and a partition gives the full marginal probability. Note that \\(B_1,\\dots,B_7\\) forms a partition of the space where every possible outcome is contained in exactly one of these sets. Then, if we take an arbitrary event \\(A\\), we can divide \\(A\\) into the components that intersect with each partitioning set, namely \\(A \\cap B_1\\), \\(A \\cap B_2\\), and so forth.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Law of Total Probability\n\n\n\nGiven a partition, \\(B_1, B_2, \\dots\\), and an event \\(A\\), the law of total probability states that \\[P(A) = \\sum_i P(A, B_i) = \\sum_i P(A|B_i)P(B_i).\\]\n\n\nIntuitively, since the whole sample space is divided into the different \\(B_i\\)s, this rule breaks down the calculation of \\(A\\) happening into manageable chunks. Each term in the summation is “the probability that \\(A\\) happens, given \\(B_i\\) happening” weighted by how likely it is that \\(B_i\\) happens. Then by summing over all possible \\(B_i\\), we know that we must be capturing all possible ways that \\(A\\) can occur since all parts of the sample space are contained in exactly one of the sets of our partition. The law of total probability is an indispensible tool for computing probabilities in practice.\n\nExample 4.6 (Sadie’s Possibly Late Package) Sadie has still not received the package that Charles had ordered. While it is not late yet, Charles decides to figure out the probability that the package will end up being late. Recall that company \\(A\\) is late \\(75\\%\\) of the time while company \\(B\\) is late \\(15\\%\\) of the time, and the store that Charles ordered from sends out \\(10\\%\\) of packages with company \\(A\\), and the rest with company \\(B\\), which is more likely.\nWhat is the probability that the package is late?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNote that \\(A, B\\) forms a partition of the sample space as every package is either sent with \\(A\\) or with \\(B\\), and no package can be sent with both. Further, if \\(L\\) represents the event that the package is late, then we know that \\(P(L|A) = 0.75\\) and \\(P(L|B) = 0.15\\). Since \\(P(A) = 0.1\\) and \\(P(B) = 0.9\\), an application of the law of total probability gives \\[P(L) = P(L|A)P(A) + P(L|B)P(B) = (0.75)(0.1) + (0.15)(0.9) = 0.21.\\] As a result, knowing nothing else, the package has a probability of \\(0.21\\) of being late.\n\n\n\n\n\nExample 4.7 (Charles’ Many Urns) While shopping at some garage sales one Sunday morning, Charles and Sadie stumble across a wonderful find! They see three urns which are exactly identical to the one that Charles had already purchased to store the different balls which turned out to not be balls at all. Realizing the opportunity they splurge and purchase them all, and then divide the various objects between the four urns, placing all the spheres in one container, all the cubes in another, all the pyramids in a third, and all the cones in a fourth.\nOnce done, they use a different selection mechanism. First, they pick an urn at random. Next, they random grab one of the items from within it. The distribution of colours and shapes is included in the following table.\n\n\n\n\n\n\n\n\n\n\nColour\nSphere (Urn 1)\nCube (Urn 2)\nPyramid (Urn 3)\nCone (Urn 4)\n\n\n\n\nRed\n2\n3\n2\n0\n\n\nBlue\n1\n0\n0\n6\n\n\nGreen\n2\n2\n1\n2\n\n\nYellow\n0\n4\n2\n1\n\n\nBlack\n3\n1\n1\n2\n\n\n\n\nWhat is the probability that they select any of the five colours under this sampling scheme?\nHow does this change if the probability that each urn is selected is proportional to the number of items in it? (Thus, urn 1 is selected with probability \\(8/35\\), and so forth).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nLet \\(R\\), \\(B\\), \\(G\\), \\(Y\\), and \\(Bk\\) be the events that a red, blue, green, yellow, or black ball are selected. Further, let \\(U_1\\), \\(U_2\\), \\(U_3\\), and \\(U_4\\) be the events that the first, second, third, or fourth urn are selected. Then note that, according to the law of total probability, \\[P(R) = P(R, U_1) + P(R, U_2) + P(R, U_3) + P(R, U_4) = \\sum_{j=1}^4 P(R|U_j)P(U_j).\\] An equivalent argument holds for each of the other colours. Now, \\[P(R|U_j) = \\frac{N_{R\\cap U_j}}{N_{U_j}},\\] where \\(N_{R\\cap U_j}\\) is the number of red objects in urn \\(j\\) and \\(N_{U_j}\\) is the total number in urn \\(j\\). Plugging this in and simplifying we get \\[P(R) = \\sum_{j=1}^4P(R|U_j)\\left(\\frac{1}{4}\\right) = \\frac{1}{4}\\left\\{\\frac{2}{8} + \\frac{3}{10} + \\frac{2}{6}\\right\\} = \\frac{53}{240}.\\]\nWe can apply analogous arguments to the other colours giving \\[\\begin{align*}\nP(B) &= \\frac{1}{4}\\left\\{\\frac{1}{8} + \\frac{6}{11}\\right\\} = \\frac{59}{352} \\\\\nP(G) &= \\frac{1}{4}\\left\\{\\frac{2}{8} + \\frac{2}{10} + \\frac{1}{6} + \\frac{2}{11}\\right\\} = \\frac{527}{2640}\\\\\nP(Y) &= \\frac{1}{4}\\left\\{\\frac{4}{10} + \\frac{2}{6} + \\frac{1}{11}\\right\\} = \\frac{34}{165}\\\\\nP(Bk) &= \\frac{1}{4}\\left\\{\\frac{3}{8} + \\frac{1}{10} + \\frac{1}{6} + \\frac{2}{11}\\right\\} = \\frac{1087}{5280}.\n\\end{align*}\\]\nNote that these probabilities sum to \\(1\\). In decimal these simplify to approximately 0.221, 0.168, 0.2, 0.206, 0.206.\nUsing the same setup as before, we have \\[P(R) = P(R, U_1) + P(R, U_2) + P(R, U_3) + P(R, U_4) = \\sum_{j=1}^4 P(R|U_j)P(U_j).\\] Now \\(P(U_1) = 8/35\\), \\(P(U_2) = 10/35\\), \\(P(U_3) = 6/35\\), and \\(P(U_4) = 11/35\\). Moreover, the conditional probabilities themselves do not change, and so instead we have \\[P(R) = \\left\\{\\frac{2}{8}\\cdot\\frac{8}{35} + \\frac{3}{10}\\cdot\\frac{10}{35} + \\frac{2}{6}\\frac{6}{35} + \\frac{0}{11}\\cdot\\frac{11}{35}\\right\\} = \\frac{N_R}{35}.\\] Note that when multiplying by the marginal probability of the urn, the denominator will always cancel. As a result, we end up with the total number of reds over \\(35\\), which leads to \\(P(R) = \\frac{1}{5}\\). The same will be true for the other colours, and as a result, if we choose the urn based on a weighted selection, this will result in equal probability once more.\n\n\n\n\n\n\n\n4.3.3 Bayes’ Theorem\nWe have seen the direct computation of marginal probabilities (while using an equally likely outcome model), the computation of conditional probabilities, the use of the multiplication rule for joint probabilities, and the use of the law of total probability to indirectly calculate marginal probabilities through conditioning arguments. Throughout these discussions we have been primarily concerned with keeping events \\(A\\) and \\(B\\) arbitrary. Everything that we have indicated for \\(P(A)\\) holds for \\(P(B)\\), as does \\(P(A|B)\\) and \\(P(B|A)\\). In reality, it will often be the case that conditioning on one of the events will be natural, while conditioning on the other will be more tricky. In these events, it can be useful to be able to transform statements regarding \\(P(A|B)\\) into statements regarding \\(P(B|A)\\), and vice versa.\nNote that because the definitions are symmetric, \\[P(A|B)P(B) = P(A,B) = P(B|A)P(A).\\] This is an application of the multiplication rule in two different orientations. If we divide both sides of the equality by \\(P(B)\\), assuming that it is not \\(0\\), then we get \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}.\\] Now, if we form a partition, say \\(A,A_2,A_3,\\dots\\), then we can rewrite \\(P(B)\\) using the law of total probability as \\[P(B) = P(B|A)P(A) + P(B|A_2)P(A_2) + \\cdots = P(B|A)P(A) + \\sum_{i=2}P(B|A_i)P(A_i),\\] which can replace \\(P(B)\\). Taken together this gives a result known as Bayes’ Theorem.4\n\n\n\n\n\n\nBayes’ Theorem\n\n\n\nSuppose that there are two events, \\(A\\) and \\(B\\), with \\(P(B) &gt; 0\\). Moreover, suppose that \\(A\\) taken with \\(A_2, A_3, \\dots\\) forms a partition. Then Bayes’ Theorem states that \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B|A)P(A)}{P(B|A)P(A) + \\sum_{i=2} P(B|A_i)P(A_i)}.\\]\n\n\nBayes’ Theorem allows us to convert statements regarding \\(P(B|A)\\) into statements regarding \\(P(A|B)\\). Note that, as we derived above, Bayes’ Theorem is an application of the multiplication rule and an application of the law of total probability.5 Sometimes we may have \\(P(B)\\) directly, rendering the law of total probability in the denominator unnecessary.\nOften, the natural partition to select when we do need the law of total probability is to take \\(A\\) and \\(A^C\\). Note that any set with its complement forms a partition, since by definition they occupy the entire space and are non-overlapping. When this is done we get the slightly more compact relationship of \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^C)P(A^C)}.\\]\nBayes’ Theorem differs from our previous relationships as it allows us to translate one set of conditional probabilities into another. Every other relationship we have looked at has moved between types of probabilities, whereas Bayes’ Theorem deals directly with conditional relationships.\nThe most commonly cited example application is in medical testing. Suppose that we know the performance characteristics of a particular medical test: it is \\(99\\%\\) accurate for positive cases, and \\(95\\%\\) accurate for negative cases. That is, with probability \\(0.99\\) it correctly returns positive when an individual is infected, and with probability \\(0.95\\) it returns negative when an individual is not infected.These are both statements of conditional probability. If we take \\(A\\) to be the event that the test returns positive, and \\(B\\) to be the event that the patient is infected,6 then we are saying that \\(P(A|B) = 0.99\\) and \\(P(A^C|B^C) = 0.95\\) which means that \\(P(A|B^C) = 0.05\\). Suppose that we know that, across the entire population, one in a thousand individuals is likely to be infected. This means that \\(P(B) = 0.001\\).\nNow if a random individual goes into a doctor’s office and tests positive for the disease, how likely are they to actually be infected? In this case we want to know the probability of them being infected given that they have tested positive. In notation, this is \\(P(B|A)\\). We do not know this quantity directly, but given an application of Bayes’ Theorem, we can find it. Using the natural partition of \\(B\\) and \\(B^C\\), we get \\[P(B|A) = \\frac{P(A|B)P(A)}{P(A|B)P(B)+P(A|B^C)P(B^C)} = \\frac{(0.99)(0.001)}{(0.99)(0.001)+(0.05)(0.999)} \\approx 0.019.\\] That is, despite the fact that this test is exceptionally effective at detecting this disease, a positive test still means that an individual has a probability of only \\(0.019\\) of actually having the illness.7\n\nExample 4.8 (Sadie’s Late Package) Sadie eventually received the package that Charles had sent, but it arrived very late. Sadie was not home when the package was delivered and there no obvious markings on the box to indicate which of the two delivery companies had sent it.\nGiven this information, and knowing that company \\(A\\) is late \\(75\\%\\) of the time while company \\(B\\) is late \\(15\\%\\) of the time, and the store that Charles ordered from sends out \\(10\\%\\) of packages with company \\(A\\), and the rest with company \\(B\\), what is the probability that the package was delivered by each of the two companies?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe want to determine \\(P(A|L)\\) and \\(P(B|L)\\). We know that \\(P(L|A) = 0.75\\) and \\(P(L|B) = 0.15\\). Moreover, we know that \\(P(A) = 0.1\\) and \\(P(B) = 0.9\\). Applying Bayes’ Theorem directly we get \\[P(A|L) = \\frac{P(L|A)P(A)}{P(L|A)P(A) + P(L|B)P(B)} = \\frac{(0.75)(0.10)}{(0.75)(0.10) + (.10)(0.90)} = \\frac{5}{11}.\\] Similarly, we get \\[P(B|L) = \\frac{P(B|A)P(B)}{P(L|A)P(A) + P(L|B)P(B)} = \\frac{(0.15)(0.90)}{(0.75)(0.10) + (.10)(0.90)} = \\frac{6}{11}.\\] Note, we could have also used the fact that \\(P(A|L) + P(B|L) = 1\\) to determine this. As a result, it is more likely that \\(B\\) delivered the package than \\(A\\).8\n\n\n\n\nBayes’ Theorem highlights a key lesson when considering conditional probabilities, and it’s a common mistake to make which should be avoided at all costs. Namely, we cannot interchange \\(P(A|B)\\) with \\(P(B|A)\\). These probabilities are not necessarily highly correlated with one another, and it is important to distinguish clearly which is the probability of interest.9 The stakes of these types of confusion can be quite high, and it is tremendously important to ensure that you are conditioning on the correct events. Fortunately, Bayes’ Theorem allows us to translate between events for conditioning, giving a mechanism from translating between the two.10",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilities with More than One Event</span>"
    ]
  },
  {
    "objectID": "notes/chapter4.html#independence",
    "href": "notes/chapter4.html#independence",
    "title": "4  Probabilities with More than One Event",
    "section": "4.4 Independence",
    "text": "4.4 Independence\nWe have seen that, in most cases, conditioning on an event changes the probability of that event. For instance, if we want to know the probability it is raining, if we condition on knowing that it is a day full of gray skies, the conditional probability is likely higher than the marginal probability. By considering how the marginal and conditional probabilities differ, we are in effect indicating a dependence of the events on one another. In terms of probability, this dependence is captured by an influence on the degree of uncertainty present depending on what we know.\nIt is totally possible that two events do not influence one another at all. The weather outside today is likely not influenced by your favourite sports team’s performance last night.11 In this case, we would have \\(P(A|B) = P(A)\\).\nWe saw an example of this previously when we wanted to know the probability of a randomly selected card being a heart (\\(A\\)) given that it was an ace (\\(B\\)). We found that this was \\(\\frac{1}{4}\\), exactly the same as the probability if we did not know that it was an ace. Thus here we have \\(P(A|B)=P(A)\\). We could have also said that \\(P(B|A)=P(B)=\\frac{1}{13}\\). The symmetry of these events makes it somewhat more convenient to express this relationship differently.\nInstead of writing \\(P(A|B) = P(A)\\) and \\(P(B|A) = P(B)\\), we can multiply the first relationship by \\(P(B)\\) on both sides, or the second by \\(P(A)\\) on both sides. The multiplication rule gives \\(P(A|B)P(B) = P(A,B)\\), and so the first relationship becomes \\(P(A,B) = P(A)P(B)\\). The second follows exactly the same. Any two events that satisfy this relationship are said to be independent.\n\nDefinition 4.6 (Independence) Any two events, \\(A\\) and \\(B\\), which satisfy \\(P(A,B) = P(A)P(B)\\) are said to be independent. If \\(A\\) and \\(B\\) are independent we write \\(A\\perp B\\), and read “\\(A\\) is independent of \\(B\\)”. Any events which are not independent are said to be dependent, and we write \\(A\\not\\perp B\\).\n\nNote that independence is always a symmetric property: if \\(A\\) is independent of \\(B\\), then \\(B\\) is independent of \\(A\\). To check whether two events are independent, we check whether their joint probability is equal to the product of their marginal probabilities.\n\n\n\n\n\n\nProperties of Independence\n\n\n\n\n\nNote that if \\(A\\perp B\\), then \\(A^C\\perp B\\), \\(A^C\\perp B^C\\), and vice versa. To see this note \\[P(A^C, B) + P(A, B) = P(B),\\] by the Law of Total Probability. Then, by independence of \\(A\\) and \\(B\\) this gives \\[\\begin{align*}\nP(A^C, B) + P(A)P(B) &= P(B) \\\\\n\\implies P(A^C, B) &= P(B) - P(A)P(B) \\\\\n&= P(B)(1 - P(A)) \\\\\n&= P(B)P(A^C),\\end{align*}\\] as is required. The other combinations follow in the same manner.\n\n\n\nIf \\(P(A)\\neq 0\\), then we can divide both sides by \\(P(A)\\) to gives \\(P(B|A) = P(B)\\). Similarly, if \\(P(B)\\neq 0\\), then we can divide both sides by \\(P(B)\\) to give \\(P(A|B)=P(A)\\). This expression in terms of conditional probabilities is the more intuitive expression of independence. It directly captures the idea that “knowing \\(B\\) does not change our belief about \\(A\\)”. However, we must be careful. This conditional argument is only valid when the event that is being conditioned on is not probability \\(0\\), where the defining relationship, \\(P(A,B) = P(A)P(B)\\), will hold for all events. Recall that, in general, \\(P(A\\cap B) = P(A)+P(B)-P(A\\cup B)\\). It is only when assuming independence that this simplifies further.\n\nExample 4.9 (Independence and Board Games) Charles and Sadie are invited over to a friends house, Garth, to play some board games. Both of them are quite excited by this prospect as board games feel like the next logical step for the two of them, being as into probability and games as they are! Garth has a large board game collection, and begins to explain the games to Charles and Sadie across a variety of axes:\n\nWhether the games are competitive or cooperative.\nHow many players the games play best at (\\(\\{1, 2, 3, 4+\\}\\)).\nHow “heavy” the games tend to be (friendly for everyone, moderately involved, or very heavy).\n\nWhile Garth continues on about several other topics including the themes, the mechanics, or the rating on BoardGameGeek, Charles drifts off wondering whether the traits listed are independent in Garth’s collection. Suppose that Garth has \\(100\\) games, of which:\n\n\\(25\\) are cooperative.\n\\(5\\) are best played at one player, \\(20\\) at two, and \\(55\\) at three.\n\\(10\\) are friendly for everyone and \\(45\\) are moderately involved.\n\n\nIf \\(5\\) games are best played at two players and are cooperative, are these events independent?\nIf \\(20\\) games are heavy and best played with \\(4+\\) players, are these events independent?\nIf \\(0\\) games are both moderately involved and cooperative, are these events independent?\nCharles figures that competitive games and two player games are independent. How many competitive two player games are there?\nIs it possible that competitive games are independent of heavy games?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe are suggesting that \\(P(A,B) = 0.05\\), where \\(A\\) is “two player” and \\(B\\) is “cooperative”. We know that \\(P(B) = 0.25\\) and \\(P(A) = 0.20\\). Calculating \\(P(A)P(B) = (0.20)(0.25) = 0.05 = P(A,B)\\), and so these traits are independent.\nWe have \\(P(A,B) = 0.2\\) where \\(A\\) is “heavy” and \\(B\\) is “\\(4+\\) players. We know that \\(P(A) = 0.45\\) and \\(P(B) = 0.20\\), and so \\(P(A)P(B) = (0.45)(0.20) = 0.09 \\neq P(A,B)\\). As a result, these traits are not independent.\nWe know that \\(P(A) &gt; 0\\) and \\(P(B) &gt; 0\\) for \\(A\\) being moderately involved and \\(B\\) being cooperative. As a result, \\(P(A)P(B) &gt; 0\\), and so if \\(P(A,B) = 0\\), they must not be independent.\nTaking \\(A\\) to be “competitive” we have \\(P(A) = 0.75\\). Taking \\(B\\) to be “two player” we have \\(P(B) = 0.20\\). As a result, if \\(A\\perp B\\) then \\(P(A,B) = P(A)P(B) = (0.75)(0.20) = 0.15\\). Thus, there must be \\(15\\) competitive, two player games.\nTaking \\(A\\) to be “competitive” we have \\(P(A) = 0.75\\). Taking \\(B\\) to be “heavy” weh ave \\(P(B) = 0.45\\). Thus, if they were independent, we would have \\(P(A,B) = P(A)P(B) = (0.75)(0.45) = 0.3375\\). This would require \\(33.75\\) games to be heavy, competitive games and so we must conclude that they are not independent.\n\n\n\n\n\n\n4.4.0.1 Mutually Exclusive Events\nImportantly, if \\(A\\perp B\\), then \\(A\\cap B\\neq \\emptyset\\) unless either \\(A=\\emptyset\\), \\(B=\\emptyset\\), or both. To see this recall that \\(P(\\emptyset) = 0\\), and so if \\(A\\cap B = \\emptyset\\) then \\(P(A\\cap B) = P(A)P(B) = P(\\emptyset) = 0\\). This only holds if either \\(P(A) = 0\\) or \\(P(B) = 0\\). This may seem to be a rather technical point, however, it is the source of much confusion regarding independence. In particular, it is common to mistake independent events for mutually exclusive events.\n\nDefinition 4.7 (Mutually Exclusive Events) Two events, \\(A\\) and \\(B\\) are said to be mutually exclusive if they are disjoint. In particular, if \\(P(A,B) = 0\\) then \\(A\\) and \\(B\\) are mutually exclusive. If \\(A\\) and \\(B\\) are mutually exclusive events, with \\(P(A) &gt; 0\\) and \\(P(B) &gt; 0\\), then \\(A\\not\\perp B\\).\n\nWhenever only one event from a set of events can happen, we refer to the events as being mutually exclusive. If one happens, we know that the others did not. Mutually exclusive events are always dependent since knowing that \\(A\\) occurs dramatically shifts our belief about \\(B\\),\\(C\\), and \\(D\\).12\nThe primary concern with mutually exclusive and independent events is a linguistic one. We often use words like independent to mean unrelated, and in a sense, mutually exclusive events are unrelated in that one has nothing to do with another. However, in statistics and probability, when we discuss independence, it is not an independence of the events themselves but rather an independence relating to our beliefs regarding the uncertainty associated with the events. In this sense, mutually exclusive events are very informative regarding the uncertainty associated with them.\n\nExample 4.10 (Charles and Sadie Cooking Dinner) Suppose that Charles and Sadie always eat dinner together. It will either be the case that Charles cooks at home, that Sadie cooks at home, that the two of them order in, or that they go out to eat. If we take these to be four events, \\(A\\), \\(B\\), \\(C\\), and \\(D\\), then are any of these events independent? Mutually exclusive? Explain.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming, as it seems reasonable given the information in the question, that only one of the possible tasks occurs for dinner in a night, then we know that \\[P(A,B) = P(A,C) = P(A,D) = P(B,C) = P(B,D) = P(C,D) = 0.\\] As a result, all of the events described are mutually exclusive, and correspondingly, are not independent.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilities with More than One Event</span>"
    ]
  },
  {
    "objectID": "notes/chapter4.html#contingency-tables",
    "href": "notes/chapter4.html#contingency-tables",
    "title": "4  Probabilities with More than One Event",
    "section": "4.5 Contingency Tables",
    "text": "4.5 Contingency Tables\nThrough to this point we have discussed probabilities in the abstract, either through an enumeration of equally likely outcomes, or else by directly specifying the likelihood of various events. While these are useful in many regards, we are often looking for more concise manners of summarizing information of interest. One tool for accomplishing this is a contingency table.\n\nDefinition 4.8 (Contingency Table) A contingency table is a tabular summary of information which summarizes the joint probabilities of two or more variables. Typically a contingency table will take one factor for the columns and a secondary factor for the rows, where each cell then represents the frequency with which observations occur in the two corresponding categories simultaneously.\n\nTo begin, you could imagine constructing a frequency table relaying the frequency with which undergraduate students are enrolled in various faculties at a particular university. This tells you, of the whole population of students at the university, what is the faculty breakdown. By dividing the number in each faculty by the total number of students, you convert the frequencies to proportions, and these proportions can be viewed as probabilities. (See Table 4.1.13) The interpretation of proportions as probabilities implies a very specific statistical experiment. In particular, the proportion represents the probability that an individual selected at random from the entire population has the given trait. This is frequently a probability of interest, which makes these summary tables a useful tool.14\n\n\n\n\nTable 4.1: Frequency and corresponding proportions of enrolment by faculty in a university.\n\n\n\n\n\n\nFaculty\nEnrolment\nProportion\n\n\n\n\nArts\n1266\n0.2182382\n\n\nComputer Science\n749\n0.1291157\n\n\nEducation\n786\n0.1354939\n\n\nEngineering\n1315\n0.2266851\n\n\nLaw\n266\n0.0458542\n\n\nNursing\n543\n0.0936046\n\n\nScience\n876\n0.1510084\n\n\nTotal\n5801\n1.0000000\n\n\n\n\n\n\n\n\n\n\nWhen a single trait is displayed we refer to these tabular summaries as frequency tables or frequency distributions. A contingency table instead plots two or more traits on the same table, with each cell representing the frequency of both traits occurring simultaneously in the population. Extending the university example, we may further include the student’s current year of study to see the breakdown of both faculty and year of study, in one table.\n\n\n\n\nTable 4.2: Frequency of enrolment by faculty and year of study in a university.\n\n\n\n\n\n\n\nYear 1\nYear 2\nYear 3\nYear 4\nYear 5+\n\n\n\n\n\nArts\n222\n276\n273\n225\n270\n1266\n\n\nComp. Sci.\n164\n87\n164\n90\n244\n749\n\n\nEducation\n100\n136\n184\n128\n238\n786\n\n\nEngineering\n189\n290\n354\n298\n184\n1315\n\n\nLaw\n54\n50\n58\n37\n67\n266\n\n\nNursing\n55\n90\n154\n112\n132\n543\n\n\nScience\n242\n114\n206\n183\n131\n876\n\n\n\n1026\n1043\n1393\n1073\n1266\n5801\n\n\n\n\n\n\n\n\n\n\nBy including two (or more) factors in the table we are able to capture not only the marginal probabilities for the population, but also the joint probabilities for the population, and in turn, the conditional probabilities for the population. Being able to concisely summarize all of these concepts regarding traits in a population of interest renders contingency tables immensely useful in the study of uncertainty broadly.\nConsider the two-way contingency table, Table 4.2. Each cell consists of frequency with which a combination of the two traits occurs in the population.15 If we take events corresponding to each of the levels of the two variables of interest, then these central cells represent the frequency of joint events. That is, each interior cell gives the total number of observations with a set level for variable one16 AND a set level for variable two17. For instance, there are 50 individuals who are in Law and studying in Year 2.\nEach row is then summed, with the total number following into the corresponding row recorded in the right hand margin. each column is summed, with the total number corresponding to the given column recorded in the bottom margin. For instance there are a total of 786 students in Education and a total of 1393 in Year 3. Then the margin totals are summed and the total is recorded in the lower right margin space (in this case, 5801). Whether the rows or columns are summed, they should sum to the same total, which is the total of the population under consideration. This is the same as simply adding all of the observed interior frequencies. To turn a frequency into a probability, you need only divide the correct frequency by the correct total.\n\n\n\n\nTable 4.3: Proportions of enrolment by faculty and year of study in a university.\n\n\n\n\n\n\n\nYear 1\nYear 2\nYear 3\nYear 4\nYear 5+\n\n\n\n\n\nArts\n0.0382693\n0.0475780\n0.0470609\n0.0387864\n0.0465437\n0.2182382\n\n\nComp. Sci.\n0.0282710\n0.0149974\n0.0282710\n0.0155146\n0.0420617\n0.1291157\n\n\nEducation\n0.0172384\n0.0234442\n0.0317187\n0.0220652\n0.0410274\n0.1354939\n\n\nEngineering\n0.0325806\n0.0499914\n0.0610240\n0.0513705\n0.0317187\n0.2266851\n\n\nLaw\n0.0093087\n0.0086192\n0.0099983\n0.0063782\n0.0115497\n0.0458542\n\n\nNursing\n0.0094811\n0.0155146\n0.0265471\n0.0193070\n0.0227547\n0.0936046\n\n\nScience\n0.0417169\n0.0196518\n0.0355111\n0.0315463\n0.0225823\n0.1510084\n\n\n\n0.1768661\n0.1797966\n0.2401310\n0.1849681\n0.2182382\n1.0000000\n\n\n\n\n\n\n\n\n\n\nFor the standard joint probabilities, you take the interior cell count and divide by the population total. Here we are saying that some fixed number, \\(m\\), of the \\(N\\) total individuals have both traits under consideration. For instance, the joint probability that a student is in Law and studying in Year 2 is 0.0086192.18 If instead you wish to find a marginal probability, you have to consider the value in the corresponding margin: this is the total number of individuals with the given trait, ignoring the level of the other variable. These marginal values are also divided by the total population size. For instance, there is a 0.1354939 probability of observing a student in Education and a probability 0.240131 of observing a student in Year 3.\nOutside of joint and marginal probabilities, we can also find conditional probabilities. To do so, we restrict our focus to either only one row, or one column. Then, we can take the joint cell and divide by the value in the margin, which gives the conditional probability of interest. Note, this works with either the contingency table directly or with the propensity table. The reasoning is that the propensity table divided by the same totals in the numerator and the denominator. Suppose we take some cell, \\(A\\cap B\\), which has \\(N_{A, B}\\) in the contingency table. Then, \\[P(A\\cap B) = \\frac{N_{A, B}}{N} \\quad\\quad\\text{and}\\quad\\quad P(B) = \\frac{N_B}{N}.\\] If we consider then forming \\(P(A|B)\\) we get \\[P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{N_{A \\cap B}/\\cancel{N}}{N_B/\\cancel{N}} = \\frac{N_{A, B}}{N_B}.\\] Thus, given that we know a student is in Education, then the probability that they are in Year 3 is approximately 0.2341.\nNote that these procedures are exactly in line with what we had seen before. The conditional probability is defined as the joint probability divided by the marginal probability. The process of computing the marginal can be seen as an application of the law of total probability. As a result, contingency tables can be a useful, tangible tool for investigating the techniques we have been discussing: they are not a substitute for direct manipulation of the mathematical objects, but they can present insight into the underlying processes where it may be hard to derive that insight otherwise.\n\n\n\n\n\n\nThe Law of Total Probability: Contingency Table’s Version\n\n\n\n\n\nRecall that the law of total probability states that, if \\(B_1,\\dots,B_n\\) forms a partition of the sample space, then \\[P(A) = \\sum_{i=1}^n P(A|B_i)P(B_i) = \\sum_{i=1}^n P(A, B_i)\\]. In a contingency table it is easier to see how either of the two factors at play (either those in the rows or the columns) forms a partition of the space. Every observation has to fall in exactly one row and exactly one column. Thus, if we want to know the marginal frequency of a single trait (say represented by some row of the table), then one way to find this total is to sum up every observation in each of the corresponding columns. This is precisely the same process as the law of total probability. Note that, denoting each column as \\(B_i\\), and supposing there are \\(k\\) total columns, then the total number in a row is taken to be \\[N_A = \\sum_{i=1}^k N_{A, B_i},\\] simply as the summation of the corresponding row. By definition, \\(P(A) = \\frac{N_A}{N}\\), and so dividing both sides by \\(N\\) gives \\[P(A) = \\frac{N_A}{N} = \\frac{1}{N}\\sum_{i=1}^k N_{A, B_i} = \\sum_{i=1}^k \\frac{N_{A, B_i}}{N} = \\sum_{i=1}^n P(A\\cap B_i).\\]\n\n\n\nIt is important to note that there is redundant information within a contingency table. For instance, the margins need not be listed explicitly, as they can be directly calculated from the interior points. Same goes for interior points, given the margins of the table (assuming some interior points are also presented). This can be useful for a compact representation of the information, and manipulating these tables – being able to find the required information in many places – should become familiar to you as you continue to work with them more and more.\n\nExample 4.11 (The Evolving Contents of Charle’s Urns) After learning of contingency tables, Sadie points out to Charles that with the whole urn debacle they went through, the two of them actually ended up using a contingency table to summarize the information. How neat! In the interim, there has been some development in the contents of Charles’s urns, and armed with the new knowledge of contingency tables, the following summary is produced.\n\n\n\n\n\n\nCone\nCube\nPyramid\nSphere\n\n\n\n\n\nBlack\n2\nA\n1\n3\n10\n\n\nBlue\n1\n2\n1\n1\n5\n\n\nGreen\n3\n3\nB\n5\n15\n\n\nRed\n2\nC\n3\n2\nD\n\n\nYellow\n5\n3\n4\n1\n13\n\n\n\n13\nE\n13\n12\n50\n\n\n\n\n\n\n\nSadly, Charles does not have the best writing and Sadie cannot make out what values were written in for the cells marked \\(A\\), \\(B\\), \\(C\\), \\(D\\), and \\(E\\).\n\nWhat are the values for the missing values?\nWhat is the probability that a black cube is drawn?\nWhat is the probability that a red object is drawn?\nWhat is the probability that a cube is drawn?\nGiven that the drawn object was a pyramid, what is the probability that it is green?\nGiven that the object is green, what is the probability that it is a pyramid.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe start by filling in the missing cells, in order.\n\nFor \\(A\\) we can note that \\(2 + A + 1 + 3 = 10\\), which gives that \\(A = 4\\). We could have also tried a column sum, however, this would involve \\(3\\) unknowns and so would not have given a numeric result.\nFor \\(B\\) either a row sum or column sum would produce the correct answer. We either take \\(3 + 3 + B + 5 = 15\\) giving \\(B = 4\\) or \\(1 + 1 + B + 3 + 4 = 13\\) giving \\(B=4\\).\n\\(C\\) is more challenging as we either have that \\(2 + C + 3 + 2 = D\\) or \\(4 + 2 + 3 + C + 3 = E\\)19 As a result, for this technique we will need to find either \\(D\\) or \\(E\\) first. There is an alternative technique to use, which would be to note that we have all the internal points specified at this point, and we know that these sum to \\(50\\). As a result, we could note that \\(C = 50 - 10 - 5 - 15 - 13 - 2 - 3 - 2 = 0\\) or that \\(C =  50 - 13 - 13 - 12 - 4 - 2 - 3 - 3 = 0\\).\nWe can find \\(D\\) either by subtracting from the total, \\(D = 50 - 10 - 5 - 15 - 13 = 7\\), or by adding along the row, \\(2 + 0 + 3 + 2 = 7\\).20 Note that with \\(D = 7\\), had we not found \\(C = 0\\) above, we could now take \\(2 + C + 3 + 2 = 7\\) and find \\(C = 0\\).\nLike \\(D\\), we have two options for working this out. Either \\(E = 50 - 13 - 13 - 12 = 12\\) or \\(E = 4 + 2 + 3 + 0 + 3 = 12\\).21 Like with \\(D\\), had we solved for \\(E\\) first, then we could find \\(C\\) via \\(4 + 2 + 3 + C + 3 = 12\\).\n\nHere we want \\(P(\\text{Black}, \\text{Cube})\\). This is given by the frequency of black cubes divided by \\(50\\). Thus, \\[P(\\text{Black}, \\text{Cube}) = \\frac{A}{50} = \\frac{4}{50} = 0.08.\\]\nHere we want \\(P(\\text{Red})\\). This is given by the marginal frequency of red objects divided by \\(50\\). Thus, \\[P(\\text{Red}) = \\frac{D}{50} = \\frac{7}{50} = 0.14.\\]\nHere we want \\(P(\\text{Cube})\\). This is given by the marginal frequency of cubes divided by \\(50\\). Thus, \\[P(\\text{Cube}) = \\frac{E}{50} = \\frac{12}{50} = 0.24.\\]\nHere we want \\(P(\\text{Green}|\\text{Pyramid})\\). This is given by the frequency of green pyramids divided by the marginal frequency of pyramids. Thus \\[P(\\text{Green}|\\text{Pyramid}) = \\frac{B}{13} = \\frac{4}{13} \\approx 0.308.\\]\nHere we want \\(P(\\text{Pyramid}|\\text{Green})\\). This is given by the frequency of green pyramids divided by the marginal frequency of green objects.22 Thus \\[P(\\text{Pyramid}|\\text{Green}) = \\frac{B}{15} = \\frac{4}{15} \\approx 0.267.\\]\n\n\n\n\n\nIt is also important to recognize that independence and mutually exclusive events can be codified via the table as well. Zeros on the interior points indicate events which are mutually exclusive: if we know that one of them occurred, we also know that the other one did not. For independence, it requires a degree of solving proportions. We can either check that the joint probability (\\(N_{A,B}/N\\)) is equal to the product of the two marginal probabilities, (\\(N_AN_B/N^2\\)), or else (assuming that the events are all non-zero), that the conditional probability (\\(N_{A,B}/N_A\\)) equals the marginal probability \\(N_B/N\\). Either way this is represented by \\(N\\times N_{A,B} = N_AN_B\\), and when this holds, we can conclude that the events are independent.\n\nExample 4.12 (Independent or Mutually Exclusive Urn Shapes) After helping Charles neatly fill in the contingency table, Sadie begins to wonder about whether there are any shape-colour combinations which are independent, or if any are mutually exclusive.\n\n\n\n\n\n\nCone\nCube\nPyramid\nSphere\n\n\n\n\n\nBlack\n2\n4\n1\n3\n10\n\n\nBlue\n1\n2\n1\n1\n5\n\n\nGreen\n3\n3\n4\n5\n15\n\n\nRed\n2\n0\n3\n2\n7\n\n\nYellow\n5\n3\n4\n1\n13\n\n\n\n13\n12\n13\n12\n50\n\n\n\n\n\n\n\n\nAre there any events represented in the contingency table which are mutually exclusive?\nAre there any events represented in the contingency table which are independent?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMutually exclusive events are codified via a \\(0\\) frequency. We can see then that \\(\\text{Cube}\\) and \\(\\text{Red}\\) are mutually exclusive. That is, if we know that an object is red, we also know that it is not a cube. And if we know that an object is a cube, then we know that it is not red.\nIndependence is more cumbersome to check. We require the product of marginal counts to be equal to the total times by the joint counts. As a result, the easiest way to check is to consider the product of each marginal value in the columns by each marginal value in the row, divided by \\(50\\). If this value corresponds to the value in that row/column pairing, then we know that those features are independent. Note that immediately we can rule out any products which are not divisible by \\(50\\), since we know that \\(N_{A,B}\\) is an integer for all \\(A,B\\) and \\(N_{A,B} = N_AN_B/50\\) if there is independence. To this end, we only need to check the product of each of the row totals with each of \\(12\\) and \\(13\\) (as those are the only two unique column totals). This gives, in order \\(120\\), \\(130\\), \\(60\\), \\(65\\), \\(180\\), \\(195\\), \\(84\\), \\(91\\), \\(156\\), and \\(169\\). None of these values are divisible by \\(50\\) and as a result we know that there is no independence codified in this table.\n\n\n\n\n\n\n4.5.0.1 Contingency Tables and Data Frames in R\nIn R we can use the table and prop.table to calculate the (interior) of a contingency table. This will return a table type object in R, which can be thought of as a matrix of sorts. On it, we can use the functions rowSums and colSums to get the summation of the rows and columns, respectively. Typically the object we pass to table will be a data frame. A data frame is another R object type that we have not yet seen. The idea with a data frame is that we have multiple columns with different variables (of possibly different types) represented. Each row corresponds to a single observation, and then each column is read as a feature of those observations. Data frames are essentially large spreadsheets (or data tables) which indicate the various observations that we have. In practice, data frames are the most commonly used object in an R analysis, and we will see them plenty going forward.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWith a data frame specified, we can then use the table function called on it to form a contingency table.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThen, to get the totals, we can either use rowSums or colSums to return a vector with the corresponding row or column sums.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFinally, we can take our formed contingency table, and use prop.table on it, in order to return a table with the proportions (rather than the frequencies).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilities with More than One Event</span>"
    ]
  },
  {
    "objectID": "notes/chapter4.html#footnotes",
    "href": "notes/chapter4.html#footnotes",
    "title": "4  Probabilities with More than One Event",
    "section": "",
    "text": "Note, this is another result which seems to (on the surface) defy expectations. We will see this again later on in this chapter in a slightly different context. It seems strange that company \\(A\\) is more likely to be late, and yet, we are more likely to see a late package that is sent by company \\(B\\) then a late package that is sent by company \\(A\\). The reason for this is that company \\(B\\) is used much more frequently than company \\(A\\), which overcomes the added likelihood of company \\(A\\) being late.↩︎\nFor instance, \\(5\\) is neither a multiple of \\(2\\) nor of \\(3\\).↩︎\nFor instance, \\(6\\) is a multiple of both \\(2\\) and \\(3\\).↩︎\nBayes’ Theorem is named in the same way that the Bayesian interpretation of probability is, and that Bayesian statistics is more broadly. The connections are more than merely surface: Bayes’ Theorem can be viewed as the primary technique with which we can update subjective beliefs about the world. Importantly, however, even those who use a Frequentist view of statistics accept the math of Bayes’ Theorem, and use it frequently.↩︎\nIn fact, I would go as far as to suggest that learning Bayes’ Theorem by itself is less important than fully grasping the definition of conditional probability alongside the multiplication rule and the law of total probability. I myself do not remember Bayes’ Theorem directly, but can write it down directly from these definitions without any thought.↩︎\nIt is worth drawing attention to the language that we have started to use at this point in the notes regarding “events”. In this case our sample space would actually be formed using pairs of information. In particular, we might have \\[\\mathcal{S} = \\{(\\text{Pos. Test}, \\text{Illness}), (\\text{Neg. Test}, \\text{Illness}),(\\text{Pos. Test}, \\text{No Illness}),(\\text{Neg. Test}, \\text{No Illness})\\}.\\] Then the event \\(A\\) here is actually \\(A = \\{(\\text{Pos. Test}, \\text{Illness}),(\\text{Pos. Test}, \\text{No Illness})\\}\\) and \\(B\\) is \\(\\{(\\text{Pos. Test}, \\text{Illness}),(\\text{Neg. Test}, \\text{Illness})\\}\\). It is far more clunky to make explicit these events, and so we move towards using more natural language. Until you feel confident that you can identify the specific outcomes associated with the events of interest, it is worth writing these out in full.↩︎\nThis counter intuitive fact was an intensely frustrating reality for statisticians everywhere during the height of the COVID-19 pandemic, when politicians and the population at large turned away from testing owing to its perceived “ineffectiveness”. The quantity of interest for knowing how good a test is is \\(P(A|B)\\) and \\(P(A^C|B^C)\\). However, if a disease is sufficiently rare, with \\(P(B)\\) sufficiently small, then no matter how effective the tests are you will likely have \\(P(B|A)\\) to be low. Note that \\(P(B|A) &gt;&gt; P(B)\\) in the example, and this will also be true in general. A single test cannot say with certainty, however, they are an incredibly effective tool at reducing our uncertainty.↩︎\nNote that this is another example of a counterintuitive result. Here, \\(A\\) is far more likely to be late, but it is more likely that \\(B\\) delivered a late package to us than \\(A\\) because of the base rates. That is, \\(P(B)\\) is much higher to begin than \\(P(A)\\), and so that is hard to overcome. It is worth noting, however, that originally \\(P(B)\\) is \\(9\\) times more likely than \\(P(A)\\), and after knowing that package is late it is \\(\\frac{6}{5} = 1.2\\) times more likely. This major reduction in the relative likelihood is owed to how much more likely \\(A\\) is to deliver late packages than \\(B\\).↩︎\nMixing up \\(P(B|A)\\) and \\(P(A|B)\\) is often called confusion of the inverse, and it can lead to very faulty conclusions when ignored. In the medical testing example above, it is important to not confuse “the probability that the test returns positive, assuming you have the illness” with “the probability that you have the illness, given that the test returns positive.” This type of faulty logic has long been used to justify discriminatory behaviour in medicine, the law, and so on. A thorough understanding of statistics and probability helps to ensure that these types of errors are not made, and gives you the tools to push-back on the spots where people are making these arguments incorrectly, particularly when the stakes are high or harm is being done.↩︎\nWhen discussing Bayes’ Theorem we introduced two frequently occurring sources of error when reasoning about probabilities, and showed how to remedy them. Confusion of the inverse occurs when you mix up \\(P(A|B)\\) with \\(P(B|A)\\), and can lead to disastrous consequences. The base rate fallacy occurs when you fail to take into account the rareness of the marginal events and instead only consider the conditionals (such as seeing that delivery company \\(A\\) was more likely to be late than \\(B\\), without considering that \\(B\\) was more likely to be used than \\(A\\)). These are both examples of the challenges at the heart of probability and statistics: namely, the subjects are fairly unintuitive once moving beyond the basics. As a result, we need to rely on building up our intuitions over time by making use of the formal rules that we are able to derive.↩︎\nThough, perhaps the world will freeze over if (insert-your-least-favourite-team-here) wins the (insert-the-name-of-the-championship-for-the-league-you-care-about-here)?↩︎\nNamely, we know that all of the others are then impossible.↩︎\nNote, the values in this table are roughly inspired by a subset of the Fall 2022 University of New Brunswick Enrolment Numbers.↩︎\nThis provides further emphasis for the utility of urn models. If you imagine an urn that has balls with two or more traits (say like those in Example 4.3), then randomly selecting a single ball from this population has an analogous probability distribution.↩︎\nThat is, the number of students who are enrolled in that faculty, in that year of study.↩︎\nFaculty.↩︎\nYear of study.↩︎\nIt is worth reemphasizing what this probability actually means. If we were to randomly sample, with equal probability, individuals from this population then the probability that an individual selected has both the faculty and year specified is given by the joint probability. That is to say, if we did this over and over and over again (with replacement) in the long-term, these probabilities represent proportion of time those combinations would be observed.↩︎\nNote here, \\(A=4\\) has been filled in.↩︎\nWe have filled in \\(C = 0\\) here.↩︎\nThis uses \\(A=4\\) and \\(C=0\\).↩︎\nNote, we could also apply Bayes’ Theorem to find the result here. We know that \\(P(\\text{Green}|\\text{Pyramid}) = 4/13\\), that \\(P(\\text{Green}) = 15/50\\) and that \\(P(\\text{Pyramid}) = 13/50\\). Thus, Bayes’ Theorem gives \\[P(\\text{Pyramid}|\\text{Green}) = \\frac{(4/13)(13/50)}{15/50} = \\frac{4}{15},\\] exactly as in the direct derivation.↩︎",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilities with More than One Event</span>"
    ]
  },
  {
    "objectID": "notes/chapter5.html#independent-and-identically-distributed-a-framework-for-interpretation",
    "href": "notes/chapter5.html#independent-and-identically-distributed-a-framework-for-interpretation",
    "title": "5  Random Variables",
    "section": "5.1 Independent and Identically Distributed: A Framework for Interpretation",
    "text": "5.1 Independent and Identically Distributed: A Framework for Interpretation\nA very common assumption when addressing questions in statistics and in probability is that we have a set of random variables which are independent and identically distributed (iid). We now have the tools to understand concretely what this means. Specifically, a set of random variables, \\(X_1,X_2,\\dots,X_n\\) are said to be independent and identically distributted (denoted iid almost all the time) whenever (i) every subset of random variables in the collection is independent of every other subset of random variables in the collection, and (ii) the marginal distribution for each of the random variables are exactly the same. The assumption of iid random quantities will often come up when we are repeating a process many times over, and thinking about what observations will arise from this. Suppose that \\(X_1\\) is a random variable that takes the value \\(1\\) if a flipped coin comes up heads, and \\(0\\) otherwise. If we imagine flipping this coin \\(100\\) times then it is reasonable to assume that each sequential coin flip will be independent of eacch other coin flip, since the result on one flip of a coin should not influence the result of any other flip of a coin. Moreover, every time the coin is flipped, it is reasonable to assume that the probability it shows up heads remains the same. As a result, these \\(100\\) random quantities, \\(X_1,  X-2,  \\dots,  X_{100}\\) can be said to be iid.\nTODO: include other iid examples\nWhile we will use the assumption of iid random variables later, they also provide an intuitive method for interpretting probability functions and distributions. Suppose that we wwere to take a distribution function, \\(p_X(x)\\). If we were able to generate independent and identically distributed realizations from this probability mass ufnction, then the function \\(p_X(x)\\) describes the behaviour for these repeated realizations. Specifically, \\(p(x)\\) will give the long-run proportion of realizations of the iid random variables which take the value \\(x\\).\nTODO: Include interpretation statement\nThis type of statement is always the flavour of interpretation statements that are made with respect to probability and statistics. It will always be the case that, in order to understand what is meant specifically by a statement of probability will involve the repetition of some statistical experiment over and over again. When we were discussing sample spaces and experiments directly, we talked about repeating the experiment over and over again. When we begin to work with random variables instead, it becomes more natural to think about the replication procedures coming through the use of independent and identically distributed random variablles.\nAs the study of probability foes on, we begin to need to work with random quantities in a strictly theoretical sense. In introductory level problems, we are often holding in mind very concrete examples to illustrate the procedures and concepts. In this setting it is easy enough to hold in mind the experiment of interest: for instance, we may have a random variable representing the result of a coin toss, and you can envision repeatedly tossing a coin. As the concepts become less concrete, more abstract, and harder to draw directl parallels to tangible scenarios, it becomes more and more important to rely on the interpretations rooted in a series of independent and identically distributed random variables. A large component of statistics as an area of study is making explicit the assumptions we are working with, and doing our best to ensure that these are reasonable. By interpreting probability mass functions as the proportion of independent and identically distributed random variables that take on a particular value, when we repeatedly take realizations of these random variables ad infinitum, this philosophy is made clear and explicit.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter5.html#expectation",
    "href": "notes/chapter5.html#expectation",
    "title": "5  Random Variables",
    "section": "5.2 Expectation",
    "text": "5.2 Expectation\nUntil this point in our discussions of probability we have relied upon characterizing the behaviour of a random variable via the use of probability mass functions. In some sense, a probability mass function captures all of the probabilistic behaviour of a discrete random variable. Using the mass function you are able to characterize how often, in the long-run, any particular value will be observed, and any questions associated with this. As a result, the mass function remains a critical area of focus for understanding how random quantities behave.\nHowever, these functions need to be explored and manipulated in order for useful information to be extracted from them. They do not summarize this behaviour effectively, as they are not intended to be a summary tool, and understandably we often wish to have better numeric quantities which are able to concisely indicate components of the behaviour of a distribution. Put differently, provided with a priobability mass function it is hard to immediately answer ``what do we expect to happen, with this random variable?’’ despite the fact that this is a very obvious first question.\nTo address questions related to expectations, we turn towards the statistical concept of an expected value. We refer to expected values as expectations, averages, and means of a distribution, interchangeably. The idea with an expected value is that we are trying to capture, with one single number, what value is expected when we make observations from the random quantity. There are many ways one might think to describe our expectations, and it is worth exploring these concepts in some detail.\nOne way that we may think to define our expected value is by asking what value is the most probable. This is a question which can be directly answered using the probability mass function. The process for this would require looking at the function and determining which value for \\(x\\) corresponds to the highest probability: this is the value that we are most likely to see. Sometimes this procedure is fairly straightforward, sometimes it is quite complicated. No matter the complexity of the specific situation, the underlying process is the same: what value has the highest probability of being seen, and that is the most likely one.\nTODO: Example of mode\nAs intuitive as this may seem, this is not the value that will be used as the expectedvalue generally. Instead, this quantity is referred to as the mode. While the mode is a useful quantity, and for some decisions will be the most pertinent summary value, there are some major issues with it as a general measure which make it less desirable. For starters, consider that our most common probability model considered until this point has been that of equally likely outcomes. Here, there is no well-defined mode (convention tends to be taking it as the set of all the most probable values). Presenting the mode is equivalent to presenting the full mass function in this setting.\nWhile the case of equally likely outcomes is a fairly strong explanation for some issues with the mode, it need not be so dramatic to undermine its utility. It is possible for a distribution to have several modes which are quite distinct from one another, even if it’s not all values. Moreover, it is quite common for the modal value to be not particularly likely itself. Consider a random variable that can take on a million different values. If all of the probabilities are approximately \\(0.000001\\) then presenting the mode as the most probable value does not translate to saying that the mode is particularly probable.\nTODO: incllude example where the mode is slightly more likeely than a string of similar values.\nIf the mode has these shortcomings, what else might work? Another intuitive concept is to try to select the ``middle’’ oof thedistribution. One way to define the middle would be to select the value such that half of observations are beneath it, and half of observations are above it. That way, when you are told this value, you immediately know that it is equally likely to observe values on either side of this mark. This is also a particularly intuitive definition for expected value, and is important enough to be named: the median.\nThe median is the midpoint of a distribution, and is very important for describing the behaviour of random ariables. Medians are often the most helpful single value to report to indicate the typical behaviour of a distribution, and they are frequently used. When people interpret averages, in general, it is often the median that they are actually interpretting. It is very intuitive to be given a value and know that it is the middle of all the possible values for a distribution.\nTODO: include examples on medians\nDespite the advantages of medians, they have their own drawbacks as well. For starters, the median can be exceptionally challenging to compute in certain settings. As a result, even when a median is appropriate, it may not be desirable if it is too challenging to determine. Beyond the difficulties in computation, medians have some properties which may be undesirable, depending on the specific use case. One concern which arises frequently is that medians are not translated to totals, which can make them challenging in certain use cases.\nSuppose that you are a store and you know that your median quantity of items sold in a day is \\(50\\) and the median cost of these items is \\(\\$10\\), you cannot simply multiply the \\(50\\) and the \\(\\$10\\) to suggest that your median revenue in a day is \\(\\$500\\). Doing this ttype of unit conversion or basic arithmetic with medians can be challenging, and as a result they are not always the most useful when reporting values that are going to be interpretted as rates.\nTODO: Expand on the above example.\nBeyond the basic manipulation medians have a feature which is simultaneously a major benefit in some settings, and a major fallback in others. Specifically, medians are less influenced by extreme values in the probability distribution. Consider two different distributions: one of them is equally likely to take any value between \\(1\\) and \\(10\\), where the other is equally likely to take any value between \\(1\\) and \\(9\\) or \\(1,000,000\\). In both of these settings, we can take the median to be \\(5.5\\) since half of the probability mass falls above \\(5.5\\) and half falls below it.\nThe median, in some sense, ignores the extreme value in the probability distribution and remains stable throughout it. In certain settings, this can be very desirable. For instance, in the distribution of household incomes, the median may be an appropriate measure seeing as there are a few families who have very extreme incomes which otherwise distort the picture provided by most families. In this sense, the median’s robustness to extreme values is a positive feature of it in terms of a summary measure for distributional behaviour.\nSuppose instead that you work for an insurance company and are concerned with understanding the value of insurance claims that your company will need to pay out. Thedistribution will look quite similar to the income distribution: most of the probability will be assigned to fairly small claims, with a small chance of a very large one. As an insurance company, if you use the median this large claim behaviour will be smoothed over, perhaps leaving you unprepared for the possibility of extremely large payouts. In this setting, the extreme values are informative and important, and as a result the median’s robustness becomes a hindrance to correctly describing the behaviour.\nTODO: Another median example?\nBetween the median and the mode we have two measures which capture some sense of expected value, each with their own set of strengths and drawbacks. Neithercapture what it is that is referred to as the expected value. For this, we need to take inspiration from the median, and consider another way that we may think to find the center of the distribution.\nIf the median gives the middle reading along the values sequentially, we may also wish to think about trying to find the ``center of gravity’’ of the numbers. Suppose you take a pen, or marker, or small box of chocolates, and you wish to balance this object on a finger or an arm. To do so, you do not place the item so that half of it sits on one side of the appendage and half on the other: you adjust the location so that half of the masssits on either side of the appendage.\nThroughout our probability discussions, we have always referred to probability as mass itself. We use the probability mass function to generate our probability values. This metaphor can be extneded when we try to find the center of the distribution. If we imagine placing a mass with weight equal to the probability mass functions value at each value that a random variable can take on, we may ask: where would we have to place a fulcrum to have this number line be balanced? The answer to this question serves as another possible measure of center.\nIt turns out that this notion of center is the one that we are all most familiar with: the simple average. And this simple average is also the conception of expectation which gets bestowed with the name ``expected value’’. Mathematically, the expeccted value is desirable for many reasons, some of which we will study in more depth later on. One of these desirable features, which stands in contrast with the median, is the comparative ease with which expected values can be computed. For a random variable, \\(X\\), we write the expected value of \\(X\\) as \\(E[X]\\), and assume that \\(X\\) takes values in \\(\\mathcal{X}\\) with a probability mass function \\(p_X(x)\\), we get \\[E[X] = \\sum_{x \\in \\mathcal{X}} xp_X(x).\\]\nTODO: Example compute simple expected value.\nIn the case of an equally likely probability model, the expected value becomes the standard average that is widely used. Suppose that there are \\(n\\) options in the sample space, denoted \\(x_1,\\dots,x_n\\), then we can write \\[E[X] = \\sum_{i=1}^n x_i\\frac{1}{n} = \\frac{1}{n}\\sum_{i=1}^nx_i.\\] When the probability models are more complex, the formula is not precisely the stnadard average - instead, it becomes a weighted average.\nTODO: Add basic average example.\nWhile less commonly applied than the simple average, a weighted average is familiar to most students for a crucial purpose: grade calculations. If you view the weight of each item in a course as a probability mass, and the grade you scored as the value, then your final grade in the course is exactly the expected value of this distribution. The frequency with which expectedvalues are used make them attractive as a quick summary for the center of a distribution.\nTODO: Include pricing calculation showing mean versus median.\nWhile the mean provides a useful, intuitive measure of center of the distribution, it is perhaps counter intuitive to name it the expected value. To understand the naming convention it is easiest to consider the application which has likely spurred more development of statistics and probability than any other: gambling.\nSuppose that there is some game of chance that can pay out different amounts with different probabilities. A critical question for a gambler in deciding whether or not to play such a game is ``how much can I expect to earn, if I play?’’ This is crucial to understanding, for instance, how much you should be willing to pay to participate, or if you are the one running the game, how much you should charge to ensure that you make a profit.\nIf you want to understand what you expect to earn, the intuitive way of accomplishing this is to weight each possible outcome by how likely it is to occur. This is exactly the expected value formula that has been provided, and so the expected value can be thought of as the expected payout of a game of chance where the outcomes are payouts corresponding to each probability.\nTODO: Include expected payout calculation.\nThis also represents the cost at which a rational actor should be willing to pay to participate. If a game ofchance costs more than the expected value to play, in the long run you will lose money. If a game of chance costs less than the expected value, in the long run you will earn money. It is hard to overstate the utility of gambling in developing probabiility theory, and as such these types of connections are expected.\nTo interpret the expected value of a random variable, one possibility is using the intuition that we used to derive the result. Notably, the expected value is the center of mass of the distribution, where the masses correspond to probabilities. This means that it is not necessarily an actual central number over the range, but rather that it sits in the weighted middle. While this interpretation is useful in many situations, there are times where the point of balance is a less intuitive description. For these, it can sometimes be useful to frame the expected value as the long-term simple average from the distribution.\nIf we imagine observing many independent and identically distributed random variables, then as the number of samples tends to infinity, the expected value of \\(X\\) and the simple average will begin to coincide with one another. That is the distance between \\(E[X]\\) and \\(\\frac{1}{n}\\sum_{i=1}^n X_i\\) will shrink to \\(0\\). As a result, we can view the expected value as the average over repeated experiments. This interpretation coincides nicely with the description based on games of chance. Specifically, if you were to repeatedly play the same game of chance, the average payout per game will be equal to the expected value, if you play for long enough.\nTODO: Include convergence graphic.\nSomtimes the value of a random variablle needs to be mapped through a function to give the value which is most relevant to us. Consider, for instance, a situation wherein the side lengths of boxes being manufactured by a specific supplier are random, due to incorrectly calibrated tolerances in the machines. The resulting boxes are cubes, but what is of more interest is the volume of the produced box, not the side length. If a box has side length \\(x\\), then its volume will be \\(x^3\\), and so we may desire some way of computing \\(E[X^3]\\) rather than \\(E[X]\\).\nIn general, for some function \\(g(X)\\), we may want to compute \\(E[g(X)]\\). It is important to recognize that, generally speaking, \\(E[g(X)] \\neq g(E[X])\\). This is a common mistake, and an attractive one, but a mistake nonetheless. If we are unable to simply apply the function to the epxected value, then the question of how to compute the expected value remains. Instead of applying the function to overall expected value, instead, we simply apply the function to each value in the defining relationship for the expected value. That is, \\[E[g(X)] = \\sum_{x\\in\\mathcal{X}} g(x)p_X(x).\\] This is sometimes referred to as the ``law of the unconscious statistician,’’ a name which may be aggressive enough to help remember the correct way to compute the expectation.\nTODO: Move the next thing up. Where the median demonstrated robustness against extreme values in the distribution, the mean (or expected value) does not. For instance, if we consider the distribution of incomes across a particular region, the mean will be much higher than the median, as those families with exceptionally high incomes will not be smoothed over as they were with medians. In this case, the lack of robustness for the expected value will render the mean a less representative summary for the true behaviour of the random quantity.\nTo see this concretely, consider the difference between a random variable which with equal probability takes a value between \\(1\\) and \\(10\\). This will have \\(E[X] = 5.5\\). Now, if the \\(10\\) is made to be \\(1,000,000\\), the expected value will now be \\(E[X] = 100,004.5\\). This is a far cry from the median which does not change from \\(5.5\\) in either case. This lack of robustness is desirable in the event of the insurance example from the median discussion, but will be less desirable in other settings.\nThe mean, median, and mode are the three standard measures of central tendency. They are also referred to as measures of location, and in general, are single values which describe the standard behaviour of a random quantity. Each of the three has merits as a measure, and each has drawbacks for certain settings. The question of which to use and when depends primarily on the question of interest underconsideration, rather than on features of the data alone. Often, presenting more than one measure can give a better sense of the distributional behaviour that any one individual will.\nTODO: Include example of choosing measures.\nDespite the utility of all three measures, the expected value holds a place of more central importance in probability and statistics. A lot of this has to do with further mathematical properties of the mean. Because of its central role, it is worth studying the expected value in some more depth. % END OF MOVING SECTION.\nTODO: include example using LOTUS.\nThese functions applied to random variables are often thought of as ``transformations’’ of the random quantities. For instance, we transformed a side length into a volume. While the law of the unconscious statistician will apply to any transformation for a random variable, we can sometimes use shortcuts to circumvent its application. In particular, when \\(g(X) = aX + b\\), for constant numbers \\(a\\) and \\(b\\), we can greatly simplify the expected value of the transformation. To see this note \\[\\begin{align*}\nE[aX + b] &= \\sum_{x\\in\\mathcal{X}}(ax + b)p_X(x) \\\\\n&= \\sum_{x\\in\\mathcal{X}}axp_X(x) + bp_X(x) \\\\\n&= a\\sum_{x\\in\\mathcal{X}}xp_X(x) + b\\sum_{x\\in\\mathcal{X}}p_X(X) \\\\\n&= aE[X] + b.\n\\end{align*}\\] That is, in general, we have that \\(E[aX + b] = aE[X] + b\\).\nThis is particularly useful as linear transformations like \\(aX+b\\) arise very commonly. For instance, most unit conversions are simple linear combinations. If a random quantity is measured in one unit then this result can be used to quickly convert expectations to another.\nTODO: include example of temperature or weight conversion.\nThis type of linear transformation also frequently comes up with games of chance and payouts, or with scoring more generally. For instance, suppose you are betting a certain amount on the results of a coin toss, or that you are taking a multiple choice test that gives \\(2\\) points for a correct answer.\nMeasures of central tendency are important to summarize the beahviour of a random quantity. Whether using the mean, median, or mode, these measures of location describe, on average, what to expect from observations of the random quantity. However, understanding a distribution requires understanding far more than simply the measures of location. As was discussed previously, the probability mass function captures the complete probabilistic behaviourof a discrete random variable, it is only intuitive that some information would be lost with a single numeric summary.\nTODO: Example with equivalent mean, median, and mode.\nA key characteristic of the behaviour of a random variable which is not captured by the measures of location is the variability of the quantity. If we imagine taking repeated realizations of a random variable, the variability of the random variable captures how much movement there will be observation to observation. If a random variable has low variability, we expect that the various observations will cluster together, becoming not too distant from one another. If a random variable has high variability, we expect the observations to jump around each time.\nJust as was the case with measures of location, there are several measures of variability which may be applicable in any given setting. One fairly basic measure of the spread of a random variable is simply the range of possible values: whati s the highest possible value, what is the lowest possible value, and how much distance is there between those two points? This is a fairly intuitive notion, and is particularly useful in the equal probability model over a sequence of numbers. Consider, for instance, dice. dice are typically defined by the range of values that they occupy, say \\(1\\) to \\(6\\), or \\(1\\) to \\(20\\). Once you know the values present on any die, you have a sense for how much the values can move observation to observation.\nTODO: Include example for the range.\nWhile the range is an important measure to consider to determine the behaviour of a random variable, it is a fairly crude measurement. It may be the case that, while the extreme values are possible, they are sufficiently unlikely so as to come up very infrequently and not remain representative of thelikely spread of observations. Alternative, many random variables have a theoretically infinite range. In these cases, providing the range will likely not provide much utility.\nTODO: Include example.\nTo rememdy these two issues, we can think of some techniques for modifying the range. Instead of taking the start and end points to be the lowest and highest values, we can instead consider ranges of values which remain more plausible. A common way to do this is to extend our concept of a median beyond the half-way point. The median of a random variable \\(X\\), is the value, \\(m\\), such that \\(P(X \\leq m) = 0.5\\). While there is good reason to care about the midpoint, we can think of generalizing this to be any probability.\nThat is, we could find a number \\(z\\), such that \\(P(X \\leq z) = 0.1\\). We could then use this value to conclude that \\(10\\%\\) of observations are below \\(z\\), and \\(90\\%\\) of observations are above \\(z\\). (TODO: Change this to be ``probability of observation’’). These values are referred to, generally, as percentiles and they are the natural extension of medians. We will typically denote the \\(100p\\)th percentile as \\(\\zeta(p)\\), which is the value \\(P(X \\leq \\zeta(p)) = p\\). Thus, the median of a distribution is \\(\\zeta(0.5)\\).\nTODO: Include examples\nWe can leverage percentiles to remedy some of the issues with the range as a measure of variability. Framed in terms of percentiles, the minimum value is \\(\\zeta(0)\\), and the maximum value is \\(\\zeta(1)\\). Instead of considering theextreme endpoints, if we consider the difference between more moderate percentiles, we can overcome the major concenrs outlined with the range. The most common choices would be to take \\(\\zeta(0.25)\\) and \\(\\zeta(0.75)\\); these are referred to as the first and third quartiles, respectively. They are named as, taking \\(\\zeta(0.25)\\), \\(\\zeta(0.5)\\) and \\(\\zeta(0.75)\\), the distribution is cut into quarters.\nTODO: Include examples.\nWith the first and third quartiles computed, we can compute the interquartile range, which is given by \\(\\zeta(0.75)-\\zeta(0.25)\\). Typically, we denote the interquarite range simply as \\(\\text{IQR}\\), and like the overall range, it gives a measure of how much spread there tends to be in the data. Unlike the range, however, we can be more certain that both the first and third quartiles are reasonable values around which repeated observations of the random variable would be observed. Specifically, there is a oribability of \\(0.5\\) that a value between the first and third quartile will be observed. The larger the \\(\\text{IQR}\\), the more spread out these moderate observations will be, and as a result, the more variable the distribution is.\nTODO: Write examples.\nBoth the range and the interquartile range give a sense of the variation in the distribution irrespective of the measures of location for that distribution. Another plausible method for assessing the variability of a distribution is to assess how far we expect observations to be from the cneter. Intuitively, if observations of \\(X\\) are near the center with high probability, then the distribution will be less variable than if the averagedistance to the center is larger.\nThis intuitive measure of variability is useful for capturing the behaviour of a random variable, particularly when paired iwth a measure of location. However, we do have to be careful: not all measures of dispersion based on this notion will be useful. Consider the most basic possibility, to consider \\(X - E[X]\\). We might ask, for instance, what is the expected value of this quantity. If we take \\(E[X - E[X]]\\) then note that this a linear combination in expectation since \\(E[X]\\) is just some number. Thus, \\(E[X-E[X]] = E[X] - E[X] = 0\\). In other words, the expected difference between a random variable and its mean is exactly \\(0\\). We thus need to think harder about how best to turn this intuition into a useful measure of spread as the first idea will result in \\(0\\) for all random quantities.\nThe issue with this procedure is that some realizations are going to be below the mean, making the difference negative, and some will be above the mean, making the difference positive. Our defining relationship for the mean relied on balancing these two sets of mass. However, when discussing the variability of the random variable, we do not much care whether the observations are lower than expected or higher than expected, we simply care how much variability there is around what is expected. To remedy this, we should consider only the distance between the observation and the expectation, not the sign. That is, if \\(X\\) is \\(5\\) below \\(E[X]\\) we should treat that the same as if \\(X\\) is \\(5\\) above \\(E[X]\\).\nThere are two common ways to turn value into its magnitude in mathematics generally: squaring the number and using absolute values. Both of these tactics are useful approaches to defining measures of spread, and they result in the variance when using the expected value of the squared deviations, and the mean absolute deviation when using the absolute value. While \\(E[|X-E[X]|]\\) is perhaps the more intuitive quantity to consider, generally speaking it will not be the one that we use.\nIn general when we need a positive quantity in mathematics it will typically be preferable to consider the square to the absolute value. The reasons for this are plentiful, but generally squares are easier to handle than absolute values, and as a result become more natural quantities to handle. The variance is the central measure of deviation for random variables, so much so that we give it its own notation, \\[\\text{var}(X) = E[(X-E[X])^2].\\]\nNote that if we take \\(g(X) = (X-E[X])^2\\), then the variance of \\(X\\) is the expected value of a transformation. We have seen that to compute these we apply the law of the unconscious statistician, and substitute \\(g(X)\\) into the defining relationship for the expected value, which for the variance gives \\[\\text{var}(X) = \\sum_{x\\in\\in\\mathcal{X}} (x-E[X])^2p_X(x).\\] Prior to computing the variance, we must first work out the mean as the function \\(g(X)\\) relies upon this value.\nTODO: Include example for calculating variance.\nThe higher that an individual random variables variance is, the more spread we expect there to be in repeatedly realizations of that quantity. Specifically, the more spread out around the mean value the random variable will be. A random variable with a low variance will concentrate more around the mean value than one with a higher variance. One confusing part of the variance of a random variable is in trying to assess the units. Suppose that a random quantity is measured in a particular set of units - dollars, seconds, grams, or similar. In this case, our interpretations of measures of location will all be in the same units, which aids in drawing connections to the underlying phenomenon that we are trying to study. However, because the variance is squared, we cannot make the same extensions to it: variance is not measured in the regular units, but in the regular units squared.\nSuppose you have a random time being measured, perhaps the reaction time for some treatment to take effect in a treated patient. Finding the mean or median will give you a result that you can read off in seconds as well. The range and interquartile range both give you the spread in seconds. However, if you work out the variance of this quantity it will be measured in seconds squared - a unit that is challenging to have much intuition about. To remedy this we will often use a transformed version of the variance, called the standard deviation, returning the units to be only the original scale. The standard deviation of a random variable is simply given by the square root of the variance, which is to say \\[\\text{SD}(X) = \\sqrt{\\text{var}(X)}.\\] We do not often consider computing the standard deviation directly, and so will most commonly refer to the variance when discussing the behaviour of a random variable, but it is important to be able to move seamlessly between these two measures of spread.\nTODO: Standard deviation.\nWhen computing the variance of a random qauntity, we often use a shortcut for the formula. Consider \\[\\begin{align*}\n\\text{var}(X) &= \\sum_{x\\in\\mathcal{X}} (x-E[X])^2p_X(x) \\\\\n&= \\sum_{x\\in\\mathcal{X}} (x^2 - 2xE[X] + E[X]^2)p_X(x) \\\\\n&=\\sum_{x\\in\\mathcal{X}} x^2p_X(x) - 2E[X]\\sum_{x\\in\\mathcal{X}}xp_X(x) + E[X^2]\\sum_{x\\in\\mathcal{X}}p_X(x)\\\\\n&= E[X^2] - 2E[X]E[X] + E[X]^2\\\\\n&= E[X^2] - E[X]^2.\n\\end{align*}\\]\nThis result gives us the identity that the variance of \\(X\\) can be found via \\(E[X^2] - E[X]^2\\). Generally, this is moderately more straightforward to calculate since \\(X^2\\) is an easier transformation than \\((X-E[X])^2\\). This identity will come back time and time again, with a lot of versatility in the ways that it can be used. Typically, when a variance is needed to be calculated the process is to simply compute \\(E[X]\\) and \\(E[X^2]\\), and then apply this relationship.\nTODO: include variance calculation example.\nWith expectations, we saw that \\(E[g(X)]\\) needed to be directly computed from the definition. The same is true for variances of transformations. Specifically, \\(\\text{var}(g(X))\\) is given by \\(E[(g(X) - E[g(X)])^2]\\) which can be simplified with the previous relationship as \\(E[g(X)^2] - E[g(X)]^2\\). Just as with expectations, it is important to realize that \\(\\text{var}(g(X)) \\neq g(\\text{var}(X))\\), and so dealing with transformations requires further work.\nTODO: Transformation example. TODO: Move the following discussion up. Beyond being linear over simple transformations, summations in general behave nicely with expectations. Specifically, for any quantities separated by addition, say \\(g(X) + h(X)\\), the expected value will be the sum of each expected value. Formally, \\[\\begin{align*}\nE[g(X) + h(X)] &= \\sum_{x\\in\\mathcal{X}} (g(X) + h(X))p_X(x) \\\\\n&= \\sum_{x\\in\\mathcal{X}} g(X)p_X(x) + h(x)p_X(x) %todo fix capitals\\\\\n&= \\sum_{x\\in\\mathcal{X}} g(x)p_X(x) + \\sum_{x\\in\\mathcal{X}} h(x)p_X(x) \\\\\n&= E[g(X)] + E[h(X)].\n\\end{align*}\\] Behaving well under linearity is one of the very nice properties of expectations. It will come in useful when dealing with a large variety of important quantities, and as we will see shortly, this linearity will also extend to multiple different random quantities.\nTODO: add example of linearity. %End section to move.\nWith expectations, we highlighted linear transformations as a special case, with \\(g(X) = aX + b\\). For the variance, the linear transformations are also worth distinguishing from others. To this end, we can apply the standard identity for the variance, giving \\[\\begin{align*}\nE[(aX+b)^2] &= E[a^2X^2 + 2abX + b^2] \\\\\n&= E[a^2X^2] + E[2abX] + E[b^2]\\\\\n&= a^2E[X^2] + 2abE[X] + b^2.\n\\end{align*}\\]\nTODO:Move upwards Note that part of the property of the linearity of expectation that we can immediately see if that the expected valuye of any constant is always that constant. If we take \\(a = 0\\), then we see that \\(E[aX + b] = E[b] = b\\). Thus, any time that we need to take the expected value of any constant number, we know that it is just that number. %End upwards movement\nNext, we note that \\(E[aX + b] = aE[X] + b\\) and so \\[\\begin{align*}\nE[aX + b]^2 &= (aE[X] + b)^2 \\\\\n&= a^2E[X]^2 + 2abE[X] + b^2.\\end{align*}\\] Differencing these two quantities gives \\[a^2E[X^2] + 2abE[X] + b^2 - a^2E[X]^2 - 2abE[X] - b^2 = a^2(E[X^2] - E[X]^2).\\] By noting that \\(E[X^2] - E[X]^2\\), we can complete the statement that \\[\\text{var}(aX + b) = a^2\\text{var}(X).\\]\nThus, when applying a linear transformation, only the multiplicative constant matters , and it transforms the varaince by a squared factor. This should make some intuitive sense that the additive constant does not change anything. If we consider that variance is a measure of spread, adding a constant value to our random quantity will not make it more or less spread out, it will simply shift where the spread is located. This is not true of the mean, which measures where the center of the distribution is, which helps explain why the result identities are different.\nTODO: Example using this.\nIn the same way that the linearit of expectation demonstrates that the expected value of any constant is that constant, we can use this identity to show that the variance of constant is zero. However, we can also reason to this based on our definitions so far. Suppose that we have a random variable which is constant. This seems to be an oxymoron, but it is perfectly well defined. A constant \\(b\\) can be seen as a random variable with probability distribution \\(p_X(x) = 1\\) if \\(x=b\\) and \\(p_X(x) = 0\\) otherwise. In this case, the expected value is going to be \\(E[X] = 1(b) = b\\), and \\(E[X^2] = 1(b)^2 = b^2\\). As a result, we see that \\(E[b] = b\\), as previously stated, and \\(\\text{var}(b) = E[X^2] - E[X]^2 = b^2 - b^2 = 0\\). From an intuitive perspective, there is no variation around the mean of a constant: it is always the same value. As a result, when taking the variance, we know that it should be \\(0\\).\nUnlike the expecctation, the variance of additive terms will not generally be the addition of the varainces themselves. That is, we cannot say that \\(\\text{var}(g(X) + h(X)) = \\text{var}(g(X)) + \\text{var}(h(X))\\), as a general rule. Writing out the definition shows issue with this: \\[E[(g(X) + h(X))^2] = E[g(X)^2] + 2E[g(X)h(X)] + E[h(X)^2].\\] The first and third terms here are nicely separated and behave well. However, the central term is not going to be easy to simplify, in general. You can view \\(g(X)h(X)\\) as a function itself, and so \\(E[g(X)h(X)] \\neq E[g(X)]E[h(X)],\\)$ in general. Instead, this will typically need to be worked out for any specific set of functions.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter5.html#conditional-and-joint-expectations-and-variances",
    "href": "notes/chapter5.html#conditional-and-joint-expectations-and-variances",
    "title": "5  Random Variables",
    "section": "5.3 Conditional and Joint Expectations and Variances",
    "text": "5.3 Conditional and Joint Expectations and Variances\nUp until this point we have considered the marginal probability distribution when exploring the measures of central tendency and spread. These help to summarize the marginal behaviour of a random quantity, cpaturing the distribution of, for instance, \\(X\\) alone. When introducing distributions, we also made a point to introduce the conditional distribution as one which is particularly relevantwhen there is extra information. The question ``what do wwe expect to happen, given that we have an additional piece of information?’’ is not only well-defined, but it is an incredibly common type of question to ask. To answer it, we require conditional expectations.\nTODO: Include set of questions relating to conditional expectation.\nIn principle, a conditional expectation is no more challenging to calculate than a marginal expectation. Suppose we want to know teh expected value of \\(X\\) assuming that we know that a second random quantity, \\(Y\\) has taken on the value \\(y\\). We write this as \\(E[X|Y=y]\\), and all we do is replace \\(p_X(x)\\) with \\(p_{X|Y}(x|y)\\) in the defining relationship. That is \\[E[X|Y=y] = \\sum_{x\\in\\mathcal{X}}xp_{X|Y}(x|y).\\] In a sense, we can think of the conditional distribution of \\(X|Y=y\\) as simply being a distribution itself, and then work with that no differently. The conditional variance, which we denote \\(\\text{var}(X|Y=y)\\) is also exactly the same.\nTODO: Include an example.\nAbove we supposed that we knew that \\(Y=y\\). However, sometimes we want to work with the conditional distribution more generally. That is, we want to investigate the behaviour of \\(X|Y\\), without yet knowing what \\(Y\\) equals. We can use the same procedure as above, however, this time we leave \\(Y\\) unspecified. We denote this as \\(E[X|Y]\\), and this expression will be (in general) a function of \\(Y\\). Then, whenever a value for \\(Y\\) is observed, we can simply specify \\(Y=y\\), deriving the specific value. In practice, we will typically compute \\(E[X|Y]\\) rather than \\(E[X|Y=y]\\), since once we have \\(E[X|Y]\\) we can easily find \\(E[X|Y=y]\\) for every value of \\(y\\).\nTODO: Show example.\nSince \\(E[X|Y]\\) is a function of an unknown random quantity, \\(Y\\), \\(E[X|Y]\\) is also a random variable. It is a transformation of \\(Y\\), and as such, it will have some distribution, some expectation, and some varaince itself. This is often a confusing concept when it is first introduced, so to recap: \\(X\\) and \\(Y\\) are both random variables; $E[X] and \\(E[Y]\\) are both constant, numerical values describing the distribution of \\(X\\) and \\(Y\\); \\(E[X|Y=y]\\) and \\(E[Y|X=x]\\) are each numeric constants which summarize the distribution of \\(X|Y=y\\) and \\(Y|X=x\\) respectively; \\(E[X|Y]\\) and \\(E[Y|X]\\) are functions of \\(Y\\) and \\(X\\), respectively, and can as such be seen as transformations of (and random quantities depending on) \\(Y\\) and \\(X\\) respectively.\nWe do not often think of the distribution of \\(E[X|Y]\\) directly, however, there is a very useful result about both its expected value and its variance, which will commonly be exploited. Specifically, if we take the expected value of \\(E[X|Y]\\) we will find that \\(E[E[X|Y]] = E[X]\\). Note that since \\(E[X|Y] = g(Y)\\) for some transformation, \\(g\\), the outer expectation is taken with respect to the distribution of \\(Y\\). Sometimes when this may get confusing we will use notation to emphasize this fact, specifically, \\(E_Y[E_{X|Y}[X|Y]] = E_X[X]\\). This notation is not necessary, but it can clarify when there is much going on, and is a useful technique to fallback on. \\[\\begin{align*}\nE_Y[E[X|Y]] &= \\sum_{y\\in\\mathcal{Y}} E[X|Y]p_Y(y) \\\\\n&= \\sum_{y\\in\\mathcal{Y}}\\left(\\sum_{x\\in\\mathcal{X}}xp_{X|Y}(x|Y)\\right)p_Y(y) \\\\\n&= \\sum_{y\\in\\mathcal{Y}}\\sum_{x\\in\\mathcal{X}}x\\frac{p_{X,Y}(x,y)}{p_Y(y)}p_Y(y)\\\\\n&= \\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}xp_{X,Y}(x,y)\\\\\n&= \\sum_{x\\in\\mathcal{X}} xp_X(x)\\\\\n&= E[X].\\end{align*}\\]\nTODO: Include example.\nThis property, that \\(E[E[X|Y]] = E[X]\\) is important enough that it receives its own name: the law of total expectation. In the same way that it is sometimes easier to first condition on \\(Y\\) in order to compute the marginal distribution of \\(X\\) via applications of the law of total probability, so too can it be easier to first work out conditional expectations, and then take the expected value of the resulting expression. This adds on to the so-called ``conditioning arguments’’ that were discussed previously, allowing a technique to work out the marginal mean indirectly.\nTODO: Show example use case of LOTE.\nWhile the conditional expectation is used quite prominantly, the conditional variance is less central to the study of random variables. As discussed, briefly, the conditional variance is given by the same variance relationship, replacing the marginal probability distribution with the conditional one (just as with expectations). Just as with expectations, \\(\\text{var}(X|Y=y)\\) is a numeric quantity given by \\(E[(X-E[X|Y=y])^2|Y=y]\\) and \\(\\text{var}(X|Y)\\) is a random variable given by \\(E[(X-E[X|Y])^2|Y]\\). This means that when working with the general, \\(\\text{var}(X|Y)\\), we can also consider taking expectations of the resulting transformation.\nTODO: Include examples.\nA final result relating to conditional expectations and varainces connects the two concepts. This is known as the law of total variance. For any random variables \\(X\\) and \\(Y\\), we can write \\[\\text{var}(X) = E[\\text{var}(X|Y)] + \\text{var}(E[X|Y]).\\] This result can be viewed as decomposing thevariance of a random quantity into two separate components, and comes up again in later statistics courses. At this point we can view this as a method for connecting the marginal distribution through the conditional variance nad expectation.\nTODO: Examples of this.\nThe final set of techniques to consider for now relate to making use of the joint distribution between \\(X\\) and \\(Y\\). Specifically, if we have any function of two random variables, say \\(g(X,Y)\\) and we wish to find \\(E[g(X,Y)]\\). This follows all of the expected derivations that we have used so far, this time replacing the marginal with the joint distribution. That is, \\[E[g(X,Y)] = \\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}g(x,y)p_{X,Y}(x,y).\\] For instance, if we want to consider the product of two random variables, we could use this technique to determine \\(E[XY]\\). The variance extends in the same manner as well.\nTODO: Include example.\nThis defining relationship allows us to work out the expected value of a linear combination of two random variables. That is \\[\\begin{align*}\nE[X+Y] &= \\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}(x+y)p_{X,Y}(x,y) \\\\\n&= \\sum_{x\\in\\mathcal{X}}x\\sum_{y\\in\\mathcal{Y}}p_{X,Y}(x,y) + \\sum_{y\\in\\mathcal{Y}}y\\sum_{x\\in\\mathcal{X}}p_{X,Y}(x,y) \\\\\n&= \\sum_{x\\in\\mathcal{X}}xp_X(x) + \\sum_{y\\in\\mathcal{Y}}yp_Y(y) \\\\\n&= E[X] + E[Y].\\end{align*}\\]\nThe same property does not apply with varainces, at least not in general. To see this, consider that \\[\\begin{align*}\nE[(X+Y-E[X]-E[Y])^2] &= E[((X-E[X])+(Y-E[Y]))^2] \\\\\n&= E[(X-E[X])^2] + E[(Y-E[Y])^2] + 2E[(X-E[X])(Y-E[Y])] \\\\\n&= \\text{var}(X) + \\text{var}(Y) + 2E[(X-E[X])(Y-E[Y])].\\end{align*}\\] The term that impedes the linear relationship, \\(E[(X-E[X])(Y-E[Y])]\\) can be computed as any joint function can be. This quantity, however, is particularly important when considering the relationship between two random variables. This is called the covariance and it is a measure of the relationship between \\(X\\) and \\(Y\\). Typically we write \\(E[(X-E[X])(Y-E[Y])] = \\text{cov}(X,Y)\\) so that \\[\\text{var}(X+Y) = \\text{var}(X) + \\text{var}(Y) + 2\\text{cov}(X,Y).\\]\nTODO: Include example.\nThe covariance behaves similarly to the varaince. We can see directly from the definition that \\(\\text{cov}(X,X) = \\text{var}(X)\\). Moreover, using similar arguments to those used for the varaince, we can show that \\[\\text{cov}(aX+b,cY+d) = ac\\text{cov}(X,Y).\\] Covariances remain linear, so that \\[\\text{cov}(X+Y,X+Y+Z)=\\text{cov}(X,X)+\\text{cov}(X,Y)+\\text{cov}(X,Z)+\\text{cov}(Y,X)+\\text{cov}(Y,Y)+\\text{cov}(Y,Z).\\] These make covariances somewhat nicer to deal with than variances, and on occasion it may be easier to think of variances as covariances with themselves.\nTODO: Example? Maybe.\nIt is worth considering, briefly, the ways in which conditional and joint expectations interact. Namely, if we know that \\(Y=y\\), then the transformation \\(g(X,y)\\) only has one random component, which is the \\(X\\). As a result, taking \\(E[g(X,Y)|Y=y] = E[g(X,y)|Y=y]\\). If instead we use the conditional distribution without a specific value, we still have that \\(Y\\) is fixed within the expression, it is just fixed to an unknown quantity. That is \\(E[g(X,Y)|Y]\\) will be a function of \\(Y\\). We saw before that \\(E[E[X|Y]] = E[X]\\), and the same is true in the joint case. Thus, one technique for computing the joint expectation, \\(g(X,Y)\\) is to first compute the conditional expectation, and then compute the marginal expectation of the resulting quantity.\nTODO: example.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter5.html#independence-in-all-of-this",
    "href": "notes/chapter5.html#independence-in-all-of-this",
    "title": "5  Random Variables",
    "section": "5.4 Independence in all of this",
    "text": "5.4 Independence in all of this\nWhenever we can assume independence of random quantities, this allows us to greatly simplify teh expressions we are dealing with. Recall that the key defining relationship with independence is that \\(p_{X,Y}(x,y) = p_X(x)p_Y(y)\\). Suppose then that we can write \\(g(X,Y) = g_X(X)g_Y(Y)\\). For instance, for the covariance we have \\(g(X,Y)=(x-E[X])(Y-E[Y])\\) and so \\(g_X(X) = X-E[X]\\) and \\(g_Y(Y) = Y-E[Y]\\). If we want to compute \\(E[g(X,Y)]\\) then we get \\[\\begin{align*}\nE[g(X,Y)] &= E[g_X(X)g_Y(Y)] \\\\\n&= \\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}g_X(x)g_Y(y)p_{X,Y}(x,y) \\\\\n&= \\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}g_X(x)g_Y(y)p_X(x)p_Y(y) \\\\\n&=\\sum_{x\\in\\mathcal{X}}g_X(x)p_X(x)\\sum_{y\\in\\mathcal{Y}}g_Y(y)p_Y(y)\\\\\n&= E[g_X(X)]E[g_Y(Y)].\\end{align*}\\] Thus, whenever random variables are independent, we have the ability to separate them over their expectations.\nTODO: example.\nConsider what this means, in particular, for the covariance between independent random variables. If \\(X\\perp Y\\) then \\[\\begin{align*}\n\\text{cov}(X,Y) &= E[(X-E[X])(Y-E[Y])] \\\\\n&= E[(X-E[X])]E[(Y-E[Y])] \\\\\n&= (E[X]-E[X])(E[Y]-E[Y]) \\\\\n&= 0.\\end{align*}\\] That is to say, if \\(X\\) and \\(Y\\) are indepdent, then \\(\\text{cov}(X,Y)=0\\). As a result of this, for independent random variables \\(X\\) and \\(Y\\) we also must have that \\(\\text{var}(X+Y)=\\text{var}(X)+\\text{var}(Y)\\). It is critical to note that this relationship does not go both ways: you are able to have \\(\\text{cov}(X,Y) = 0\\) even if \\(X\\not\\perp Y\\).\nTODO: Include example of independence.\nWhile we have primarily focused on joint and conditional probabilities with two random variables, the same procedures and ideas apply with three or more as well. The relevant joint distribution, or conditional distribution would simply need to be substituted in definitions. Often the complexity here becomes a matter of keeping track of which quantities are random, and which are not. For instance, if we have \\(X,Y,Z\\) as random variables, then \\(E[X|Y,Z]\\) is a random function of \\(Y\\) and \\(Z\\). We will still have that \\(E[E[X|Y,Z]] = E[X]\\), however, the outer expectation is now the joint expectation with respect to \\(Y\\) and \\(Z\\). As a result, we can also write \\(E[E[X|Y,Z]|Y]\\). The first expectation will be with respect to \\(X|Y,Z\\), while the outer expectation is with respect to \\(Z|Y\\). This becomes a useful demonstration for when making the distribution of the expectation explicit helps to clarify what is being computed. As a general rule of thumb, the innermost expectations will always have more conditioning variables than the outer ones: each time we step out, we peel back one of hte conditional variables until the outermost is either a marginal (or joint). This will help to keep things clear.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter6.html#geometric-random-variables",
    "href": "notes/chapter6.html#geometric-random-variables",
    "title": "6  The Named Discrete Distributtions",
    "section": "6.1 Geometric Random Variables",
    "text": "6.1 Geometric Random Variables\nWhile the binomial counted the number of successes in a fixed number of trials, we may also be interested in questions relating to how many trials would be needed to see a success. Instead of how many heads in $n$ flips of a coin?'' we may askhow many flips of a coin to get a head?’’ Like the binomial, quantities related to counting the number of trials until a success are related deeply to Bernoulli trials, where once more we are envisioning a sequence of independent and identically distributed trials being performed. However, instead of knowing that we will stop after \\(n\\) trials, here we will only stop once we see a particular result.\nAny random quantities following this type of procedure are said to follow a geometric distribution. The geometric distribution is parameterized with a single parameter, \\(p\\), the probability of success. We write \\(X\\sim\\text{Geo}(p)\\), and have that \\[\np_X(x) = \\begin{cases}(1-p)^{x-1}p & x \\geq 1 \\\\\n0 & \\text{otherwise}.\\end{cases}\\] Moreover, \\(E[X] = \\frac{1}{p}\\) and \\(\\text{var}(X) = \\frac{1-p}{p^2}\\).\nTODO: geometric calculation.\nThe geometric distribution differs from other named distributions that we have considered in that the random variable can take on an infinite number of possible values. The probability that \\(X\\) exceeds a very large threshold shrinks down to \\(0\\), however, there is no maximum value that can be observed. Beyond that assumption, the key set of assumptions are the same as for the binomial: independent and identically distributed Bernoulli trials are run, and are stopped only after the first observed success.\nIn the framing we are using here, the random variable \\(X\\) counts the total number of trials including the trail upon which the first success was reached. Sometimes you may see this distribution parameterized slightly differently, taking \\(X\\) to instead count the number of failures before the first success. To convert between the two framings we need only subtract \\(1\\): there is no meaningful difference in the underlying behaviour.\nTODO: include several geometric examples.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Named Discrete Distributtions</span>"
    ]
  },
  {
    "objectID": "notes/chapter6.html#negative-binomial",
    "href": "notes/chapter6.html#negative-binomial",
    "title": "6  The Named Discrete Distributtions",
    "section": "6.2 Negative Binomial",
    "text": "6.2 Negative Binomial\nA natural way to make the geometric distribution more flexible is to not stop after the first success, but rather after a set number of successes. That is, instead of flipping a coin until we see a head, we flip a coin until we see \\(r\\) heads. Any random quantity which follows this general pattern is said to follow a negative binomial distribution. We use two parameters to describe the negative binomial distribution, \\(r\\) the number of successes we are looking to achieve, and \\(p\\) the probability of a success on any given trial. We write \\(X\\sim\\text{NB}(r,p)\\).\nTODO: Basic example\nIf we know that \\(X\\sim\\text{NB}(r,p)\\), then we immediately get \\[p_X(x) = \\begin{cases}\\binom{x-1}{r-1}p^r(1-p) & x\\geq r \\\\ 0 &\\text{otherwise}.\\end{cases}\\] Moreover, we have \\(E[X] = \\frac{r}{p}\\) and \\(\\text{var}(X) = \\frac{r(1-p)}{p^2}\\). Setting \\(r=1\\), wee get the same quantities explored in the case of the geometric distribution.\nTODO: Include NB Examples\nLike for the geometric distribution, we have taken \\(x\\) to represent the total number of trials considered, including the \\(r\\) successes. (TODO: fix geometric distribution range, start at 1). There are alternative parameterizations which would count how many failures occur prior to the \\(r\\)th success, which can be viewed as the value we consider minus \\(r\\).\nTODO: Include many NB examples",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Named Discrete Distributtions</span>"
    ]
  },
  {
    "objectID": "notes/chapter6.html#hypergeometric-distributions",
    "href": "notes/chapter6.html#hypergeometric-distributions",
    "title": "6  The Named Discrete Distributtions",
    "section": "6.3 Hypergeometric Distributions",
    "text": "6.3 Hypergeometric Distributions\nOne of the use cases demonstrated for the binomial distribution is drawing with replacement. In order for the binomial distribution to be relevant it must be the case that the probability of a success is unchanging, and correspondingly, if the process under consideration cosntitutes random draws from a population then these draws must be with replacement as otherwise the probabilities would shift. Suppose that we are trying to draw the ace of apdes from a standard, shuffled deck of \\(52\\) cards. If we begin drawing cards without returning them to the deck after each draw, the probability that the next draw is the ace of spades is increasing over the draws. As a result, this type of scenario does not fit into the independent and identically distributed Bernoulli trials that we have been exploring.\nInstead, suppose that we are drawing without replacement from a finite population. The population consists of two types of items: successes and failures. If we are interested in counting how many successes we seein a set number of draws, then this random quantity will follow a hypergeometric distribution. The hypergeometric distribution is parameterized using three different parameters: the number of items in the population, \\(N\\), the number of these which are considered successes, \\(M\\), adn the total number of items that are to be drawn without replacement, \\(n\\). We write \\(X\\sim\\text{HG}(N,M,n)\\).\nTODO: Basic example\nIf \\(X\\sim\\text{HG}(N,M,n)\\) then \\[p_X(x) = \\begin{cases}\\frac{\\binom{N-M}{n-x}\\binom{M}{x}}{\\binom{N}{n}} & x\\in\\{\\max\\{0,N-M+n\\},\\dots,\\min\\{n,M\\}\\} \\\\ 0 &\\text{otherwise}.\\end{cases}\\] Moreover, \\(E[X] = \\frac{nM}{N}\\) and the variance is given by \\[\\text{var}(X) = n\\frac{M}{N}\\frac{N-M}{N}\\frac{N-n}{N-1}.\\]\nTODO: Bigger calculation.\nThe hypergeometric is closely linked to the binomial distribution. If we consider the population described in the hypergeometric setup then the probability of a success on the first draw is \\(p=\\frac{M}{N}\\). Note that \\(E[X] = np\\), exactly the same as in the binomial. However, plugging this in fro the variance we get \\(\\text{var}(X) = np(1-p)\\frac{N-n}{N-1}\\). Notice that if \\(n=1\\), this extra term is simply \\(1\\), and for \\(n &gt; 1\\) it will be less than \\(1\\). As a result, the variance of the hypergeometric is smaller than the variance of the corresponding binomial. This makes intuitive sense: in a hypergeometric, the fact that draws are without replacement means that as more draws go on the probability of observing a success increases, reducing the likelihood of long runs of no observed successes. There is a cap on the behaviour of the random quantity thanks to the finiteness of the population. Correspondingly, the multiplicative term by which the varaince shrinks, \\(\\frac{N-n}{N-1}\\) is referred to as the finite population correction and it differentiates the behaviour of the hypergeometric and the binomial.\nNote that as \\(N\\) becomes very, very large, as long as \\(n\\) is small by comparison, the finite population correction will approach \\(1\\). in other words: drawing without replacement in a large enough sample behaves almost exactly the same as drawing with replacement in the sample. The binomial distribution can be used to approximate hypergeometric distributions, so long as the population is very large. Again, this makes sense intuitively. If you have a deck with a million cards in it, and you are going to draw \\(2\\), whether or not you return the first one to the deck has very little bearing on the probabilities associated with this scenario. Generally, the binomial distribution is easier to work with, so this approximation can be useful in some settings.\nTODO: aside on this for sampling.\nTODO: Many examples of HG random variables.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Named Discrete Distributtions</span>"
    ]
  },
  {
    "objectID": "notes/chapter6.html#poisson-distribution",
    "href": "notes/chapter6.html#poisson-distribution",
    "title": "6  The Named Discrete Distributtions",
    "section": "6.4 Poisson Distribution",
    "text": "6.4 Poisson Distribution\nThe hypergeometric broke from the pattern in the other distributions that have been discussed by not being represented as the sequence of several Bernoulli random trials. However, it was still characterized by a sequence of repeated trials. While many statistical experiments can be framed in this way, there are of course processes which do not fit well into this framing.\nConsider, for instance, any process where something is observed for a set period of times and events may or may not occur during this interval. Perhaps you sit on the side of the road and count the number of cars travelling by a particular intersection over the course of an hour. Each car going by is an event, but in this setting the number of events is the random quantity itself. None of the distributions discussed until this point are suited to this type of process.\nWhen we have events which occur at a constant rate, and our interest is in the number of events which are occuring, then we can make use of the Poisson distribution. The Poisson distribution takes a single parameter, \\(\\lambda\\), which is the average rate of occurence of the events over the time period we are interest in. We write \\(X\\sim\\text{Poi}(\\lambda)\\).\nTODO: Basic example.\nIf \\(X\\sim\\text{Poi}(\\lambda)\\) then \\[p_X(x) = \\begin{cases} \\frac{e^{-\\lambda}\\lambda^x}{x!} & x \\geq 0 \\\\ 0 &\\text{otherwise}.\\end{cases}\\] Moreover, \\(E[X]=\\lambda\\) and \\(\\text{var}(X) = \\lambda\\). The Poisson distribution is interesting in that the mean and variance are always equal to one another.\nTODO: Bigger example\nWhile the most common applications for the Poisson distribution have to do with the occurences of events throughout time, it is also possible to view this as the occurences of events throughout space. For instance, if there is a manufcturer producing rope, then the number of defects in a set quantity of rope is likely to follow the Poisson distribution. Similarly, in a set geographic area, the number of birds of a particular species is likely to follow a Poisson distribution. For the Poisson distribution, we are typically thinking that there is a rate at which events of interest occur, and we can use the Poisson to model the total number of occurences over some specified interval.\nTODO: many examples for the use of Poisson.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Named Discrete Distributtions</span>"
    ]
  },
  {
    "objectID": "notes/chapter6.html#using-named-distributions",
    "href": "notes/chapter6.html#using-named-distributions",
    "title": "6  The Named Discrete Distributtions",
    "section": "6.5 Using Named Distributions",
    "text": "6.5 Using Named Distributions\nWhile many other named, discrete distributions exist, these are likely the most common. When confronted with a problem in the real-world for which you wish to understand the uncertainty associated with it, a reasonable first step is to determine whether a named distribution is well-suited to representing the underlying phenomenon. Is it a situation with enumerated events which are equally likely? Use the discrete uniform. Is it a binary outcome? Use the Bernoulli. Are you counting the number of success in a fixed number of trials? Use the binomial. Are you running repeated trials until a (certain number of) success(es)? Use the geomtric (negative binomial). Are you sampling without replacement? Use the hyper geometric. Are you counting events over a fixed space? Use the Poisson.\nOnce identified, the distribution can be used in exactly the same way as any probability mass function. That is, we still require all of the probability rules, event descriptions, and techniques from before. The difference in these cases is that we immediately have access to the correct form of the probability mass function, the expected value, and the variance.\nAn additional utility with this approach to solving probability questions is that, over time and repeated practice, you can build-up an intuition as to the behaviour of random variables following these various distributions. Probabilities in general are deeply unintuitive: it can be hard to assess, without fomrally working it out, whether an evetn is likely or unlikely, let alone how likey any event is. However, the lack of intuition from our wider experience can be negated almost entirely by building of intuition through the repeated application of these distributions. You can start to gain a sense of how binomial random variables behave, being able to determine just from inspection whether events seem plausible or not. Much of the study of probability and statistics is about building a set of tools that can overcome the flaws in our intuitive reasoning regarding uncertainty. This comes only through practice, however, this framework of named distributions provides a very solid foundation to perform such practice.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Named Discrete Distributtions</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html",
    "href": "notes/chapter7.html",
    "title": "7  Continuous Random Variables",
    "section": "",
    "text": "8 The Normal Distribution\nThe normal distribution, also sometimes referred toas the Gaussian distribution, is a named continuous distribution function defined on the complete real line. The distribution is far and away the most prominently used distribution in all of probability and statistics. In fact, most people have heard of normal distributions even if they are not aware of this fact. Any time that there is a discussion of a bell curve, for instance, this is in reference to the normal distribution. Normally distributed quantities arise all over the place from measurements of heights, grades, or reaction times through to levels of job satisfaction, reading ability, or blood pressure. There is a tremendous number of normally distributed phenomena naturally occurring in the world, which renders the normal distribution deeply important across a wide range of domains.\nPerhaps more important than the places where the normal distribution arises in nature are hte places where it arises mathematically. At the end of this course we will see a result, the central limit theorem, which is one of the core results in all of statistics. Most of the statistical theory that drives scentific inquiry sits ontop of the central limit theorem, and at the core of the central limit theorem is the normal distribution. It is virtually impossible to overstate the importance of the normal distribution, and as a result, we will spend a great deal of time investigating it.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#continuous-versus-discrete",
    "href": "notes/chapter7.html#continuous-versus-discrete",
    "title": "7  Continuous Random Variables",
    "section": "7.1 Continuous Versus Discrete",
    "text": "7.1 Continuous Versus Discrete\nDistinguishing whether a random quantity is continuous or discrete is crucial as, broadly speaking, the two types of quantities are treated differently. The same underlying ideas are present, but the distinctions between the two settings require some careful thought. As a general rule, the use of continuous random variables necessitates an understanding of introductory calculus. This is not a pre-requisite for this course, and as a result, we will not focus as deeply on working with the underlying quantities. However, continuous random variables are also the dominant type of random variables outside of introductory courses. As a result, understanding the distinctions, and beginning to become familair with how they are to be manipulated is an important skill.\nThe key difference between discrete and continuous random variables is that, for discrete random variables the beahviour is governed by assessing \\(P(X=x)\\) for all possible values of \\(x\\), and for continuous random variables \\(P(X=x)=0\\) for every value of \\(x\\). This is a surprising statement, and as such it is worth reiterating. With discrete random variables we discussed how all of the probabalistic behaviour was governed by the probability mass function which was defined as \\(p_X(x) = P(X=x)\\). If \\(X\\) is continuous, it will aalways be the case that \\(P(X=x) = 0\\). Correspondingly, continuous random variables do not have probability mass functions, and to understand the behaviour of these random variables we must turn to other quantities.\nWhile the fact that \\(P(X=x) = 0\\) may seem unintuitive at first glance, it is worth exploring this even further. The nature of a continuous random variable is such that there is no possible way to enumerate all of the values which are possible to be realized by the random quantity. Suppose that we take a set of countably many possible observations and gave each of these a probability of greater than \\(0\\) of occuring directly. Even if we take an infinite number of them, there will still be an uncountably infinite number of events in the sample space which we have not accounted for. We know that the total probability of the sample space must be \\(1\\), and so we must have the total probability of the first set of events being less than \\(1\\) (if it summed to \\(1\\) then this would not be a continuous random variable, since there would only be a countable number of possible events). Now suppose we take another set of countably many events, again giving each of them a positive probability. Once more the sum of all of these probabilities must be less than \\(1\\), and specifically, the sum of both sets must also be less than \\(1\\). Even after these two sets there are still uncountably infinite events to go, and so we continue this process. Because we always need the total probability to be \\(1\\) once all events have been accounted for, and because we will always have an uncountably infinite number of events left to account for, we can never have a positive probability assigned to each event in a set of events. Even if we made the probability of each of these sets very, very small (say \\(1\\) in a million) after some fixed number of countable eevents the probabilities would be greater than \\(1\\) which cannot happen. As such, each event itself must have \\(0\\) probability.\nAn alternative technique for understanding this intuition is to think about how uunlikely it really would be to observe any specific value. Suppose that \\(X\\) takes values on the interval \\([0,1]\\). Recall that when we defined probabilities, we discussed them as being the long run proportion of time that an event occurs. Take some event, say \\(X=0.5\\). Suppose that we took repeated measurements of \\(X\\) which are independent and identically distributed. Now suppose that at some point we exactly do observe \\(X=0.5\\). Should we expect that this will ever happen again? The next time we get near \\(0.5\\) might we instead not observe \\(0.51\\) or \\(0.49\\) or \\(0.5000000000000000001\\) or any of the other uncountably infinite values in the very near vicinity of \\(0.5\\)? Each time that we make an observation the denominator of our proportion is growing, but if every value between \\(0\\) and \\(1\\) is truly possible, as time goes on the number of times that \\(X=0.5\\) must stay much, much smaller than the total number of trials. If we continue this off to infinity, in the limit, the probability must become \\(0\\).\nThis conclusion leads to a few different points. First, impossibility is not the same as probability \\(0\\). Impossible events do have probability \\(0\\), but possible events may also have probability \\(0\\). Events which are outside of the sample space are impossible. Events inside the sample space, even probability zero events, remain possible. Second, we require alternative mathematical tools for discussing the probability of events in a continuous setting. Ideally this would be analogous to a probability mass function, but would somehow function in the case of continuity.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#cumulative-distribution-functions",
    "href": "notes/chapter7.html#cumulative-distribution-functions",
    "title": "7  Continuous Random Variables",
    "section": "7.2 Cumulative Distribution Functions",
    "text": "7.2 Cumulative Distribution Functions\nTo begin building up to the continuous analogue to the probability mass function, we will start by focusing on events that are easier to define in the continuous case. Suppose that \\(X\\) is defined on some continuous interval. Instead of thinking of events relating to \\(X=x\\), we instead turn our focus to events of the form \\(X \\in (a,b)\\) for some interval defined by the endpoints \\(a\\) and \\(b\\). Now note that, relying only on our knowledge of probabilities relating to generic events, we can rewrite \\(P(X\\in(a,b))\\) slightly. Specifically, \\[\\begin{align*}\nP(X\\in(a,b)) &= 1 - P(X\\not\\in(a,b))\\\\\n&= 1 - P(\\{X &lt; a\\}\\cup\\{X &gt; b\\}) \\\\\n&= 1 - \\left(P(X &lt; a) + P(X &gt; b)\\right)\\\\\n&= 1 - P(X &gt; b) - P(X &lt; a)\\\\\n&= P(X &lt; b) - P(X &lt; a).\\end{align*}\\]\nTODO: diagram showing this graphically.\nIn words we know that the probability that \\(X\\) falls into any particular interval is given by the probability that it is less than the upper bound of the interval minus the probability that it is less than the lower bound of the interval. Notice that \\(X &lt; a\\) is an event, and if we knew how to assign probabilities to \\(X&lt;a\\) for arbitrary \\(a\\), then we could assign probabilities to any interval. Also note that, even in the continuous case, it make sense to talk of \\(P(X &lt; a)\\) for some value \\(a\\). These intervals will contain an uncountably infinite number of events, and as such, can certainly occur with greater than \\(0\\) probability. Using our common example of \\(X\\) being defined on \\([0,1]\\), then \\(P(X&lt;1)=1\\). Note that we could have written \\(P(X \\leq 1) = 1\\), which may have been more obviously true. However, \\(P(X\\leq 1) = P(\\{X&lt;1\\}\\cup\\{X=1\\}) = P(X&lt;1) + P(X=1)\\) and we know that \\(P(X=1)=0\\). In the continuous case we do not need to worry whether we use \\(X\\leq a\\) or \\(X &lt; a\\), and we will interchange them throughout.\nTODO: uniform example\nThe centrality of events of the form \\(X&lt;a\\) prompts the definition of a mathematical function which we call the cumulative distribution function. We will typically denote the cumulative distribution function of a random variable \\(X\\) as \\(F(x)\\), sometimes using \\(F_X(x)\\) to emphasize that this function relates to \\(X\\) specifically. We may also refer to the cumulative distribution function simply as the distribution function. By definition, we take \\(F_X(x) = P(X\\leq x)\\). Once we have defined the distribution function for a random variable, using the above derivation we are able to determine the probability associated with any events based on intervals.\nIt is worth noting that the cumulative distribution function can also be defined for discrete random variables. In the case of a discrete random variable, we would have \\[F_X(x) = \\sum_{k \\in\\mathcal{X}; k \\leq x} p_X(k).\\] Since it is simply the summation of the probability mass function it tends to be a less useful quantity. Still, the cumulative distribution functions for discrete random variables do come up on ocassion, and it is worth recognizing that they are defined in exactly the same way.\nTODO: Include example with the CDF.\nSupposing that, for some continuous random variable \\(X\\) we have the cumulative distribution function, then thisknowledge actually permits us to compute any probability associated with the random variable that we can want. Consider any event associated with \\(X\\) which we may wish to determine the probability of. We know that events are merely subsets of the sample space. Every one of these events can be written using our basic set operations (unions, intersections, and complements) applied to intervals of the form \\((a,b)\\) and singelton sets of the form \\(\\{x\\}\\). Our basic axioms of probability allow us to compute probabilities across the set operations, and our knowledge of the cumulative distribution, the conversion of \\(P(X \\in (a,b)) = F_X(b) - F_X(a)\\), and the fact that \\(P(X=0) = 0\\) gives all of the results we need to derive probabilities for these events.\nTODO: Include simple examples",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#the-probability-density-function",
    "href": "notes/chapter7.html#the-probability-density-function",
    "title": "7  Continuous Random Variables",
    "section": "7.3 The Probability Density Function",
    "text": "7.3 The Probability Density Function\nThe distribution function will be the core object used to discuss the probabalistic behaviour of a continuous random variable. All of the beahviour of these random quantities will be described by the distribution function, and as such we will take the distribution function as a function which defines the distribution of a continuous random quantity. This is all that we need in orderto analyze the probabalistic behaviour of these random variables, however, it may be a little unsatisfying in contrast with the discrete case.\nWe had set out to find a quantity which was a parllel to the probability mass function, and instead concluded that the cumulative distribution function can eb made to play the same role in terms of describing the behaviour of the random quantity. Still, it may be of interest for us to have a function which takes into account the relative likelihood of being near some value. Suppose, for instance, that for a random variable defined on \\([0,1]\\) we wanted to know how likely it was to be in the vicinity of \\(X=0.5\\). We could take a small number, say \\(\\delta = 0.01\\) and calculate \\(P(X\\in(0.5-\\delta,0.5+\\delta)) = F(0.5+\\delta)-F(0.5-\\delta)\\). This is perfectly well defined based on our discussions to this point. Now, if we assume that the probability isfairly evenly distributed throughout this interval, then if we wanted to assign a likelihood to each value we could divide this total probability by hte length of the interval, which is \\(2\\delta\\). As a result, we are saying that the probability that \\(X\\) is nearly \\(0.5\\) will be approximately given by the expression \\(\\frac{F(0.5+\\delta)-F(0.5-\\delta)}{2\\delta}\\).\nWe had taken \\(\\delta=0.01\\), but the same process could be applied for smaller and smaller \\(\\delta\\), say \\(0.001\\) or \\(0.0001\\). Intuitively, as the size of this itnerval shrinks more and more we are getting a better and better estimate for the likelihood that the random variable is in the immediate vicinity of \\(0.5\\). Moreover, as \\(\\delta\\) gets smaller and smaller our assumption of a uniform probability over the interval becomes more and more reasonable. Now,, we cannot set \\(\\delta=0\\) exactly, however, we can ask what happens in the limit as \\(\\delta\\) continues to get smaller and smalller. This question is in the purvue of calculus, and can in fact be answered. While working out the answer is beyond the scope of the course, we will provide the result anyway.\nThe resulting function is called the probability density function, and is related to the cumulative distribution function through derivatives (and integrals). If a random variable \\(X\\) has a cumulative distribution function, \\(F(x)\\), we typically denote the corresponding density function as \\(f(x)\\). The density function also describes the behaviour of the random variable, and mirrors the behaviour of the probability mass function in the discrete case. Roughly speaking, the density function evaluates how likely (relatively speaking) it is for a continuous quantity to be in a small neighbourhood of the given value. Critically, probability density functions do not give probabilities directly. In fact, probability density functions may give values that are greater than \\(1\\)!\nTODO: Example of uniform PDF.\nStill, if we see the shape of the probability density function, we can state how likely it is to make observations near the results of interest. We will often graph the density function. The high points of the graph indicate regions with more probability than the regions of the graph which are lower. Again, the specific probability of any event \\(X=x\\) will always be \\(0\\), but some events fall in neighbourhoods which are more likely to observe than others.\nTODO: include examples.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#using-continuous-distributions",
    "href": "notes/chapter7.html#using-continuous-distributions",
    "title": "7  Continuous Random Variables",
    "section": "7.4 Using Continuous Distributions",
    "text": "7.4 Using Continuous Distributions\nWith the exception of the differences indicated until this point, there is otherwise not much difference between continuous and discrete random variables. The tools to analyze them differ (in the continuous case, we cannot sum over the sample space, and so we must use techniques from calculus to mirror this process, for instance), but the fundamentals remain the same. It is still possible to compute expected values (and medians and modes) with roughly the same interpretations. It is still possible to describe the range, interquartile range, and variance, again with corresponding interpretations. The axioms of probability still underpin the manipulation and analysis of these random variables. The distinction is merely that in place of elementary mathematics to complete the calculations, calculus is required.\nJust as with discrete distributions, there are continuous named distributions. These are typically governed by either a density function or else a distribution function, alongside the expected value and variance. And just like the named discrete distributions, by matching the underlying scenario to the correct process, we are able to side step a lot of work in understanding the beahviour of the random quantities. Now, because there is no assumed knowledge of calculus, we will not work too widely with continuous distributions. We will introduce only two named continuosu distributions: the uniform distribution, which we have already started to see, and the normal distribution, which is far and away the most important distribution (discrete or continuous) in all of probability and statistics.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#the-uniform-distribution",
    "href": "notes/chapter7.html#the-uniform-distribution",
    "title": "7  Continuous Random Variables",
    "section": "7.5 The Uniform Distribution",
    "text": "7.5 The Uniform Distribution\nThe uniform distribution, sometimes called the continuous uniform distribution to distinguish it from the discrete counterpart, is parameterized over a set interval specified as \\((a,b)\\). On this interval, equal probability density is given to every event, which is to say that the density function is constant. Specifically, for \\(X\\sim\\text{Unif}(a,b)\\) we note that \\[f(x) = \\begin{cases}\\frac{1}{b-a} & x \\in (a,b) \\\\ 0 & \\text{otherwise}.\\end{cases}\\] From the density function we can work out that \\[F(x) = \\frac{x - a}{b - a}\\] for \\(x \\in (a,b)\\), with \\(F(x) = 0\\) for \\(x &lt; a\\) and \\(F(x) = 1\\) for \\(x &gt; b\\). Moreover, we have \\(E[X] = \\frac{a+b}{2}\\) and \\(\\text{var}(X) = \\frac{(b-a)^2}{12}\\).\nTODO: Include some calculations\nThe uniform distribution is analogous to the discrete uniform. Any time there is an interval of possible outcomes which are all equally likely, the uniform distribution is the distribution to use. Compared with other distributions it is also fairly straightforward to work with, which makes it a useful demonstration of the concepts relating the continuous probability calculations.\nTODO: Include examples.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#the-specification-of-the-distribution",
    "href": "notes/chapter7.html#the-specification-of-the-distribution",
    "title": "7  Continuous Random Variables",
    "section": "8.1 The Specification of the Distribution",
    "text": "8.1 The Specification of the Distribution\nA normal distribution is parameterized by two different parameters: the mean, \\(\\mu\\), and the varaince \\(\\sigma^2\\). We write \\(X\\sim N(\\mu,\n\\sigma^2)\\). These parameters directly correspond to the relevant quantities such that \\(E[X] =  \\mu\\) and \\(\\text{var}(X) = \\sigma^2\\). The density function is given by \\[f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\\] This can be quite unwieldy to work with, however, when it is plotted we ssee that the normal distribution takes on a bell curve which is centered at \\(\\mu\\).\nTODO: include plots",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#the-standard-normal-distribution",
    "href": "notes/chapter7.html#the-standard-normal-distribution",
    "title": "7  Continuous Random Variables",
    "section": "8.2 The Standard Normal Distribution",
    "text": "8.2 The Standard Normal Distribution\nNormally distributed random variables are particularly well-behaved. One way in which this is true is that if you multiply a normally distributed random variable by a constant, it will remain normally distributed, and if you add a constant to a normally distributed random variable, it will remain normally distributed. Consider then if \\(X\\sim N(\\mu,\\sigma^2)\\), taking the quantity \\(X - \\mu\\). We have seen in our discussions of expected values that \\(E[X-\\mu] = E[X]-\\mu = 0\\). Furthermore, adding or subtracting a constant will not change the variance. Thus, \\(X-\\mu\\sim N(0,\\sigma^2)\\).\nNow, consider dividing this by \\(\\sigma\\), or equivalently, multiply by \\(\\frac{1}{\\sigma}\\). The expected value of the new quantity will be \\(\\frac{1}{\\sigma}\\times 0 = 0\\), and the variance of the newquantity will be \\(\\frac{1}{\\sigma^2}\\times\\sigma^2 = 1\\). Taken together then, if \\(X\\sim N(\\mu,\\sigma^2)\\) then \\[Z = \\frac{X - \\mu}{\\sigma} \\sim N(0,1).\\] This holds true for any starting normal distribution, with any mean or variance values. As a result, this straightforward transformation allows us to discuss any normal distribution in terms of \\(N(0,1)\\). We call this the standard normal distribution, and will typically use \\(Z\\) to denote a random variable from the standard normal distriubtion.\nTODO: Include example\nIf \\(Z\\sim N(0,1)\\), then we use a special notation for the density function and distribution function of \\(Z\\). Specifically, we take the density function to be denoted \\(\\text{var}phi(z)\\) and the cumulative distribution function to be given by \\(P(Z \\leq z) = \\Phi(z)\\). The cumulative distribution function does not have a nice form to be written down, however, it is a commonly applied enough function that many computing languages have implemented it, including of course R.\nTODO: Demonstration of using *norm.\nThe utility in this is demonstrated by realizing that events can be converted using the same transformations. Specifically, suppose we have \\(X \\sim N(\\mu,\\sigma^2)\\), and we want to find \\(P(X \\leq x)\\). Note that, \\(X \\leq x\\) must also mean that \\[\\frac{X-\\mu}{\\sigma} \\leq \\frac{x - \\mu}{\\sigma},\\] simply by applying the same transformation to both sides. But we know that the left hand side of this inequality is exactly \\(Z\\), a standard normal random variable with cumulative distribution function \\(\\Phi(z)\\). Thus, \\[P(X \\leq x) = P\\left(Z \\leq \\frac{x - \\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right).\\] Using this trick of standardization any normal probability can be converted into a probability regarding the standard normal, for which we can easily use computer software.\nTODO: Include normal calculation TODO: Include discussion of R calculating normal probabilities\nAs a result, combining our knowledge of continuous random variables, with the process of standardization we are able to calculate normal probabilities for any events relating to normally distributed random quantities. Moreover, since the shape of the normal distribution is so predictable, it is often easy to draw out the density function, and indicate on this graphic the probabilities of interest, which in turn helps with the required probability calculations. Calculating probabilities from normal distributions will remain a central component of working with statistics and probabilities beyond this course. Developing the skills and intuition at this point, through repeated practice is a key step in successfully navigating statistics here and beyond.\nTODO: another example.\nWhen you have access to a computer, and your interest is in calculating a normal probability, as described above, there is not properly a need for standardization. However, it rmeains an important skill for several reasons. First, by always working with the same normal distribution, you will develop a much more refined intuition for the likelihoods of different events. It goes beyond working with the same family of distributions, you get very used to working with exactly the same distribution. Second, you will likely become quite familiar with certain key critical values of the standard normal distribution. These values arise frequently, and allow you to quickly approximate the likelihood of different events. Finally, as we begin to move away from studying probability and into studying statistics, the standard normal will feature prominently there.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#the-empirical-rule",
    "href": "notes/chapter7.html#the-empirical-rule",
    "title": "7  Continuous Random Variables",
    "section": "8.3 The Empirical Rule",
    "text": "8.3 The Empirical Rule\nAnother way in which the normal distribution is well behaved is summarized in the emprical rule. The shape of the distribution is such that, no matter the specific mean or variance, all members of the family remain quite similar. This enables the derivation of an easy, approximate result, to help intuitively gauge the probabilities of normal events. The emprical rule states that, if \\(X\\) has a normal distribution, then the probability of observing a value within \\(\\sigma\\) or the mean is approximately \\(0.68\\), the probability of observing a value within \\(2\\sigma\\) of the mean is approximately \\(0.95\\), and the probability of observing a value within \\(3\\sigma\\) of the mean is approximately \\(0.997\\).\nTODO: Include graphic\nIn words, the empirical simply states that almost all of the observations from a normal distribution will fall within \\(\\mu\\pm3\\sigma\\). In mathematical terms, the empirical rule is summarized as \\(P(\\mu-\\sigma\\leq X \\leq \\mu + \\sigma) \\approx 0.68\\), \\(P(\\mu - 2\\sigma \\leq X \\leq \\mu + 2\\sigma) \\approx 0.95\\), and \\(P(\\mu - 3\\sigma \\leq X \\leq \\mu + 3\\sigma) \\approx 0.997\\). With the standard normal we can replace \\(\\mu\\) with \\(0\\), and \\(\\sigma\\) with \\(1\\) to get a version which is slightly more concise to state. It is then possible to combine these different intervals by recognizing the symmetry in the normal distribution. That is, \\(P(\\mu \\leq X \\leq \\mu + \\sigma) \\approx \\frac{0.68}{2} = 0.34\\).\nTODO: Calculations with the empirical rule.\nThe empirical rule is not exact, and again, when computing probabilities with access to statistical software, it is likely of limiteddirect utility. However, it is another tool to leverage to continue developing a refined intuition for the behaviour of random quantities. It is also a good check to have a sense of the likelihood of different events. If you compute an answer which seems out of line with the empirical rule, take a look moreclosely. If you have someone tell you that they have observed events which are out of line with the empirical rule, be skeptical.\n\n8.3.1 Chebyshev’s Inequality\nThe empirical rule is a useful result to aid in building intuition regarding the normal distribution. However, when quantitiesare not normally distributed, we cannot use the empirical rule. A related, though somewhat weaker result, does hold for any distribution, and it is a useful extension to the empirical rule. Stated in words, Chebyshev’s inequality says that there is a probability of \\(0.75\\) or more of observing an observation within two standard deviations, and a probability of at least \\(0.8889\\) of observing a value within three standard deviations of the mean.\nFormally, we can write that \\[P(\\mu - k\\sigma \\leq X \\leq \\mu + k\\sigma) \\geq 1 - \\frac{1}{k^2}.\\] Here \\(k\\) can be any real number which is greater than \\(0\\). If \\(k\\leq 1\\), this result is uninteresting since the bound simply is \\(0\\). However, taking \\(k=2\\) gives the \\(0.75\\) lower bound outlined above, which is a more useful result. Additionally, there is no requirement for \\(k\\) to be an integer here, and so, for instance, the probability of observing a value within \\(\\mu\\pm\\sqrt{2}\\sigma\\) is at least \\(0.5\\), for all distributions.\nTODO: Include some examples.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#closure-of-the-normal-distribution",
    "href": "notes/chapter7.html#closure-of-the-normal-distribution",
    "title": "7  Continuous Random Variables",
    "section": "8.4 Closure of the Normal Distribution",
    "text": "8.4 Closure of the Normal Distribution\nWe have seen a certain type of “closure property” for the normal distribution when we discussed standardization. That is, adding and multiplying by constants does not change the distribution when working with normally distributed quantities. This is an interesting property which does not hold for most distributions, and makes normally distributed random variables quite nice to work with. The normal distribution has an addition type of closure property which is frequently used, and is also somewhat surprising.\nSuppose that \\(x\\) and \\(Y\\) are independent of one another, with \\(X\\sim N(\\mu_X, \\sigma_X^2)\\), and \\(Y\\sim N(\\mu_Y, \\sigma_Y^2)\\). In this setting, \\[X+Y\\sim N(\\mu_X+\\mu_Y, \\sigma_X^2 + \\sigma_Y^2).\\] in words, the addition of two independent normally distributed random variables will also be normally distributed. This extends beyond two in the natural way, simply by applying and reapplying the rule (as many times as is required).\nTODO: Include example of this.\nThis becomes particularly useful when we think of generating many iid realizations in an experiment from a normal population. For instance, if \\(X_1,\\dots,X_n\\) are all iid from a \\(N(\\mu,\\sigma^2)\\) distribution, then \\[\\sum_{i=1}^n X_i \\sim N(n\\mu, n\\sigma^2).\\] If instead we consider the average of these \\(n\\) independent variables, then \\[\\frac{1}{n}\\sum_{i=1}^n X_i \\sim N(\\mu, \\frac{\\sigma^2}{n}),\\] through an application of our standard expectation and variance transformation rules. This type of result is central to the practice of statistics, and this closure under addition further aids in the utility of the normal distribution.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  },
  {
    "objectID": "notes/chapter7.html#normal-approximations",
    "href": "notes/chapter7.html#normal-approximations",
    "title": "7  Continuous Random Variables",
    "section": "8.5 Normal Approximations",
    "text": "8.5 Normal Approximations\nA final utility to the normal distribution is in its ability to approximate other distributions. While several of these approximations exist, we will focus on the normal approximation to the binomial as an illustrative example. Historically, these approximations were critical for computing probabilities by hand in a timely fashion. Owing to the widespread use of statistical software, these usecases are more and more limited, which removes the necessity of these approximations directly. However, there are two major advantages to learning these approximations. First, with an approximation it becomes easier to leverage the intuition you will build regarding the normal distribution in order to better understand the behaviour of other random quantities. Second, the normal approximation has the same “flavour” as many results in statistics, and so it presents an additional path to familiarity with these types of findings.\nSuppose that \\(X\\sim\\text{Bin}(n,p)\\). Through knowledge of the binomial distribution, we know that \\(E[X] = np\\) and \\(\\text{var}(X) = np(1-p)\\). If \\(n\\) is sufficiently large then it is possible to approximate a binomial distribution using a normal distribution with the corresponding mean and variance. That is, for \\(n\\) large enough, we can take \\(X\\sim\\text{Bin}(n,p)\\) to have approximately the same distribution as \\(W\\sim N(np, np(1-p))\\).\nTODO: Brief example.\nOne consideration that we need to make when applying this approximation has to do with the fact that the normal distribution is continuous while the binomial distribution is discrete. As a result, the normal distribution can take on any value on the real line, where the binomial is limited to the integers. A question that we must answer is what to do with the non-integer valued numbers. The natural solution is to rely on rounding. That is, for any value between \\([1.5, 2.5)\\) we would round to the nearest integer, which is \\(2\\).\nThis natural solution is in fact a fairly useful technique, and it is the one that we will make use of in the normal approximation. While rounding is quite natural, the process for leveraging this idea in probability approximation is somewhat backwards. That is, we typically will need to go from probabilities relating to \\(X\\) and transform those into probabilities relating to \\(W\\). So, for instance, if we wish to know \\(P(X \\leq 2)\\), then we need to be able to make this a statement regarding the random variable \\(W\\). In order to do this we need to ask “what is the largest value for \\(W\\) that would get rounded to \\(2\\)?” The answer is \\(2.5\\) and so \\(P(X \\leq 2) \\approx P(W \\leq 2.5)\\).\nA similar adjustment would be required if we instead wanted \\(P(X \\geq 5)\\). Here we would ask “what is the smallest value for \\(W\\) which would get rounded to \\(5\\)?” and note that the answer is \\(4.5\\). Thus, \\(P(X \\geq 5) \\approx P(W \\geq 4.5)\\). Once we have expressed the probability of interest in terms of the normla random variable, we can use the standard techniques previously outlined to compute the relevant probabilities.\nTODO: Examples\nIt is very important to keep in mind that the two results discussed above were of the form \\(X \\geq x\\) and \\(X \\leq x'\\). If we instead had considered \\(X &gt; x\\) or \\(X &lt; x'\\), we would need to take an additional step. For continuous random variables whether \\(X \\geq x\\) or \\(X &gt; x\\) is considered it makes no difference. However, for discrete random variables this is not the case. As a result we should first convert the event the an equivalent event which contains the equality sign within the inequality, and then apply the continuity correction. (TODO: ensure that continuity correction was referenced before). That is, if we want \\(P(X &gt; 3)\\) first note that for \\(X &gt; 3\\) to hold, we could equivalently write this as \\(X \\geq 4\\). Alternatively, if the event of interest is \\(X &lt; 8\\), this is the same as \\(X \\leq 7\\).\nTODO: Further examples.\nWhen it is not necessary, it rarely makes sense to use an approximation. There will be cases where the approximation is directly useful, and in those moments it is great to be able to use it. This example of using the normal distribution to approximate a discrete random variable serves as a nice bridge from the study of probability to the study of statistics. In statistics we take a different view of the types of problems we have been considering to date, and we require the tools of probability that have been brought forth. As a result, a deep comfort with manipulating probability expressions is required to build a strong foundation while studying statistics.",
    "crumbs": [
      "Part 1: Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Random Variables</span>"
    ]
  }
]
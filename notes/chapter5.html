<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.533">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 1793: Course Notes - 5&nbsp; The Named Discrete Distributtions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/chapter6.html" rel="next">
<link href="../notes/chapter4.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "classic",
  "enableExperimentalNewNoteButton": true,
  "showHighlights": "whenSidebarOpen"
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/chapter1.html">Part 1: Probability</a></li><li class="breadcrumb-item"><a href="../notes/chapter5.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Named Discrete Distributtions</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">STAT 1793: Course Notes</a> 
        <div class="sidebar-tools-main">
    <a href="../STAT-1793--Course-Notes.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Part 1: Probability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Core Concepts of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Probailities with More than One Event</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter5.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Named Discrete Distributtions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Continuous Random Variables</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Part 2: Statistics</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-binomial-distribution" id="toc-the-binomial-distribution" class="nav-link active" data-scroll-target="#the-binomial-distribution"><span class="header-section-number">5.0.1</span> The Binomial Distribution</a></li>
  <li><a href="#geometric-random-variables" id="toc-geometric-random-variables" class="nav-link" data-scroll-target="#geometric-random-variables"><span class="header-section-number">5.1</span> Geometric Random Variables</a></li>
  <li><a href="#negative-binomial" id="toc-negative-binomial" class="nav-link" data-scroll-target="#negative-binomial"><span class="header-section-number">5.2</span> Negative Binomial</a></li>
  <li><a href="#hypergeometric-distributions" id="toc-hypergeometric-distributions" class="nav-link" data-scroll-target="#hypergeometric-distributions"><span class="header-section-number">5.3</span> Hypergeometric Distributions</a></li>
  <li><a href="#poisson-distribution" id="toc-poisson-distribution" class="nav-link" data-scroll-target="#poisson-distribution"><span class="header-section-number">5.4</span> Poisson Distribution</a></li>
  <li><a href="#using-named-distributions" id="toc-using-named-distributions" class="nav-link" data-scroll-target="#using-named-distributions"><span class="header-section-number">5.5</span> Using Named Distributions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/chapter1.html">Part 1: Probability</a></li><li class="breadcrumb-item"><a href="../notes/chapter5.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Named Discrete Distributtions</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Named Discrete Distributtions</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>So far, our discussion of probability distributions and their summaries has centered on general results for arbitrary probability mass functions. The basic premise has been that, by knowing a probability mass function, you are able to understand the complete behaviour of a random quantity. Directly from this mass function we are able to derive summaries for the behaviour, for instance describing the location and variability of the random variable. In short by knowing the probability mass function (and to a lesser extent, the expected value and variance), we immediately understand how the random variable behaves. We have not, however, spent much time discussing where the probability mass functions come from.</p>
<p>We have, indirectly, seen one fairly general probability mass function, the one deriving from the equally likely outcomes model. This probability mass function is completely defined by the set of possible values that the random variable can take on. Suppose that we restrict our attention to the sample space being a set of <span class="math inline">\(k\)</span> integers from <span class="math inline">\(a\)</span> through to <span class="math inline">\(a+k\)</span>. Note that this assumption is not actually restrictive: if you have any <span class="math inline">\(k\)</span> items you can simply label each of the items one of the numbers between <span class="math inline">\(1\)</span> and <span class="math inline">\(k\)</span> and take <span class="math inline">\(a=1\)</span>. When setup in this way, this distribution is often referred to as the <strong>discrete uniform distribution</strong>. Typically, we will use the values for the lower bound (<span class="math inline">\(a\)</span>) and the upper bound (<span class="math inline">\(b=a+k\)</span>) to define <em>which</em> discrete uniform we are discussing. If we say that <span class="math inline">\(X\)</span> follows a discrete uniform distribution with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> we are say that <span class="math inline">\(X\)</span> is a random variable which has an equal probability of taking any of the integers from <span class="math inline">\(a\)</span> to <span class="math inline">\(b\)</span>. Put different, we have that <span class="math display">\[p_X(x) = \begin{cases} \frac{1}{b-a+1} &amp; x \in\{a,a+1,\dots,b\}\\ 0 &amp; \text{otherwise}.\end{cases}\]</span></p>
<p>Knowing the probability mass function of <span class="math inline">\(X\)</span>, we also immediately can work out the expectation and variance for the random variable. Doing this results in <span class="math inline">\(E[X] = \frac{a+b}{2}\)</span> and <span class="math inline">\(\text{var}(X) = \frac{(b-a+1)^2-1}{12}\)</span> (see the following derivation!). This means that just by knowing that a random variable follows a discrete uniform distribution, we immediately know (or can look up) any of the properties that we have discussed up until this point.</p>
<p>TODO: Include derivation.</p>
<p>This is a partiuclarly powerful realization. There are many real-world quantities which we immediatley know follow a discrete uniform distribution. For instance, rolling a die. In the case of a die roll we take <span class="math inline">\(a=1\)</span>, <span class="math inline">\(b=6\)</span>, and immediately understand that <span class="math inline">\(E[X] = 3.5\)</span>, that <span class="math inline">\(\text{var}(X) = \frac{17}{6}\)</span>, and that the probability of each value is <span class="math inline">\(\frac{1}{6}\)</span>. Of course,we could do the same calculations for any die, with any sides labeled in consecutive order.</p>
<p>TODO: Include examples of real-world discrete uniform settings.</p>
<p>Despite the fact that the discrete uniform is fairly prominent in terms of real-world utility, it is also a comparatively simple distribution. However, the main point of this discussion is not actually to introduce the discrete uniform, but rather to introduce the concept of a <strong>named distribution</strong>. The basic premise here is that there are processes in the world which occur frequently enough, in a diverse array of settings, with the same underlying structure ot their uncertainty. If we can study one version of these general problems, then we can extract the mass function, expectation, and varaince and we are easily able to describe the probabilistic behaviour of these quantities. At that point, understanding the uncertainty of random quantities becomes a matter of matching the processes to the correct distribution, and then applying what we know about that distribution directly. While not every process will directly correspond to a known, named distribution, we can often get very close using just a handful of these [TODO: write aside on the pareto distribution].</p>
<p>For each named distribution there is an underlying structure describing in what scenarios it will arise. For instance, for the discrete uniform, this is when there is a set of equally likely outcomes which can be described using consecutive integers. Once matched, there will also be a probability mass function, an expected value, and a variance associated with the distribution. Importantly, all of tehse quantities will depend on some <strong>parameters</strong>. In the discrete uniform we used the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. These parameters specify which <em>version</em> of the distribution is relevant for the underlying scenario. It is best to think of the named distributions as families of distributions, with specific iterations being dictated by the parameter values. If two processes follow the same distribution with different parameters they will not be identically distributed, they are simply drawn from the same family. If two processes have the same parameter values and the same underlying distribution, they are identically distributed and their probabilistic behaviour will be exactly the same. For instance, there is no probabalistic differencebetween rolling a fair, six-sided die or drawing a card at random from a set of <span class="math inline">\(6\)</span> cards labelled <span class="math inline">\(1\)</span> through <span class="math inline">\(6\)</span>. There may be real-world differences which matter, but from a probabilistic point of view, they are exacttly the same.</p>
<p>This is a useful realization as it allows the use of simple models to understand more complex phenomenon. Perhaps the best way to demonstrate the effectiveness of these simple models is to introduce perhaps the most basic named probability distribution: <strong>the Bernoulli distribution</strong>. The Bernoulli distribution characterizes any statistical experiment with a binary outcome when these resultsare denoted <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. The parameter that indexes the distribution is <span class="math inline">\(p\)</span>, which gives the probability of observing a <span class="math inline">\(1\)</span>.</p>
<p>Whenever we want to say that <span class="math inline">\(X\)</span> follows a particular distribution, wwe use a mathematical shorthand to do so. Specifically, we write <span class="math inline">\(X \sim \text{Distribution}(\text{parameters})\)</span> to mean ``<span class="math inline">\(X\)</span> follows the <span class="math inline">\(\text{Distribution}\)</span> with <span class="math inline">\(\text{parameters}\)</span>.’’ For instance, if <span class="math inline">\(X\)</span> represents the results of a fair six-sided die roll, we can write <span class="math inline">\(X\sim\text{Discrete Uniform}(1,6)\)</span>. We will typically shorten this to be <span class="math inline">\(X\sim\text{D.Unif}(1,6)\)</span>. %TODO: Move this up.</p>
<p>If <span class="math inline">\(X\sim\text{Bern}(p)\)</span> (where <span class="math inline">\(\text{Bern}\)</span> is used for Bernoulli distributions) then we know that <span class="math display">\[p_X(x) = \begin{cases} p^x(1-p)^{1-x} &amp; x\in\{0,1\}\\ 0 &amp; \text{otherwise}.\end{cases}\]</span> Further, we know that <span class="math inline">\(E[X] = p\)</span> and <span class="math inline">\(\text{var}(X) = p(1-p)\)</span>. We typically call <span class="math inline">\(X=1\)</span> a <code>success'' and $X=0$ a</code>failure’’ when discussing Bernoulli random variables. The most straightforward application of a Bernoulli random variable is the flip of a coin. Take <span class="math inline">\(X=1\)</span> if a head is shown, and <span class="math inline">\(X=0\)</span> if a tails is shown. Then <span class="math inline">\(X\sim\text{Bern}(p)\)</span>.</p>
<p>A coin flip is, by itself, not particularly interesting. However <em>any</em> statistic experiment with binary outcomes coded this way can be seen as a Bernoulli random variable. Suppose, for instance, you are interested in whether you will pass a particular course or not. There are two options, a <code>success'' (passing) and a</code>failure’’ (failing), and the chances of this are governed by some probability <span class="math inline">\(p\)</span>. Suppose you want to know whether the next flight you take will land safely, it is the same situation. Or whether a particular medical treatment will effectively treat an illness. Each of these scenarios is analyzed in exactly the same way as a coin toss: the probabilities change, but the underlying functions and mathematical objects do not. There is no probabalistic difference between determining whether a coin will come up heads or whether a plane will safely land.</p>
<p>TODO: Examples</p>
<p>Thediscrete uniform and Bernoulli are both distributions which map on to the real-world, but which are quite basic and which do not demonstrate the most meaningful applications of named distributions. These ideas become far more powerful when we begin to explore furthernamed distributions.</p>
<section id="the-binomial-distribution" class="level3" data-number="5.0.1">
<h3 data-number="5.0.1" class="anchored" data-anchor-id="the-binomial-distribution"><span class="header-section-number">5.0.1</span> The Binomial Distribution</h3>
<p>A natural extension to tossing a coin once and seeing if it comes up heads or not is tossing a coin <span class="math inline">\(n\)</span> times and counting how many times it comes up heads. If we take <span class="math inline">\(X\)</span> to be the number of successes in <span class="math inline">\(n\)</span> independent and identically distributed Bernoulli trials, then we say that <span class="math inline">\(X\)</span> has a <strong>binomial distribution</strong>. The binomial distribution is characterized by two different parameters, the number of trials that are being performed, denoted <span class="math inline">\(n\)</span>, and the probability of a success on each trial, <span class="math inline">\(p\)</span>. We write <span class="math inline">\(X\sim\text{Bin}(n,p)\)</span>.</p>
<p>TODO: Simple example for bin.</p>
<p>If we know that <span class="math inline">\(X\sim\text{Bin}(n,p)\)</span> then we get that <span class="math display">\[p_X(x) = \begin{cases} \binom{n}{x}p^x(1-p)^{n-x} &amp; x\in\{0,1,\dots,n\} \\ 0 &amp;\text{otherwise}.\end{cases}\]</span> This is the first distribution we have seen which knowing the underlying distribution would not have immediately translated into knowing the probability mass function, which begins to illustrate why this is a useful area of study. We can work out that <span class="math inline">\(E[X] = np\)</span> and <span class="math inline">\(\text{var}(X) = np(1-p)\)</span>.</p>
<p>TODO: Example with binomial probabilities.</p>
<p>Note that the binomial can be constructed by summing iid Bernoulli variables. Specifically, if <span class="math inline">\(X_1,\dots,X_n\stackrel{iid}{\sim}\text{Bern}(p)\)</span> (note, <span class="math inline">\(\stackrel{iid}{\sim}\)</span> means ``independent and identically distributed according to…’’) then taking <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> gives a binomial with <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> distribution. This should be intuitive from the underlying structure: if a Bernoulli comes up <span class="math inline">\(1\)</span> when we get a heads on a single flip of teh coin, then if we flip the coin <span class="math inline">\(n\)</span> times and count the number of heads this is the same as counting the number of <span class="math inline">\(1\)</span>’s from each corresponding Bernoulli trial, or in other words, summing them. Once we know this construction, we can use the properties we have previously seen about independent random variables to work out the mean and variance for the distribution.</p>
<p>TODO: Aside on the workout of this.</p>
<p>It is important to note and remember that binomial random variables make several key assumptions. First, we are counting the number of successes in a <strong>fixed</strong> number of trials. In order for something to be binomially distributed, we must know in advance how many trials there are under consideration. Second, each of these trials must be independent of one another: the outcome on one cannot impact any of the others. Third, there must be a constant probability of success across all trials. If the probabilities are shifting overtime, then a binomial is no longer appropriate.</p>
<p>TODO: identify several examples for this.</p>
</section>
<section id="geometric-random-variables" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="geometric-random-variables"><span class="header-section-number">5.1</span> Geometric Random Variables</h2>
<p>While the binomial counted the number of successes in a fixed number of trials, we may also be interested in questions relating to how many trials would be needed to see a success. Instead of <code>how many heads in $n$ flips of a coin?'' we may ask</code>how many flips of a coin to get a head?’’ Like the binomial, quantities related to counting the number of trials until a success are related deeply to Bernoulli trials, where once more we are envisioning a sequence of independent and identically distributed trials being performed. However, instead of knowing that we will stop after <span class="math inline">\(n\)</span> trials, here we will only stop once we see a particular result.</p>
<p>Any random quantities following this type of procedure are said to follow a <strong>geometric distribution</strong>. The geometric distribution is parameterized with a single parameter, <span class="math inline">\(p\)</span>, the probability of success. We write <span class="math inline">\(X\sim\text{Geo}(p)\)</span>, and have that <span class="math display">\[
p_X(x) = \begin{cases}(1-p)^{x-1}p &amp; x \geq 1 \\
0 &amp; \text{otherwise}.\end{cases}\]</span> Moreover, <span class="math inline">\(E[X] = \frac{1}{p}\)</span> and <span class="math inline">\(\text{var}(X) = \frac{1-p}{p^2}\)</span>.</p>
<p>TODO: geometric calculation.</p>
<p>The geometric distribution differs from other named distributions that we have considered in that the random variable can take on an infinite number of possible values. The probability that <span class="math inline">\(X\)</span> exceeds a very large threshold shrinks down to <span class="math inline">\(0\)</span>, however, there is no maximum value that can be observed. Beyond that assumption, the key set of assumptions are the same as for the binomial: independent and identically distributed Bernoulli trials are run, and are stopped only after the first observed success.</p>
<p>In the framing we are using here, the random variable <span class="math inline">\(X\)</span> counts the total number of trials <em>including</em> the trail upon which the first success was reached. Sometimes you may see this distribution parameterized slightly differently, taking <span class="math inline">\(X\)</span> to instead count the number of failures before the first success. To convert between the two framings we need only subtract <span class="math inline">\(1\)</span>: there is no meaningful difference in the underlying behaviour.</p>
<p>TODO: include several geometric examples.</p>
</section>
<section id="negative-binomial" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="negative-binomial"><span class="header-section-number">5.2</span> Negative Binomial</h2>
<p>A natural way to make the geometric distribution more flexible is to not stop after the first success, but rather after a set number of successes. That is, instead of flipping a coin until we see a head, we flip a coin until we see <span class="math inline">\(r\)</span> heads. Any random quantity which follows this general pattern is said to follow a <strong>negative binomial distribution</strong>. We use two parameters to describe the negative binomial distribution, <span class="math inline">\(r\)</span> the number of successes we are looking to achieve, and <span class="math inline">\(p\)</span> the probability of a success on any given trial. We write <span class="math inline">\(X\sim\text{NB}(r,p)\)</span>.</p>
<p>TODO: Basic example</p>
<p>If we know that <span class="math inline">\(X\sim\text{NB}(r,p)\)</span>, then we immediately get <span class="math display">\[p_X(x) = \begin{cases}\binom{x-1}{r-1}p^r(1-p) &amp; x\geq r \\ 0 &amp;\text{otherwise}.\end{cases}\]</span> Moreover, we have <span class="math inline">\(E[X] = \frac{r}{p}\)</span> and <span class="math inline">\(\text{var}(X) = \frac{r(1-p)}{p^2}\)</span>. Setting <span class="math inline">\(r=1\)</span>, wee get the same quantities explored in the case of the geometric distribution.</p>
<p>TODO: Include NB Examples</p>
<p>Like for the geometric distribution, we have taken <span class="math inline">\(x\)</span> to represent the total number of trials considered, including the <span class="math inline">\(r\)</span> successes. (TODO: fix geometric distribution range, start at 1). There are alternative parameterizations which would count how many failures occur prior to the <span class="math inline">\(r\)</span>th success, which can be viewed as the value we consider minus <span class="math inline">\(r\)</span>.</p>
<p>TODO: Include many NB examples</p>
</section>
<section id="hypergeometric-distributions" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="hypergeometric-distributions"><span class="header-section-number">5.3</span> Hypergeometric Distributions</h2>
<p>One of the use cases demonstrated for the binomial distribution is drawing <em>with</em> replacement. In order for the binomial distribution to be relevant it must be the case that the probability of a success is unchanging, and correspondingly, if the process under consideration cosntitutes random draws from a population then these draws must be with replacement as otherwise the probabilities would shift. Suppose that we are trying to draw the ace of apdes from a standard, shuffled deck of <span class="math inline">\(52\)</span> cards. If we begin drawing cards without returning them to the deck after each draw, the probability that the next draw is the ace of spades is increasing over the draws. As a result, this type of scenario does not fit into the independent and identically distributed Bernoulli trials that we have been exploring.</p>
<p>Instead, suppose that we are drawing <strong>without</strong> replacement from a finite population. The population consists of two types of items: successes and failures. If we are interested in counting how many successes we seein a set number of draws, then this random quantity will follow a <strong>hypergeometric distribution</strong>. The hypergeometric distribution is parameterized using three different parameters: the number of items in the population, <span class="math inline">\(N\)</span>, the number of these which are considered successes, <span class="math inline">\(M\)</span>, adn the total number of items that are to be drawn without replacement, <span class="math inline">\(n\)</span>. We write <span class="math inline">\(X\sim\text{HG}(N,M,n)\)</span>.</p>
<p>TODO: Basic example</p>
<p>If <span class="math inline">\(X\sim\text{HG}(N,M,n)\)</span> then <span class="math display">\[p_X(x) = \begin{cases}\frac{\binom{N-M}{n-x}\binom{M}{x}}{\binom{N}{n}} &amp; x\in\{\max\{0,N-M+n\},\dots,\min\{n,M\}\} \\ 0 &amp;\text{otherwise}.\end{cases}\]</span> Moreover, <span class="math inline">\(E[X] = \frac{nM}{N}\)</span> and the variance is given by <span class="math display">\[\text{var}(X) = n\frac{M}{N}\frac{N-M}{N}\frac{N-n}{N-1}.\]</span></p>
<p>TODO: Bigger calculation.</p>
<p>The hypergeometric is closely linked to the binomial distribution. If we consider the population described in the hypergeometric setup then the probability of a success on the first draw is <span class="math inline">\(p=\frac{M}{N}\)</span>. Note that <span class="math inline">\(E[X] = np\)</span>, exactly the same as in the binomial. However, plugging this in fro the variance we get <span class="math inline">\(\text{var}(X) = np(1-p)\frac{N-n}{N-1}\)</span>. Notice that if <span class="math inline">\(n=1\)</span>, this extra term is simply <span class="math inline">\(1\)</span>, and for <span class="math inline">\(n &gt; 1\)</span> it will be less than <span class="math inline">\(1\)</span>. As a result, the variance of the hypergeometric is smaller than the variance of the corresponding binomial. This makes intuitive sense: in a hypergeometric, the fact that draws are without replacement means that as more draws go on the probability of observing a success increases, reducing the likelihood of long runs of no observed successes. There is a cap on the behaviour of the random quantity thanks to the finiteness of the population. Correspondingly, the multiplicative term by which the varaince shrinks, <span class="math inline">\(\frac{N-n}{N-1}\)</span> is referred to as the <strong>finite population correction</strong> and it differentiates the behaviour of the hypergeometric and the binomial.</p>
<p>Note that as <span class="math inline">\(N\)</span> becomes very, very large, as long as <span class="math inline">\(n\)</span> is small by comparison, the finite population correction will approach <span class="math inline">\(1\)</span>. in other words: drawing without replacement in a large enough sample behaves almost exactly the same as drawing with replacement in the sample. The binomial distribution can be used to approximate hypergeometric distributions, so long as the population is very large. Again, this makes sense intuitively. If you have a deck with a million cards in it, and you are going to draw <span class="math inline">\(2\)</span>, whether or not you return the first one to the deck has very little bearing on the probabilities associated with this scenario. Generally, the binomial distribution is easier to work with, so this approximation can be useful in some settings.</p>
<p>TODO: aside on this for sampling.</p>
<p>TODO: Many examples of HG random variables.</p>
</section>
<section id="poisson-distribution" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="poisson-distribution"><span class="header-section-number">5.4</span> Poisson Distribution</h2>
<p>The hypergeometric broke from the pattern in the other distributions that have been discussed by not being represented as the sequence of several Bernoulli random trials. However, it was still characterized by a sequence of repeated trials. While many statistical experiments can be framed in this way, there are of course processes which do not fit well into this framing.</p>
<p>Consider, for instance, any process where something is observed for a set period of times and events may or may not occur during this interval. Perhaps you sit on the side of the road and count the number of cars travelling by a particular intersection over the course of an hour. Each car going by is an event, but in this setting the number of events is the random quantity itself. None of the distributions discussed until this point are suited to this type of process.</p>
<p>When we have events which occur at a constant rate, and our interest is in the number of events which are occuring, then we can make use of the <strong>Poisson distribution</strong>. The Poisson distribution takes a single parameter, <span class="math inline">\(\lambda\)</span>, which is the average rate of occurence of the events over the time period we are interest in. We write <span class="math inline">\(X\sim\text{Poi}(\lambda)\)</span>.</p>
<p>TODO: Basic example.</p>
<p>If <span class="math inline">\(X\sim\text{Poi}(\lambda)\)</span> then <span class="math display">\[p_X(x) = \begin{cases} \frac{e^{-\lambda}\lambda^x}{x!} &amp; x \geq 0 \\ 0 &amp;\text{otherwise}.\end{cases}\]</span> Moreover, <span class="math inline">\(E[X]=\lambda\)</span> and <span class="math inline">\(\text{var}(X) = \lambda\)</span>. The Poisson distribution is interesting in that the mean and variance are always equal to one another.</p>
<p>TODO: Bigger example</p>
<p>While the most common applications for the Poisson distribution have to do with the occurences of events throughout time, it is also possible to view this as the occurences of events throughout space. For instance, if there is a manufcturer producing rope, then the number of defects in a set quantity of rope is likely to follow the Poisson distribution. Similarly, in a set geographic area, the number of birds of a particular species is likely to follow a Poisson distribution. For the Poisson distribution, we are typically thinking that there is a rate at which events of interest occur, and we can use the Poisson to model the total number of occurences over some specified interval.</p>
<p>TODO: many examples for the use of Poisson.</p>
</section>
<section id="using-named-distributions" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="using-named-distributions"><span class="header-section-number">5.5</span> Using Named Distributions</h2>
<p>While many other named, discrete distributions exist, these are likely the most common. When confronted with a problem in the real-world for which you wish to understand the uncertainty associated with it, a reasonable first step is to determine whether a named distribution is well-suited to representing the underlying phenomenon. Is it a situation with enumerated events which are equally likely? Use the discrete uniform. Is it a binary outcome? Use the Bernoulli. Are you counting the number of success in a fixed number of trials? Use the binomial. Are you running repeated trials until a (certain number of) success(es)? Use the geomtric (negative binomial). Are you sampling without replacement? Use the hyper geometric. Are you counting events over a fixed space? Use the Poisson.</p>
<p>Once identified, the distribution can be used in exactly the same way as any probability mass function. That is, we still require all of the probability rules, event descriptions, and techniques from before. The difference in these cases is that we immediately have access to the correct form of the probability mass function, the expected value, and the variance.</p>
<p>An additional utility with this approach to solving probability questions is that, over time and repeated practice, you can build-up an intuition as to the behaviour of random variables following these various distributions. Probabilities in general are deeply unintuitive: it can be hard to assess, without fomrally working it out, whether an evetn is likely or unlikely, let alone how likey any event is. However, the lack of intuition from our wider experience can be negated almost entirely by building of intuition through the repeated application of these distributions. You can start to gain a sense of how binomial random variables behave, being able to determine just from inspection whether events seem plausible or not. Much of the study of probability and statistics is about building a set of tools that can overcome the flaws in our intuitive reasoning regarding uncertainty. This comes only through practice, however, this framework of named distributions provides a very solid foundation to perform such practice.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          trigger: 'click',
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          positionFixed: true,
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notes/chapter4.html" class="pagination-link  aria-label=" &lt;span="" variables&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Random Variables</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/chapter6.html" class="pagination-link" aria-label="<span class='chapter-number'>6</span>&nbsp; <span class='chapter-title'>Continuous Random Variables</span>">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Continuous Random Variables</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>
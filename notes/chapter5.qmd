

# Random Variables
When introducing probability originally we worked from a sample space and then corresponding events. This is a very general framework which allows us to effectively capture any statistical experiment that we may want. Sample spaces are not resitrcted to be numeric, for instance, and events are simply subsets of the sample space. As a result, this framewokr provides the tools for capturing uncertainty quantification in just about every setting that we may desire. Still, the need to enumerate sample sapces adn events over complex sets of arbitrary items is cumbersome and may prevent succinct representations of the underlying phenomenon. Often, rather than caring about the entire space out outcomes from an experiment of interest, we are primarily concerned we a summary of the experiment.

When we can summarize the experiment using a numerical quantity, we are able to define a **random variable**.  Arandom variable is simply a numeric quantity whose speciifc value depends on chance, through the outcome of a statistical experiment. Specifically, a random variable is a mapping from the result of an experiment to a set of numbers, and by reporting the numeric value of the random variable, we are able to summarize the key part of the experiment, succinctly. 

For instance, suppose that we are repeatedly tossing a coin. If we toss the coin $1100$ times, then the sample space is going to consist of $2^{100}$ total possible outcomes, each of which is a sequence of $100$ heads and tails. Instead, it may be more convenient to assign a random varaible to be the number of heads that show up on the $100$ tosses of the coin. In this case, the random variable takes on a non-negative integer between $0$ and $100$. In many situations, such a summary may be all the is relevant from the experiment.

TODO: Include an example of random variables.

because of their ability to summarize effectively the components of a random experiment that we care about, random variables will become the default paradigm for discussing randomness going forward. Wehn discussing a random variable we will typically use capital letters, $X$, to represent the random quantity with an unknown value. In the event that an experiment is actually performed, and a value is realized for a random variable, we will record this value as a lower case letter, such as $x$. For instance, the number of heads showing in $100$ flips of a coin is an unknown quantity depending on chance, which we call $X$. Once we have flipped the coin $100$ times and observed $57$ heads, we denote this as $x=57$.

The importance of this notation is merely to emphasize what values are unknown and random, and what values are simply numeric quantities. because $x$ is a known value, taking on some set number, we will not often speak of probabilities involving $x$. Instead, we wish to translate the language of probability that we have built to statements regarding the random variable $X$.

The random variable $X$ has a corresponding sample space of possible values that it can take on. This sample space, which we can think of as directly analogous to the sample space of arbitrary elements from before, will be dictated by the possible realizations from the underlying experiment. Then, after the experiment has been performed, the random variable will take on a single value from this set. Often we are able to very compactly describe the set of possible values for a random variable, for instance, by stating all of the integers, or integers  between $5$ and $10$, or even vvalues less than $1000$. The probability of realizing any of these outcomes is then, just as in the case of the arbitrary sample space, dictated by the underlying probability model.

When introducing the concepts of probability we indicated that probability was assigned to events. When using random variables we still use this convention,  and as  a result, we need to define events in terms of random variables. When we have a random variable, $X$, an event is defined as any set of values that it can take on. For instance, we may have the event $X=4$, or the event $X \geq 18$, or the event $2 \leq X \leq 93$, or the event $X \in \{2,4,6,8,10\}$. In each case these are simply subsets of the possible outcomes that can be observed on the experiment, once summarized through the lens of the random variable. 

Note that, just as was the case before, these events can be though of as being compound events (comprising of multiple outcomes) or simple events (comprised of a single outcome). The event $\{X=5\}$ is a simple event, whereas the event $\{X \geq 25\}$ is a compound event. With events defined in this way,  we can translate all of the other concepts over from before. Specifically, we merely think of the experiment as an experiment producing a numerical  outcome,  and then use the same  sets  of tools as we did before, applied to numeric events.

TODO: Include example recapping previous discussions with regarding the random variables.

When considering  random variables there is a key distinction between two  types of random variables: discrete and continuous. A discrete random variable is a random variable which takes values from a countable set of possiblevalues. When we say countable we mean that there are  either a finite number of possible values that it can take on (say  $\{1,2,\dots,31415\}$) or an infinite number of possible values, but values that can all be written  out in sequence (say $1,2,3,4,\dots$ or $2,4,5,6,8,10,\dots$). Continuous random variables, on the other hand, are random variables which take on values from an uncountably infinite set of values. Typically this will be expressed as random varaibles which can take on any value in some interval (or set of intervals), such  as $X \in [0,1]$ or $X \in (0,\infty)$ or $X\in (-\infty,129]$. In all of these cases there is no way to enumerate the possible set of values that $X$ can take on, and so  we  say that there are an uncountable number of them.

TODO: Include example regarding discrete and continuous random variables.

We will discuss continuous random variables,  and how they  differ from the  discussions we have had up until this point, shortly. For now, we turn our focus to discrete random variables. One of the major utilities of random variables, as has been previously mentioned,  is that they  provide a shorthand for summarizing the results of a statistical  experiment. To  this end, there are also several tools which have been developed relating to random variables which help to expedite the manipulation of ocncepts related  to probability calculations.

Chief among these tools is the concept of a **probability distribution**. A probability distribution  is a summary of the probabalistic behaviour of a random variable. Distributions capture the underlying random behaviour of the random variables of interest, and in so doing, summarize information regarding the experiment or process that is being considered. When concerned with discrete random variables, we typically summarize probability distributions through the use of a **probability mass function**.

A probability mass function is a mathematical function which maps possible values for a discrete random variable to the probability correspond to that event. That is, if a random variable $X$ can take on the values $x_1,x_2,\dots,x_k$, a probability mass function is a function $p(x)$ such that $p(x_1) = P(X = x_1)$, $p(x_2) = P(X = x_2)$, ..., $p(x_k) = P(X=x_k)$. As a result, once a probability mass function is known,  all of the probabalistic behaviour of the random variable can be fully described.

TODO: Include example regarding pmf.

The conditions that we previously outlined for probabilities still must hold when assessing probabilities through a probability mass function. As a result, we know that probabilities are all between $0$ and $1$, and so we must have $0 \leq p(x) \leq 1$, for all $x$. Moreover, we know that the probabilities of the sample space must sum to one, and so  we must have that $$\sum_{x\in\mathcal{X}} p(x) = 1,$$ where $\mathcal{X}$ is the set of possible values that $X$ can take on.

TODO: include example regarding finding the PMF.

When solving questions related to probabilities using a probability mass function, the same secondary properties  apply. Notably, if we want to know $P(X\in A)$, for some set of possible values $A$, then we can write $$P(X\in A) = \sum_{x\in A}p(x).$$ This can be particularly helpful, for instance, if we want to know $P(X \leq c)$ for some constant value $c$. In this case we know that the possible values range from teh smallest value $X$ ccan take on, through to $c$, giving for instance, $$P(X\leq c) = \sum_{x=0}^c p(x),$$ if $X \geq 0$. Rules regarding the complements of events continue to hold as well, where for instance, $P(X > c) = 1 - P(X\leq c)$, giving a useful avenue for simplifying probability calculations.

TODO: Include probability calculations.

With events defined in terms of random variables, we can talk about events as being independent of eachother or mututally exclusive using the same definitions as before. Likewise, we can talk of joint and conditional probabilities, relating to multiple events. With joint probabilities, it is often easiest to combine the event into a single, compound event and find the marginal probability of that event. For instance, if you have the events $X$ is even and $X \leq 15$, then the intersection of these events is $X \in \{2,4,6,8,10,12,14\}$ (supposing $X > 0$).

TODO: include  example regarding conditional  and joint event probabilities.

While we can discuss independence, joint probabilities,  and conditional  probabilities relating to events on the same random variable, it is often of interest to combine multiple random variables. Sometimes these random variables will be multiple versions coming from the same distribution, and other times they will be coming from multiple different distributions. In either event, frequently our main concern is in summarizing the probabalisitc behaviour of two or more random quantities.

Note that when we talk of a random variable ``following'' a particular distribution, we are saying that the probability mass function of the random variable is described by that distribution's mass function. Thus, if two random variables ``share a distribution'', we just mean that their probabilistic behaviour is described by the same underlying mass function.  For instance, if I flip a coin $10$ times, and you flip a different coin $10$ times, and we each count the number of heads that show up, we can say that the  random quantity for the number of heads I observe will have the same distribution as the random quantity for teh number of heads that you observe. These two quantities are not equal, in general, but they aredescribed by the same random processes. 

When we describe the distribution of a particular random variable, we are implicitly describing the marginal probabilities associated with that quantity. Just as before, the marginal probabilities describe the behaviour of the radnom variable alone. What happens when we want to be able to describe multiple components of an experiment, together? For this we require extending the idea of a joint probability beyond the concept of events.

Suppose that we roll two six-sided fair dice. Let $X$ denote the sum of the two dice, and let $Y$ denote the maximum value showing on the two dice. $X$ is a discrete random variable taking on values between $2$ and $12$, while $Y$ is a discrete random variable taking on values between  $1$ and $6$. The sample spaces for the two random variables are different from one another and so immediately we know that their probabalistic behaviour must be different, despite the fact that both random variables summarize the same statistical experiment. We can also  immediately see that the two random variables, while not equal to eachother, certainly dependent on one another: if you know that $Y=1$ then you know that $X = 2$. If you know that $Y = 3$, then you know that $X \leq 6$. 

To begin to capture the joint behaviour of $X$ and $Y$ we introduce a **joint probability mass function**. The joint probability mass function describes the behaviour of the **joint distribution**, in an otherwise analogous manner as the marginal probability mass function. That is, the joint probability mass function assigns a probability value for every pair of values that $(X,Y)$ can take on. Then, once you know the joint behaviour of $X$ and $Y$, you can fully summarize the combined behaviour of the experiment.

TODO: Include an example of a joint PMF.

In practice, joint probability mass functions can be thought of as analogous to the contingency tables we previously saw. If the first variable represents the first random variable being considered, and the second variable represents the second random variable, then each cell of the contingency table assigns a probability to one of the joint events that could be observed in the experiment. Joint distributions are a useful generalization of contingency tables as they allow us to compactly represent not only two  different random variables, but sometimes  many more. All of the definitions used for the case of two  random variables extend naturally to three, four, and beyond.

TODO: Include example of contingency table and joint PMF.

If we continue to consider the case of a bivariate (two variable) joint distribution, we can use this setting to introduce the concept of independence of random variables. Recall that the joint probability mass function of $X$ and $Y$ is a function, $p(x,y) = P(X = x, Y = y)$. We have also introduced the marginal  mass  functions, $p_X(x) = P(X = x)$ and $p_Y(y) = P(Y = y)$. When dealing with events said that two  events were independent if their joint probability was  equal to the product of their marginal probabilities, that is $P(A,B) = P(A)P(B)$. 

If  we imagine taking $A=\{X=x\}$ and $B=\{Y=y\}$, then if $A\perp B$ we can write $p(x,y)=p_X(x)p_Y(y)$. If this holds for every possible $x$ and every possible $y$, then we say that $X$ and $Y$ are independent random variables, and we write $X \perp Y$. [TODO: fix this  statement to use $x'$ and $y'$]

In words, two random variables are independent whenever all possible combinations of events between them are independent. When this happens we can write that $$p(x,y) = p_X(x)p_Y(y),$$ which is to say that the joint probability mass function is the product of the two marginal probability mass functions. 

TODO: iinclude example regarding independence of PMFs.

There is an  equivalence between the described definition, and a slightly more intuitive definition for independence. Whenever $X\perp Y$ we can say that any two events corresponding to $X$ and $Y$, say $X \in A$ and $Y \in B$ are independent. The subtle distinction is that in our previous definition,  we were only concerned with  simple events of the form $X=x'$, whereas here we allow any two arbtirary events. However, note that if we have the above definition holding for simple events, then \begin{align*}
P(X \in A, Y \in B) &= \sum_{x\in A}P(X=x,Y\in B) \\
&= \sum_{x\in A}\sum_{y \in B} P(X=x,Y=y)\\
&= \sum_{x\in A}\sum_{y \in B} p_X(x)p_Y(y) \\
&= \left(\sum_{x\in A}p_X(x)\right)\left(\sum_{y\in B}p_Y(y)\right)\\
&= P(X\in A)P(Y\in B).
\end{align*} That is, even by only making the assumption for simple  events,  the conclusionregarding compound events follows naturally. Whenever any two random variables are known to be independent  we know that any two events corrresponding to these random variables will be independent. Moreover, we can  directly write down the joint probability mass function , simply by taking the product of the two  marginals.

TODO: include example of independent functions.

When introducing events, we discussed how the concepts of independence and dependence could be understood more intuitively through the use of conditional probabilities. The same is true for random variables. The conditional distribution of a random variable captures the behaviour of  a random variable when we have information about another. The conditional probability mass function will output the probability associated with any conditional event between multiple random variables. That is, if we wanted to characterize events of the form $X$ given $Y=y$, which is to say $P(X=x|Y=y)$, then we would use the conditional probability mass function of $X$ given  $Y$, $$p_{X|Y}(x|y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}.$$

This definition is analogous to the formula for conditional probabilities more generally, taking the joint over the marginal. To then determine the probability of any event (for $X$) given some information about $Y$, you  simply plug in $X=x$ and $Y=y$ into the conditional probability mass  function. If you want to condition on more than one random variable, the quantities extend in exactly the same way, where for instance $$P(X=x|Y=y,Z=z)=\frac{P(X=x,Y=y,z=z)}{P(Y=y,Z=z)}.$$

TODO: include example of condition al pmf

As was  discussed, the joint probability mass function of two  random variables is given by the product of the margianls whenever those variables are independent. If we take $X\perp Y$, then plugging $p_{X,Y}(x,y) = p_{X}(x)p_{Y}(y)$ in to the expression for the conditional probability mass function gives $p_{X|Y}(x|y) = p_X(x)$. That is, whenever two variables are independent, the conditional probability mass function  is exactly equal to the marginal probability mass function. We saw that this was true  for events,  and the same reasoning  applies here. This result gives  a more intuitive method for interpretting the independence of random variables. Two random variables are independent whenever any information about the one does not provide information  about theother,  which is to say when they are completely uninformative for one another. With this intuitive concept in mind, it becomes easier to infer when independence of  random variables  seems  reasonable, which becomes a useful skill for manipulating probability expressions.

TODO: includeexamples for independence.

Seeing as  the joint,  marginal, and conditional probability mass functions are exactly analogous to the corresponding concepts  when they were introduced  regarding events, it is reasonable to assume that we  canextend the multiplication rule, the law  of  total  probability, and Bayes' theorem to the framework  of probability functions  as well. Indeed, each of these relationships continues to hold for random variables in much the way that would be exptected.

The multiplication rule simply states that $p_{X,Y}(x,y) = p_{X|Y}(x|Y)p_Y(y) = p_{Y|X}(y|x)p_X(x)$. This can be seen by rearranging the relationship  defining the conditional probabilities. Bayes' Theorem also can be  extended in nearly an identical fashion giving  $$p_{Y|X}(y|x) = \frac{p_{X|Y}(x|y)p_Y(y)}{p_X(x)}.$$ These rules give the ability to  compute the  joint distribution and the other conditional information, when we have information regarding some of the marginals andsome of the conditionals. These properties are used  less explicitly when  dealing with probability mass  functions directly, instead becoming absorbed into the fabric of thedefining relationships themselves. That is  to say, you are less likely to see Bayes'  theorem invoked directly when moving ebtween  conditional  distributions,  however, moving between conditional  distributions is an  important skill which is very often  required.

TODO: include examples for multiplication rule and Bayes' theorem

Unlike the multiplication rule and Bayes' theorem,  the extension of the law of  total probability is frequently cited when manipulating  probability mass  functions. It is a process which  is important enough so as to warrant its  own name: **marginalization**. The idea with marginalization is  that we are going to take a joint probability mass function  and **marginalize** it, turning it into a marginal distribution. This is analogous to the law of total probability. Note that when dealing with a random variable, $Y$, there is some set of numeric values that $Y$ can take on. Suppose that wwe refer to these values as $\mathcal{Y}$. A natural partition of the space is to then take each possible value for $Y$ as the event,  and simply enumerate through the elements of $\mathcal{Y}$.

Using this partition, we can ask about the ways that $X$ can take on any particular value. In order for $X$ to be $x$, we know that one of  the $Y=y'$ for some $y'\in\mathcal{Y}$ must have ocurred. Thus, if we add up all the possible combinations, $(X=x,Y=y_1)$, $(X=x,Y=y_2)$, and so forth, then we will  have covered every possible way of making $X=x$. This is the exact same process  we used when looking at contigency  tables, where we summed a row or column to get the marginal probabilities.

TODO: include partition  graphic

Taking this argument and encoding it with mathematical  notation  we get that $$P(X=x) = \sum_{y\in\mathcal{Y}} P(X=x, Y=y) \implies p_X(x) = \sum_{y\in\mathcal{Y}} p_{X,Y}(x,y).$$ That is, the process of marginalization involves summing over one of the random variables in a joint distribution function, leaving behind only the marginal. This is often a very effective  way  of determining a marginal distribution when information about two  random variables is easy to disceren than information  about only one.

To complete the analogy to the law of total  probability, recall that the multiplication rule tells us that $p_{X,Y}(x,y) = p_{X|Y}(x|y)p_Y(y)$, and so we  may alsomarginalize by taking $$p_X(x) = \sum_{y\in\mathcal{Y}} p_{X|Y}(x|y)p_Y(y).$$ This makes it clear that marginalization is often accomplished via arguments based on conditioning.

When confronted with questions from statistics and probability, it will often be the case that the natural answer to the question  is ``it depends.'' For instance, if asked ``what si the probability that a student passes  their next exam?'' the likely response is ```it  depends.'' One very useful technique for solving these questions in a satisfactory way is to continue that line of thought and explicitly specify ``on what.'' For the students, for instance, you may  say ``it depends  on how much they study.'' The conceit in this situation is that, if you were to know how much the student has studied, then you would better understand the outcomes of the student's test. In our mathematical terms this means you have a firm belief about the conditional distribution  between the random quantities of exam performance  and study time. The process of marginalization, and the law of total  probability before that, provide useful ways of being able to translate these ``it depends'' statements into concrete beliefs  about the marginal probabilities. Recall that marginal distributions are distributions  which do not depend on any  otherquantitity, and instead, thy capture the overall  behaviour. They  have, in some sense, averaged out all other factors  and give you the beliefs which do not depend on anything else at all. The technique for accomplishing this is marginalization, and  other forms of conditioning arguments.

TODO: include worked example on conditioning.

## Independent and Identically Distributed: A Framework for Interpretation
A very common assumption when addressing questions in statistics and in probability is that we have a set of random variables which are independent and identically distributed (iid). We now have the  tools to understand concretely what this means. Specifically,  a set of random variables, $X_1,X_2,\dots,X_n$ are said to be independent and identically distributted (denoted  iid almost all the time) whenever (i) every subset of random variables in the collection is independent of every other subset of random variables in the collection, and (ii) the marginal distribution for each of the random variables are  exactly the same. The assumption of iid random quantities  will often come up when we are repeating a process many times over, and thinking about what observations will arise from this. Suppose that $X_1$ is a random variable that takes the value $1$ if a flipped coin comes up heads,  and $0$ otherwise. If we imagine flipping this coin $100$ times then it is reasonable to assume that each sequential coin flip will be independent of eacch other coin flip, since the result on one flip of a coin should not influence the result of any other flip of a coin. Moreover, every time the coin  is flipped, it is reasonable to assume that the probability it shows up  heads remains  the  same. As a result, these $100$ random quantities, $X_1,  X-2,  \dots,  X_{100}$ can be said to be iid.

TODO: include other iid examples

While we will use the assumption of iid random variables later, they also provide an intuitive method for interpretting probability functions and distributions. Suppose that we wwere to take a  distribution function, $p_X(x)$. If we were able to generate independent and identically distributed realizations from this probability mass ufnction, then the function  $p_X(x)$ describes the behaviour for these  repeated realizations. Specifically,  $p(x)$ will give the long-run proportion of realizations of the iid random variables which take the value $x$.

TODO: Include interpretation  statement

This type of statement is always the flavour of interpretation statements that are made with respect to probability and statistics. It will always be the case that, in order to understand what is meant specifically by a statement of probability will involve the repetition of some statistical experiment over and over again. When we were discussing sample spaces and experiments directly, we talked about repeating the experiment over and over again. When we begin to  work with random  variables instead, it becomes more natural to think about the replication procedures coming through the use of independent and identically distributed  random variablles. 

As  the study of probability foes on,  we begin to need to work with random quantities in a strictly theoretical sense. In introductory level  problems, we are often holding in mind very concrete examples to illustrate the procedures and concepts. In this setting it is easy enough to hold in mind the experiment of interest: for instance, we may have a random variable representing the result of a coin toss, and you can envision repeatedly tossing a coin. As the concepts become less concrete, more abstract, and harder to draw directl parallels to tangible scenarios, it becomes more and more important to rely on the interpretations rooted in a series of independent and identically distributed random variables. A large component of statistics as an area of study is making explicit the assumptions we are working with, and doing our best to ensure that these are reasonable. By interpreting probability mass functions as the proportion of independent and identically distributed random variables that take on a particular value, when we repeatedly take realizations of these random variables ad infinitum, this philosophy is made clear and explicit.

## Expectation
Until this point in our discussions of probability we have relied upon characterizing the behaviour of a random variable via the use of probability mass  functions. In some sense, a probability mass function captures all  of the probabilistic behaviour of a discrete random variable. Using the mass function  you are able to characterize how often, in the long-run, any particular value will be observed, and any questions associated with this. As a result, the mass function remains a critical  area  of focus for understanding how random quantities behave.

However, these functions need to be explored and manipulated in order for useful information  to be extracted from them. They do not summarize this behaviour effectively, as they are not intended to be a summary tool, and understandably we often wish  to have better numeric quantities which are able to concisely indicate components of the behaviour of a distribution. Put differently, provided with a priobability mass function it is hard to immediately answer ``what do we expect to happen, with this random variable?'' despite the fact that this is a very obvious first question.

To  address questions related to expectations, we  turn towards the statistical concept of an **expected value**. We refer to expected  values as expectations, averages, and means of a distribution, interchangeably. The idea with an expected value is that we are trying to capture, with one single number, what value is expected  when we make observations from the random quantity. There are many ways one might think to describe our expectations, and it is worth exploring these concepts in some detail.

One way  that we may think to define our expected value is by asking what value is the most probable. This is a question which  can be directly answered using the probability mass function. The process for this  would require looking at the function and determining which value for $x$ corresponds to the highest probability: this is the  value that we are most likely to see. Sometimes this procedure  is fairly straightforward, sometimes it is quite complicated. No matter the complexity of  the specific situation, the underlying process is the same: what value has the highest probability of being seen, and that is the most likely one.

TODO: Example of mode

As intuitive as this may seem, this is not the value that will  be used as the expectedvalue generally. Instead, this quantity is referred to as the **mode**. While the mode is a useful quantity, and for some decisions will be the most pertinent summary value, there are some major issues with it as  a general measure which make it less desirable. For starters, consider that our most common probability model considered until this point has been that of equally likely outcomes. Here, there is no well-defined mode (convention tends to be taking it as the set of all the most probable values). Presenting the mode is equivalent to presenting the full mass function in this setting.

While the case of equally likely outcomes is  a fairly strong explanation for some issues with the mode, it need not be so  dramatic to undermine its utility. It is possible for a distribution to have several modes which are quite distinct from one another, even if it's not all values. Moreover, it is quite common for the modal value to be not particularly likely itself. Consider a random variable that can take on a million different values. If all of the probabilities are approximately $0.000001$ then presenting the mode as the most probable value does not translate to saying that the mode is particularly probable. 

TODO: incllude  example where the mode is slightly more likeely than a string of similar values.

If the mode has these shortcomings, what else might work? Another intuitive concept is to try to select the ``middle'' oof thedistribution. One way to define the middle would be to select the value such that half of observations are beneath it, and half of observations are above it. That way, when you are told this value, you immediately know that it is equally likely to observe values on either side of this mark. This is also a particularly intuitive definition for expected value, and is important enough to be named: **the median**.

The median is the midpoint of a distribution, and is very important for describing  the behaviour of random ariables. Medians  are often the most helpful single value to report to indicate the typical behaviour of a distribution, and they are frequently used. When people interpret averages,  in general, it is often the median that they are actually interpretting. It is very intuitive to be given  a value and know that it is the middle of all the possible values for a distribution.

TODO: include examples on medians

Despite the advantages of medians, they have their own drawbacks as well. For starters, the median can be exceptionally challenging to compute in certain settings. As a result, even when a median is appropriate, it may not be desirable if it is too  challenging to determine. Beyond the difficulties in computation, medians have some properties which may be undesirable, depending on the specific use case. One concern which arises frequently is that medians are not translated to totals, which can make them challenging in certain use cases.

Suppose that you are a store and you know that your median quantity of items sold in a day is  $50$ and the median cost of these items is $\$10$, you  cannot simply multiply the $50$ and the $\$10$ to suggest that your median revenue in a day is $\$500$. Doing this ttype of unit conversion or basic arithmetic with medians can be challenging, and as a result they are not always the most useful when reporting values that are going to be interpretted as rates.

TODO: Expand  on the above example.

Beyond the basic manipulation medians have a feature which is simultaneously a major benefit in some settings, and a major fallback in  others. Specifically, medians are less influenced by extreme values in the probability distribution. Consider two different distributions: one of them is equally likely to take any value between $1$ and $10$, where the other is equally likely to take any value between $1$ and $9$ or $1,000,000$. In both of these settings, we can take the median to be $5.5$ since half of the probability mass falls above  $5.5$ and half falls below it. 

The median, in some sense, ignores the extreme value in the probability distribution and remains stable throughout it. In certain settings, this can be very desirable. For instance, in the distribution of household incomes, the median may be an appropriate measure seeing as there are a few families who have very extreme incomes which otherwise distort the picture provided by most families. In this sense, the  median's robustness to extreme values is a positive feature of it in terms of a summary measure for distributional behaviour.

Suppose instead that you work for an insurance company and are concerned with understanding the value of insurance claims that your company will need to pay out. Thedistribution will look quite similar to the income distribution: most of the probability will be assigned to fairly small claims, with a small chance of a very large one. As an insurance company, if you use the median this large claim behaviour will be smoothed over, perhaps leaving you unprepared for the possibility of extremely large payouts. In this setting, the extreme values are informative and important, and as a result the median's robustness becomes a hindrance to correctly describing the behaviour. 

TODO: Another  median example?

Between the median and the mode we have two measures which capture some sense of expected value, each with their own set of strengths and drawbacks. Neithercapture what it is that is referred to as *the* expected value. For this, we need to take inspiration from the median, and consider another way that we may think to find the center of the distribution.

If the median gives the middle  reading along the values sequentially, we may also wish to think about trying to find the ``center of gravity'' of the numbers. Suppose you take a pen, or marker, or small box of chocolates, and you wish to balance this object on a finger or an arm. To do so, you do not place the item so that half of it sits on one side of the appendage and half on the other: you adjust the location so that half of the masssits on  either side of the appendage.

Throughout our probability discussions, we have always referred to probability as mass itself. We use the probability mass function to generate our probability values. This metaphor can be extneded when we try to find the center of the distribution. If we imagine placing a mass with weight equal to the probability mass functions value at each value that a random variable can take on, we may ask: where would we have to place a fulcrum to have this number line be balanced? The answer to this question serves as another possible measure of center.

It turns out that this notion of center is the one that we are all most familiar with: the simple average. And this simple average is also the conception of expectation which gets bestowed with the name ``expected value''. Mathematically, the expeccted value is desirable for many reasons, some of which we will study in more depth later on. One of these desirable features, which  stands in contrast with the median, is the comparative ease with which expected values can be computed. For a random variable, $X$, we write the expected  value of $X$ as $E[X]$, and assume that $X$ takes values in $\mathcal{X}$ with a probability mass function  $p_X(x)$, we get $$E[X] = \sum_{x \in \mathcal{X}} xp_X(x).$$

TODO: Example compute simple expected value.

In the case of an equally likely probability model, the expected value becomes the standard average that is widely used. Suppose that there are $n$ options in the sample space, denoted $x_1,\dots,x_n$, then we can write $$E[X] = \sum_{i=1}^n x_i\frac{1}{n} = \frac{1}{n}\sum_{i=1}^nx_i.$$ When the probability models are more complex, the formula is not precisely the stnadard average - instead, it becomes a weighted average. 

TODO: Add  basic average example.

While less commonly applied than the simple average, a weighted average is familiar to most students for a crucial purpose: grade calculations. If you view the weight of each item in a course as a probability mass, and the grade you scored as the value, then your  final grade in the course is exactly the expected value of this distribution. The frequency with which expectedvalues are used make them attractive as a quick summary for the center of a distribution.

TODO: Include pricing calculation showing mean versus median.

While the mean provides a useful, intuitive measure of center of the distribution, it is perhaps counter intuitive to name it the expected value. To understand the naming convention it is easiest to consider the application which has likely spurred more development of statistics and probability than any other: gambling.

Suppose that there is some game of chance that can pay out different amounts with different probabilities. A critical question for a gambler in deciding whether or not to play such a game is ``how much can I expect to earn, if I play?'' This is crucial to understanding, for instance, how much you should be willing to pay to participate, or if you are the one running the game, how much you should charge to ensure that you make a profit. 

If you want to understand what you expect to earn, the intuitive way of accomplishing this  is to weight each possible outcome by how likely it is to occur. This is exactly the expected value formula that has been provided, and so the  expected value can be thought of as the expected payout of a game of chance where the outcomes are payouts corresponding to each probability. 

TODO: Include expected payout calculation.

This also represents the cost at which  a rational actor should be willing to pay to participate.  If a game ofchance costs more than the expected value to play, in the long run you will lose money. If a game of chance costs less than the expected value, in the long run you will earn money. It is hard to overstate the utility of gambling in developing probabiility theory, and as such these types of connections are expected.

To interpret the expected  value of a random variable, one possibility is using the intuition that we used to derive the result. Notably, the expected value is the center of mass of the distribution, where the masses correspond to probabilities. This means that it is not necessarily an actual central number over the range, but rather that it sits in the weighted  middle. While this interpretation is useful in many situations, there are times where the point of  balance is a less intuitive description. For these, it can sometimes be useful to frame the expected value as the long-term simple average from the distribution.

If we imagine observing many independent and identically distributed random variables, then as the number of samples tends to infinity, the  expected value of $X$ and the simple average will begin to coincide with one another. That is the distance between $E[X]$ and $\frac{1}{n}\sum_{i=1}^n X_i$ will shrink to $0$. As a result, we can view the expected value  as the average over repeated experiments. This interpretation coincides nicely with the description based on games of chance. Specifically, if you were to repeatedly play the same game of chance, the average payout per game will be equal to the expected value, if you play for long enough.

TODO: Include convergence graphic.

Somtimes the value of a random variablle needs to be mapped through a function to give the value which is most relevant to us. Consider, for instance, a situation wherein the side lengths of boxes being manufactured by a specific supplier are random, due to incorrectly calibrated tolerances in the machines. The resulting boxes are cubes, but what is of more interest is  the volume of the produced box, not the side length. If a box has side length $x$, then its volume will be $x^3$, and so we may desire some way of computing $E[X^3]$ rather than $E[X]$.

In general, for some function $g(X)$, we may want to compute $E[g(X)]$. It is important to recognize that, generally speaking, $E[g(X)] \neq g(E[X])$. This is a common mistake, and an attractive one, but a mistake nonetheless. If we are unable to simply apply the function to the epxected value, then the question of how to compute the expected  value remains. Instead of applying the function to overall expected value, instead, we simply apply the function to each value in the defining  relationship for the expected value. That is, $$E[g(X)] = \sum_{x\in\mathcal{X}} g(x)p_X(x).$$ This  is sometimes referred to as  the ``law of the unconscious statistician,'' a name which may be aggressive enough to help remember the correct way to compute the expectation.

TODO: Move the next thing up.
Where the median demonstrated robustness against extreme values in the distribution, the mean (or expected value) does not. For instance, if we consider the distribution of incomes across a particular region, the mean will be much higher than the median, as those families with exceptionally high incomes will not be smoothed over as they were with medians. In this case, the lack of robustness for the expected value will  render the mean a less representative summary for the true behaviour of the random quantity. 

To see this concretely, consider the difference  between a random variable which with equal probability takes a value between $1$ and $10$. This will have $E[X] = 5.5$. Now, if the $10$ is made to be $1,000,000$, the expected value will now  be $E[X] = 100,004.5$. This is  a far cry from the median which does not change from $5.5$ in either case. This lack of robustness is desirable in the event of the insurance example from the median  discussion, but will be less  desirable in other settings.  

The mean, median, and mode are the three standard measures of central tendency. They  are also referred to as measures of location, and in general, are single values which  describe the standard behaviour of a random quantity. Each of the three has merits as  a measure, and each has drawbacks for certain settings. The question of which to use and when depends primarily on the question of interest underconsideration, rather than on features of the data alone.  Often, presenting more than one measure can give a better sense of the distributional behaviour that any one individual will.

TODO: Include example of choosing measures.

Despite the utility of all three measures, the expected value  holds a place of more central importance in probability and statistics. A lot of this has to do with further mathematical properties of the mean. Because of its central  role, it is worth studying the expected value in some more depth.
% END OF MOVING SECTION.

TODO: include example using LOTUS.

These functions applied to random variables are often thought of as ``transformations'' of the random quantities. For instance, we *transformed* a side length into a volume. While the law of the unconscious statistician will apply to any transformation for a random variable, we can sometimes use shortcuts to circumvent its application. In particular, when $g(X) = aX + b$, for constant numbers $a$ and $b$, we can greatly simplify the expected value of the transformation. To see this note \begin{align*}
E[aX + b] &= \sum_{x\in\mathcal{X}}(ax + b)p_X(x) \\
&= \sum_{x\in\mathcal{X}}axp_X(x) + bp_X(x) \\
&= a\sum_{x\in\mathcal{X}}xp_X(x) + b\sum_{x\in\mathcal{X}}p_X(X) \\
&= aE[X] + b.
\end{align*} That is, in general, we have that $E[aX + b] = aE[X] + b$. 

This is particularly useful as linear transformations like $aX+b$ arise very commonly. For instance, most unit conversions are simple linear combinations. If a random quantity is measured in one unit then this result can be used to quickly convert expectations to another.

TODO: include example of temperature or weight conversion.

This type of linear transformation also frequently comes up with games of chance and payouts, or with scoring more generally. For instance, suppose you are betting a certain amount on the results  of a coin toss, or that you are taking a multiple choice test that gives $2$ points for a correct answer. 

Measures of central tendency are important to summarize the beahviour of a random quantity. Whether using the mean, median, or mode, these measures of location describe, on average, what to expect from observations of the random quantity. However, understanding a distribution requires understanding far more than simply the measures of location. As was discussed previously, the probability mass function captures the complete probabilistic behaviourof a discrete random variable, it is only intuitive that some information would be lost with a single numeric summary.

TODO: Example with equivalent mean, median, and mode.

A key characteristic of the behaviour of a random variable which is not captured by the measures of location is the variability of the quantity. If we imagine taking repeated realizations of a random variable, the variability of the random variable  captures how much movement there will be observation to observation. If a random variable has low variability, we expect that the various observations will cluster together, becoming not too distant from one another. If a random variable has high variability, we expect the observations to jump around each time.

Just as was the case with measures  of location, there are several measures of variability which may be applicable in any given setting. One fairly basic measure of the spread  of a random variable is simply the range of possible values: whati s the highest possible value, what is the lowest possible value, and how much distance is there between those two points? This is a fairly intuitive notion, and is particularly useful in the equal probability model over a sequence of numbers. Consider, for instance, dice. dice are typically defined by the range of values that they occupy, say $1$ to $6$, or $1$ to $20$. Once you know the values present on any die, you have a sense for how much the values can move observation to observation. 

TODO: Include example for the range.

While the range is  an important measure to consider to determine the behaviour of a random variable, it is a fairly crude measurement. It may be the case that, while the extreme values are possible, they are sufficiently unlikely so as to come up very infrequently and not remain representative of thelikely spread of observations. Alternative, many random variables have a theoretically infinite range. In these cases, providing the range will likely not provide much utility.

TODO: Include example.

To rememdy these two issues, we can think of some techniques for modifying the range. Instead of taking the start and end points to be the lowest and highest values, we can instead consider ranges of values which remain more plausible. A common way to do this is to extend our concept of a median beyond the half-way point. The median of a random variable $X$, is the value, $m$, such that $P(X \leq m) = 0.5$. While there is good reason to care about the midpoint, we can think of generalizing this to be *any* probability.

That is, we could find a number $z$, such that $P(X \leq z) = 0.1$. We could then use this value to conclude that $10\%$ of observations are below $z$, and $90\%$ of observations are above $z$. (TODO: Change this to be ``probability of observation''). These values are referred to, generally, as percentiles and they are the natural extension of medians. We will typically denote the $100p$th percentile as $\zeta(p)$, which is the value $P(X \leq \zeta(p)) = p$. Thus, the median of a distribution is $\zeta(0.5)$. 

TODO: Include examples

We can leverage percentiles to remedy some of the issues with the range as a measure of variability. Framed in terms of percentiles, the minimum value is $\zeta(0)$, and the maximum value is $\zeta(1)$. Instead of considering theextreme endpoints, if we consider the difference between more moderate percentiles, we can overcome the major concenrs outlined with the range. The most common choices would be to take $\zeta(0.25)$ and $\zeta(0.75)$; these are referred to as the first and third  quartiles, respectively. They are named as, taking $\zeta(0.25)$, $\zeta(0.5)$ and $\zeta(0.75)$, the distribution is cut into quarters.

TODO: Include examples.

With the first and third quartiles computed, we can compute the interquartile range, which is given by $\zeta(0.75)-\zeta(0.25)$. Typically, we denote the interquarite range simply as $\text{IQR}$, and like the overall range, it gives a measure of how much spread there tends to be in the data. Unlike the range, however, we can be more certain that both the first and third quartiles are reasonable values around which repeated observations of the random variable would be observed. Specifically, there is a oribability of $0.5$ that a value between  the first and third quartile will be observed. The larger the $\text{IQR}$, the more spread out these moderate observations will be, and as a result, the more variable the distribution is.

TODO: Write examples.

Both the range and the interquartile range give a sense of the variation in the distribution irrespective of the measures of location for that distribution. Another plausible method for assessing the variability of a distribution is to assess how far we expect observations to be from the cneter. Intuitively, if observations of $X$ are near the center with high probability, then the distribution will be less variable than if the averagedistance to the center is larger. 

This intuitive measure of variability is useful for capturing the behaviour of a random variable, particularly when paired iwth a measure of location. However, we do have to be careful: not all measures of dispersion based on this notion will be useful. Consider the most basic possibility, to consider $X - E[X]$. We might ask, for instance, what is the expected value of this quantity. If we take $E[X - E[X]]$ then note that this a linear combination in expectation since $E[X]$ is just some number. Thus, $E[X-E[X]] = E[X] - E[X] = 0$. In other words, the expected difference between a random variable and its mean is exactly $0$. We thus need to think harder about how best to turn this intuition into a useful measure of spread as the first idea will result in $0$ for all random quantities.

The issue with this procedure is that some realizations are going to be below the mean, making the difference negative,  and some will be above the mean, making the difference positive. Our defining relationship for the mean relied on balancing these two sets of mass. However, when discussing the variability of the random variable, we do not much care whether the observations are lower than expected or higher than expected, we simply care how much variability there is around what is expected. To remedy this, we should consider only the distance between the observation and the expectation, not the sign. That is, if  $X$ is $5$ below $E[X]$ we should treat that the same as if $X$ is $5$ above $E[X]$.

There are two common ways to turn value into its magnitude in mathematics generally: squaring the number and using absolute values. Both of these tactics are useful approaches to defining measures of spread, and they result in the **variance** when using the expected value of the squared deviations, and the **mean absolute deviation** when using the absolute value. While $E[|X-E[X]|]$ is perhaps the more intuitive quantity to consider, generally speaking it will not be the one that we use. 

In general when we need a positive quantity in mathematics it will typically be preferable to consider the square to the absolute value. The reasons for this are plentiful, but generally squares are easier to handle than absolute values, and as a result become more natural quantities to handle. The variance is the central measure of deviation for random variables, so much so that we give it its own notation, $$\text{var}(X) = E[(X-E[X])^2].$$

Note that if we take $g(X) = (X-E[X])^2$, then the variance of $X$ is the expected value of a transformation. We have seen that to compute these we apply the  law of the unconscious statistician, and substitute  $g(X)$ into the  defining relationship for the expected value, which for the variance gives $$\text{var}(X) = \sum_{x\in\in\mathcal{X}} (x-E[X])^2p_X(x).$$ Prior to computing  the variance, we must first work out the mean as the function $g(X)$ relies upon this value.

TODO: Include example for calculating variance.

The higher that an individual random variables variance is, the more spread we expect there to be in repeatedly realizations of that quantity. Specifically, the more spread out around the mean value the random variable will be. A random variable with a low variance will concentrate more around the mean value than one with a higher variance. One confusing part of the variance of a random variable is in trying to assess the units. Suppose that a random quantity is measured in a particular set of units - dollars, seconds, grams, or similar. In this case, our interpretations of measures of location will all be in the same units, which aids in drawing connections to the underlying phenomenon that we are trying to study. However, because the variance is squared, we cannot make the same extensions to it: variance is not measured in the regular units, but in the regular units squared. 

Suppose you have a random time being measured, perhaps the reaction time for some treatment to take effect in a treated patient. Finding the mean or median will give you a result that you can read off in seconds as well. The range and interquartile range both give you the spread in seconds. However, if you work out the variance of this quantity it will be measured in seconds squared - a unit that is challenging to have much intuition about. To remedy this we will often use a transformed version of the variance, called the **standard deviation**, returning the units to be only the original scale. The standard deviation of a random variable is simply given by the square root of the variance, which is to say $$\text{SD}(X) = \sqrt{\text{var}(X)}.$$ We do not often consider computing the standard deviation directly, and so will most  commonly refer to the variance when discussing the behaviour of a random variable, but it is important to be able to move seamlessly between these two measures of spread.

TODO: Standard deviation.

When computing the variance of a random qauntity, we often use a shortcut for the formula. Consider \begin{align*}
\text{var}(X) &= \sum_{x\in\mathcal{X}} (x-E[X])^2p_X(x) \\
&= \sum_{x\in\mathcal{X}} (x^2 - 2xE[X] + E[X]^2)p_X(x) \\
&=\sum_{x\in\mathcal{X}} x^2p_X(x) - 2E[X]\sum_{x\in\mathcal{X}}xp_X(x) + E[X^2]\sum_{x\in\mathcal{X}}p_X(x)\\
&= E[X^2] - 2E[X]E[X] + E[X]^2\\
&= E[X^2] - E[X]^2.
\end{align*}

This result gives us the identity that the variance of $X$ can be found via $E[X^2] - E[X]^2$. Generally, this is moderately more straightforward to calculate since $X^2$ is an easier transformation than $(X-E[X])^2$. This identity will come back time and time again, with a lot of versatility in the ways that it can be used. Typically, when a variance is needed to be calculated the process is to simply compute $E[X]$ and $E[X^2]$, and then  apply this relationship.

TODO: include variance calculation example.

With expectations, we saw that $E[g(X)]$ needed to be directly computed from the definition. The same is true for variances of transformations. Specifically, $\text{var}(g(X))$ is given by $E[(g(X) - E[g(X)])^2]$ which can be simplified with the previous relationship as $E[g(X)^2] - E[g(X)]^2$. Just as with expectations, it is important to realize that $\text{var}(g(X)) \neq g(\text{var}(X))$, and so dealing with transformations requires further work.


TODO: Transformation example.
TODO: Move the following discussion up.
Beyond being linear over simple transformations, summations in general behave nicely with expectations. Specifically, for any quantities separated by addition, say $g(X) + h(X)$, the expected value will be the sum of each expected value. Formally, \begin{align*}
E[g(X) + h(X)] &= \sum_{x\in\mathcal{X}} (g(X) + h(X))p_X(x) \\
&= \sum_{x\in\mathcal{X}} g(X)p_X(x) + h(x)p_X(x) %todo fix capitals\\
&= \sum_{x\in\mathcal{X}} g(x)p_X(x) + \sum_{x\in\mathcal{X}} h(x)p_X(x) \\
&= E[g(X)] + E[h(X)].
\end{align*}
Behaving well under linearity is one of the very nice properties of expectations. It will come in useful when dealing with a large variety of important quantities, and as we will see shortly, this linearity will also extend to multiple different random quantities.

TODO: add example of linearity.
%End section to move.

With expectations, we highlighted linear transformations as a special case, with $g(X) = aX + b$. For the variance, the linear transformations are also worth distinguishing from others. To this end, we can apply the standard identity for the variance, giving \begin{align*}
E[(aX+b)^2] &= E[a^2X^2 + 2abX + b^2] \\
&= E[a^2X^2] + E[2abX] + E[b^2]\\
&= a^2E[X^2] + 2abE[X] + b^2.
\end{align*}

TODO:Move upwards
Note that part of the property of the linearity of expectation that we can immediately see if that the expected  valuye of any constant is always that constant. If we take $a = 0$, then we see that $E[aX + b] = E[b] = b$. Thus, any time that we need to take the expected value of any constant number, we know that it is just that number.
%End upwards movement

Next, we note that $E[aX + b] = aE[X] + b$ and so \begin{align*}
E[aX + b]^2 &= (aE[X] + b)^2 \\
&= a^2E[X]^2 + 2abE[X] + b^2.\end{align*} Differencing these two quantities gives $$a^2E[X^2] + 2abE[X] + b^2 - a^2E[X]^2 - 2abE[X] - b^2 = a^2(E[X^2] - E[X]^2).$$ By noting that $E[X^2] - E[X]^2$, we can complete the statement that $$\text{var}(aX + b) = a^2\text{var}(X).$$

Thus, when applying a linear transformation, only the multiplicative constant matters , and it transforms the varaince by a squared factor. This should make some intuitive sense that the additive constant does not change anything. If we consider that variance is a measure of spread, adding a constant value to our random quantity will not make it more or less spread out, it will simply shift where the spread is located. This is not true of the mean, which measures where the center of the distribution is, which helps explain why the result identities are different.

TODO: Example using this.

In the same way that the linearit of expectation demonstrates that the expected value of any constant is that constant, we can use this identity to show that the variance of constant is zero. However, we can also reason to this based on our definitions  so far. Suppose that we have a random variable which is constant. This seems to be an oxymoron, but it is perfectly well defined. A constant $b$ can be seen as a random variable with probability distribution $p_X(x) = 1$ if $x=b$ and $p_X(x) = 0$ otherwise. In this case, the expected value is going to be $E[X] = 1(b) = b$, and $E[X^2] = 1(b)^2 = b^2$. As a result, we see that $E[b] = b$, as previously stated, and $\text{var}(b) = E[X^2] - E[X]^2 = b^2 - b^2 = 0$. From an intuitive perspective, there is no variation around the mean of a constant: it is always the same value. As a result, when taking the variance, we know that it should be $0$. 

Unlike the expecctation, the variance of additive terms will not generally be the addition of the varainces themselves. That is, we cannot say that $\text{var}(g(X) + h(X)) = \text{var}(g(X)) + \text{var}(h(X))$, as a general rule. Writing out the definition shows issue with this: $$E[(g(X) + h(X))^2] = E[g(X)^2] + 2E[g(X)h(X)] + E[h(X)^2].$$ The first and third terms here are nicely separated and behave well. However, the central term is not going to be easy to simplify, in general. You can view $g(X)h(X)$ as a function itself, and so $E[g(X)h(X)] \neq E[g(X)]E[h(X)],$$ in general. Instead, this will typically need to be worked out for any specific set of functions.

## Conditional and Joint Expectations and Variances
Up until this point we have considered the marginal probability distribution when exploring the measures of central tendency and spread. These help to summarize the marginal behaviour of a random quantity, cpaturing the distribution of, for instance, $X$ alone. When introducing distributions, we also made a point to introduce the conditional distribution as one which is particularly relevantwhen there is extra information. The question ``what do wwe expect to happen, given that we have an additional piece of information?'' is not only well-defined, but it is an incredibly common type of question to ask. To answer it, we require **conditional expectations**.

TODO: Include set of questions relating to conditional expectation.

In principle, a conditional expectation is no more challenging to calculate than a marginal expectation. Suppose we want to know teh  expected value of $X$ assuming that we know that a second random quantity, $Y$ has taken on the value $y$. We write this as $E[X|Y=y]$, and all we do is replace $p_X(x)$ with $p_{X|Y}(x|y)$ in the defining relationship. That is $$E[X|Y=y] = \sum_{x\in\mathcal{X}}xp_{X|Y}(x|y).$$ In a sense, we can think of the conditional distribution of $X|Y=y$ as simply being a distribution itself, and then work with that no differently. The conditional variance, which we denote $\text{var}(X|Y=y)$ is also  exactly the same.

TODO: Include an example.

Above we supposed that we knew that $Y=y$. However, sometimes we want to work with the conditional distribution more generally. That is, we want to investigate the behaviour of $X|Y$, without yet knowing what $Y$ equals. We can use the same procedure as above, however, this time we leave $Y$ unspecified. We denote this as $E[X|Y]$, and this expression will be (in general) a function of $Y$. Then, whenever a value for $Y$ is observed, we can simply specify $Y=y$, deriving the specific value. In practice, we will typically compute $E[X|Y]$ rather than $E[X|Y=y]$, since once we have $E[X|Y]$ we can easily find $E[X|Y=y]$ for *every* value of $y$.

TODO: Show example.

Since $E[X|Y]$ is a function of an unknown random quantity, $Y$, $E[X|Y]$ is also a random variable. It is a transformation of $Y$, and as such, it will have some distribution, some expectation, and some varaince itself. This is often a confusing concept when it is first introduced, so to recap: $X$ and $Y$ are both random variables; $E[X] and $E[Y]$ are both constant, numerical values describing the distribution of $X$ and $Y$; $E[X|Y=y]$ and $E[Y|X=x]$ are each numeric constants which summarize the distribution of $X|Y=y$ and $Y|X=x$ respectively; $E[X|Y]$ and $E[Y|X]$ are functions of $Y$ and $X$, respectively, and can as such be seen as transformations of (and random quantities depending on) $Y$ and $X$ respectively.

We do not often think of the distribution of $E[X|Y]$ directly, however, there is a very useful result about both its expected value and its variance, which will commonly be exploited. Specifically, if we take the expected value of $E[X|Y]$ we will find that $E[E[X|Y]] = E[X]$. Note that since $E[X|Y] = g(Y)$ for some transformation, $g$, the outer expectation is taken with respect to the distribution of $Y$. Sometimes when this may get confusing we will use notation to emphasize this fact, specifically, $E_Y[E_{X|Y}[X|Y]] = E_X[X]$. This notation is not necessary, but it can clarify when there is much going on, and is a useful technique to fallback on. \begin{align*}
E_Y[E[X|Y]] &= \sum_{y\in\mathcal{Y}} E[X|Y]p_Y(y) \\
&= \sum_{y\in\mathcal{Y}}\left(\sum_{x\in\mathcal{X}}xp_{X|Y}(x|Y)\right)p_Y(y) \\
&= \sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}}x\frac{p_{X,Y}(x,y)}{p_Y(y)}p_Y(y)\\
&= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}xp_{X,Y}(x,y)\\
&= \sum_{x\in\mathcal{X}} xp_X(x)\\
&= E[X].\end{align*}

TODO: Include example.

This property, that $E[E[X|Y]] = E[X]$ is important enough that it receives its own name: **the law of total expectation**. In the same way that it is sometimes easier to first condition on $Y$ in order to compute the marginal distribution of $X$ via applications of the law of total probability, so too can it be easier to first work out conditional expectations, and then take the expected value of the resulting expression. This adds on to the so-called ``conditioning arguments'' that were discussed previously, allowing a technique to work out the marginal mean indirectly.

TODO: Show example use case of LOTE.

While the conditional expectation is used quite prominantly, the conditional variance is less central to the study of random variables. As discussed, briefly, the conditional variance is given by the same variance relationship, replacing the marginal probability distribution with the conditional one (just as with expectations). Just as with expectations, $\text{var}(X|Y=y)$ is a numeric quantity given by $E[(X-E[X|Y=y])^2|Y=y]$ and $\text{var}(X|Y)$ is a random variable given by $E[(X-E[X|Y])^2|Y]$. This means that when working with the general, $\text{var}(X|Y)$, we can also consider taking expectations of the resulting transformation.

TODO: Include examples.

A final result relating to conditional expectations and varainces connects the two concepts. This is known as **the law of total variance**. For any random variables $X$ and $Y$, we can write $$\text{var}(X) = E[\text{var}(X|Y)] + \text{var}(E[X|Y]).$$ This result can be viewed as decomposing thevariance of a random quantity into two separate components, and comes up again in later statistics courses. At this point we can view this as a method for connecting the marginal distribution through the conditional variance nad expectation. 

TODO: Examples of this.

The final set of techniques to consider for now relate to making use of the joint distribution between $X$ and $Y$. Specifically, if we have any function of two random variables, say $g(X,Y)$ and we wish to find $E[g(X,Y)]$. This follows all of the expected derivations that we have used so far, this time replacing the marginal with the joint distribution. That is, $$E[g(X,Y)] = \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g(x,y)p_{X,Y}(x,y).$$ For instance, if we want to consider the product of two random variables, we could use this technique to determine $E[XY]$. The variance extends in the same manner as well.

TODO: Include example.

This defining relationship allows us to work out the expected value of a linear combination of two random variables. That is \begin{align*}
E[X+Y] &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}(x+y)p_{X,Y}(x,y) \\
&= \sum_{x\in\mathcal{X}}x\sum_{y\in\mathcal{Y}}p_{X,Y}(x,y) + \sum_{y\in\mathcal{Y}}y\sum_{x\in\mathcal{X}}p_{X,Y}(x,y) \\
&= \sum_{x\in\mathcal{X}}xp_X(x) + \sum_{y\in\mathcal{Y}}yp_Y(y) \\
&= E[X] + E[Y].\end{align*}

The same property does not apply with varainces, at least not in general. To see this, consider that \begin{align*}
E[(X+Y-E[X]-E[Y])^2] &= E[((X-E[X])+(Y-E[Y]))^2] \\
&= E[(X-E[X])^2] + E[(Y-E[Y])^2] + 2E[(X-E[X])(Y-E[Y])] \\
&= \text{var}(X) + \text{var}(Y) + 2E[(X-E[X])(Y-E[Y])].\end{align*} The term that impedes the linear relationship, $E[(X-E[X])(Y-E[Y])]$ can be computed as any joint function can be. This quantity, however, is particularly important when considering the relationship between two random variables. This is called the **covariance** and it is a measure of the relationship between $X$ and $Y$. Typically we write $E[(X-E[X])(Y-E[Y])] = \text{cov}(X,Y)$ so that $$\text{var}(X+Y) = \text{var}(X) + \text{var}(Y) + 2\text{cov}(X,Y).$$

TODO: Include example.

The covariance behaves similarly to the varaince. We can see directly from the definition that $\text{cov}(X,X) = \text{var}(X)$. Moreover, using similar arguments to those used for the varaince, we can show that $$\text{cov}(aX+b,cY+d) = ac\text{cov}(X,Y).$$ Covariances remain linear, so that $$\text{cov}(X+Y,X+Y+Z)=\text{cov}(X,X)+\text{cov}(X,Y)+\text{cov}(X,Z)+\text{cov}(Y,X)+\text{cov}(Y,Y)+\text{cov}(Y,Z).$$ These make covariances somewhat nicer to deal with than variances, and on occasion it may be easier to think of variances as covariances with themselves.

TODO: Example? Maybe.

It is worth considering, briefly, the ways in which conditional and joint expectations interact. Namely, if we know that $Y=y$, then the transformation $g(X,y)$ only has one random component, which is the $X$. As a result, taking $E[g(X,Y)|Y=y] = E[g(X,y)|Y=y]$. If instead we use the conditional distribution without a specific value, we still have that $Y$ is fixed within the expression, it is just fixed to an unknown quantity. That is $E[g(X,Y)|Y]$ will be a function of $Y$. We saw before that $E[E[X|Y]] = E[X]$, and the same is true in the joint case. Thus, one technique for computing the joint expectation, $g(X,Y)$ is to first compute the conditional expectation, and then compute the marginal expectation of the resulting quantity.

TODO: example.

## Independence in all of this
Whenever we can assume independence of random quantities, this allows us to greatly simplify teh expressions we are dealing with. Recall that the  key defining relationship with independence is that $p_{X,Y}(x,y) = p_X(x)p_Y(y)$. Suppose then that we can write $g(X,Y) = g_X(X)g_Y(Y)$. For instance, for the covariance we have $g(X,Y)=(x-E[X])(Y-E[Y])$ and so $g_X(X) = X-E[X]$ and $g_Y(Y) = Y-E[Y]$. If we  want to compute $E[g(X,Y)]$ then we get \begin{align*}
E[g(X,Y)] &= E[g_X(X)g_Y(Y)] \\
&= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g_X(x)g_Y(y)p_{X,Y}(x,y) \\
&= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g_X(x)g_Y(y)p_X(x)p_Y(y) \\
&=\sum_{x\in\mathcal{X}}g_X(x)p_X(x)\sum_{y\in\mathcal{Y}}g_Y(y)p_Y(y)\\
&= E[g_X(X)]E[g_Y(Y)].\end{align*} Thus, whenever random variables are independent,  we have the ability to separate them over their expectations.

TODO: example.

Consider what this means, in particular, for the covariance between independent random variables. If $X\perp Y$ then \begin{align*}
\text{cov}(X,Y) &= E[(X-E[X])(Y-E[Y])] \\
&= E[(X-E[X])]E[(Y-E[Y])] \\
&= (E[X]-E[X])(E[Y]-E[Y]) \\
&= 0.\end{align*} That is to say, if $X$ and $Y$ are indepdent, then $\text{cov}(X,Y)=0$. As a result of this, for independent random variables $X$ and $Y$ we also must have that $\text{var}(X+Y)=\text{var}(X)+\text{var}(Y)$. It is critical to note that this relationship does not go both ways: you are able to have $\text{cov}(X,Y) = 0$ even if $X\not\perp Y$.

TODO: Include example of independence.

While we have primarily focused on joint and conditional probabilities with two random variables, the same procedures and ideas apply with three or more as well. The relevant joint distribution, or conditional distribution would simply need to be substituted in definitions. Often the complexity here becomes a matter of keeping track of which quantities are random, and which are not. For instance, if we have $X,Y,Z$ as random variables, then $E[X|Y,Z]$ is a random function of $Y$ and $Z$. We will still have that $E[E[X|Y,Z]] = E[X]$, however, the outer expectation is now the joint expectation with respect to $Y$ and $Z$. As a result, we can also write $E[E[X|Y,Z]|Y]$. The first expectation will be with respect to $X|Y,Z$, while the outer expectation is with respect to $Z|Y$. This becomes a useful demonstration for when making the distribution of the expectation explicit helps to clarify what is being computed. As a general rule of thumb, the innermost expectations will always have more conditioning variables than the outer ones: each time we step out, we peel back one of hte conditional variables until the outermost is either a marginal (or joint). This will help to keep things clear.
```{r}
#| echo: false
```
# The Core Concepts of Probability
## Assigning Probabilities and The Equally Likely Outcome Model
There are a plethora of ways to assign probabilities to different events. At the most basic of levels any rule that maps from the space of possible events to real numbers between $0$ and $1$ are possible candidates for probability assignment. That is, probability assignment at its core is simply a set of rules which says "for this event, assign this probability."

:::{#exm-coin-toss}
## Coin Toss Probabilities
Suppose that the fair coin used by Charles and Sadie is tossed one time. Write down the probability assignments relating to this experiment.

::::{.callout .solution collapse='true'}
## Solution
In this case we have $\mathcal{S} = \{\text{H},\text{T}\}$. Thus, the possible events for which we need to assign probabilities are $\emptyset$, $\{\text{H}\}$, $\{\text{T}\}$, and $\{\text{H},\text{T}\} = \mathcal{S}$. For any probability model we have $P(\emptyset) = 0$ and $P(\mathcal{S}) = 1$. When we say that a coin is "fair" we are saying that $P(\text{T}) = P(\text{H})$, and since these are the only two possible outcomes in the sample space, we must have that they each have probability $0.5$.
::::
:::

Not every assignment of probability values is going to be valid. Suppose, for instance, that we have a six-sided die, each side labelled with a number from one to six. If I told you that there was a probability of $0.5$ that it comes up $1$, $0.5$ that it comes up $2$, $0.5$ that it comes up $3$, $0.5$ that it comes up $4$, $0.5$ that it comes up $5$ and $0.5$ that it comes up $6$, you would probably call me a liar.^[Or else conclude that I was mistaken and maybe should not be teaching probability.] If, as we have previously seen, probabilities represent the long-run proportion of time that a particular event is observed, we cannot have $6$ different outcomes each occurring in half of all cases.

beyond the requirements that we can impose on what constitutes a "valid" probability function, we have another concern: scalability. It is perfectly acceptable to indicate that in an experiment with $3$ outcomes, the first has a probability of $0.25$, the second of $0.3$, and the third of $0.45$. What if the experiment has $100$ possible outcomes? Or $1000$? It quickly becomes apparent that enumerating the probabilities of each event in the sample space is an efficient way of assigning probabilities in practice.

A core focus of our study of probability will be on finding techniques that allow us to efficiently encode probability information into manageable objects. Once we have done this, we will be in a position where we can manipulate these (comparatively) simple mathematical quantities in order to make statements and conclusions about any of the events of interest, even if they have never been explicitly outlined as having an assigned probability.

While we will consider myriad methods for accomplishing these goals throughout our study of probability, we begin  with a very useful model which simplifies probability assignment,  without any added complexity,  and creates a solid foundation for us to explore the properties of probability models. We start by considering **equally likely outcomes.** As the name suggests, the probability model considering equally likely outcomes assigns an equal probability to every possible outcome of the experiment. This is a probability model that we are already distinctly familiar with: flipping a coin, rolling a die, or drawing a card are all examples of experiments which rely on the equally likely outcomes framework.

:::{.remark}
## Statisticians and Urn Models
In statistics and probability courses and books you will often have instructors or authors using fairly simple models to illustrate probability concepts. There will often be questions relating to coin tosses, and dice, and decks of cards, and everyone's favourite: urns. It will very frequently be the case that a statistics question will state that there is an urn with some combination of coloured balls within it, from which you will be selecting some number either with or without replacement. The frequency of these types of examples and questions often feels disconnected from the frequent refrain that "uncertainty is all around us" and that "statistics is relevant to every aspect of our world!"^[One of the most famous quotes from a statistician was a thought shared by John Tukey, stating "The best thing about being a statistician is that you get to play in everyone's backyard." This is a common refrain, and one rooted in truth. Statistics is everywhere, across every field of human inquiry, and can help us make sense of everything from the trivial to the deeply important.] Why is it that we seldom see questions or examples that are directly tied to these wide spread applications of the lessons and techniques being taught?

In part these simple experiments are cleaner to handle than "real world" situations. We can easily assume that a die is fair and that takes care of any unsuspecting wrinkles that will necessarily come along with the "real world". This is not dissimilar to working under the assumption of frictionless surfaces in introductory physics, or assuming that human beings are rational in economics. Another key point is that most of us have deep familiarity with dice, and coins, and cards.^[This does not help to explain why we use urns so much, of course. When was the last time any of us drew a ball from an urn?] The same is not going to be true of stories that are derived from different use cases in the real world. A final important point, and this will be something we see in depth in the coming chapters, is that from a statistical point of view: there is no difference. Once we have the tools to work with these quantities, we have the tools to work with any of the quantities. This actually distinguishes the use of these types of examples in statistics and probability from those for other subjects: at no point is anything that we are learning incorrect, or overly simple - we are just focusing on the raw probabilistic nature of the phenomenon. As a result, we will continue to see these simple models in these notes, however, I would encourage you whenever possible to hold a topic in mind that matters more to you (whatever that may be) and start trying to draw the parallels between (for instance) rolling dice, and whatever it is that you may care about.
:::

If we have an experiment with a sample space $\mathcal{S}$ which has $|\mathcal{S}| = k$^[Note that, when we have a set, using the absolute value symbols $|\cdot|$ stands for the **cardinality** of the set. Cardinality is just a fancy way of saying the size or the number of elements that the set has in it.] total elements, then each element of the sample space occurs with probability $\frac{1}{k}$. In the case of the coin toss example, $\mathcal{S} = \{\text{H}, \text{T}\}$, and so $k=2$ and each outcome occurs with probability $\frac{1}{2}$. in the case of drawing a card at random, there are $52$ different outcomes, and so $k=52$, and the probability of drawing any particular card is $\frac{1}{52}$.

It is critically important to recognize that the equal probability model assigns equal likelihood to the possible outcomes of an experiment, not the possible  events of interest. It will not be the case in general that all events have the same probability. To make this concrete, consider the events $A$ "the ace of spades is drawn" and $B$ "any spade is drawn". It is quite clear that $B$ happens more frequently than $A$, even though we have said that this is an experiment with equally likely outcomes. Remember: an outcome is an observation from a single experimental run, an event is any subset of these possible outcomes.

A core goal is then bridging the gap between the probability of an outcome - something which in the equally likely outcomes framework we know - and the probability of an event - the quantity we are actually interested in. In order to do so, we will next consider the rules of probability, introducing the properties that are required for valid probability assignments and the techniques for  manipulating probabilities to calculate the quantities of interest.

## The Axioms of Probability
We have previously seen that not every probability assignment can be valid. For instance, assigning $0.5$ probability to each outcome on a die leads to a nonsensical scenario. with just a little imagine we can conjure equally nonsensical scenarios in other regards. For instance, it would make very little sense to discuss the probability of an event being a negative value. What would it mean for an event to occur in a negative proportion of occurrences? Alternatively, we can consider two events that are nested in one another: say event $A$ is that we draw the ace of spades, and event $B$ is that we draw any spade. Every single time that $A$ happens, we know that $B$ also happens. But there are ways that $B$ can occur where $A$ does not (for instance, the Queen of spades being drawn). If I told you the probability of $A$ was $0.5$ and the probability of $B$ was $0.2$, this would violate our base instincts: how can it be more likely to draw the ace of spades than it would be to draw any spade at all?

Often in mathematics when we have an intuitive set of rules that particular quantities must obey, we work to add formality through defining properties of these concepts. To this end, we can define the key properties that probabilities must obey in order to be well-defined, valid probabilities.

It turns out that with three (fairly) basic properties, we can completely specify what must be true in order for a set of probabilities to be valid. 

1. **Unitary** Every valid set of probabilities must assign a probability of $1$ to the full sample space. That is, $P(\mathcal{S}) = 1$. This is an intuitive requirement as  every  time the experiment is run we observe an outcome in the sample space, and as  a result, in every experimental run the event $\mathcal{S}$ occurs.
2. **Non-negative:** We require that every probability is non-negative. We can have probabilities of $0$, but we can never have a probability less than zero. Again, this is sensible^[What would it mean to have a negative probability? It is perhaps a more interesting question than it seems at first glance. It is a topic that has come up in some pretty strange places and, while it is not presently sensible to call them "probabilities" in a traditional sense, there are interesting results which follow.] but is important to include in our formalization. Specifically, for every event $E$, $P(E) \geq 0$.
3. **Additivity:** the final property  requires slightly more parsing on first pass. Suppose that we define a sequence of events,  $E_1, E_2, E_3, \dots$ such that no two events have any overlap. That is, $E_j \cap E_\ell = \emptyset$ for all $\ell\neq j$. Then, the final property we require for probabilities is that $$P\left(\bigcup_i E_i\right) = \sum_i P(E_i).$$ That is, the probability of the union of disjoint events is the summation of the probability of these events.

It is worth dwelling slightly on property (or axiom) 3 further. Consider the case of drawing a card at random from a deck of $52$ cards. Using the equally likely outcome model for probability we know that the probability that any card is drawn is given by $\frac{1}{52}$. If I were to ask  "what is the probability you draw that ace of spades?" under this model you can respond, immediately,  with $\frac{1}{52}$. Now, if I were to ask "what is the probability that you draw the ace of spades or two  of spades?" intuitively you likely figure that this will be $\frac{2}{52}$. Note that the event $E_1$, "draw the ace of spades" and the event $E_2$ "draw the two of spades", are disjoint events. Moreover, recall that the union is the "or" and so $E_1\cup E_2$ is the same as $E_1$ or $E_2$. Taken together then, $$P(E_1\cup E_2) = P(E_1) + P(E_2).$$ The axiom of additivity simply extends this intuition to an arbitrary number of events.

:::{#exm-additivity}
## Basic Additivity
Still unsure of how best to go about using cards to replace their coin game, Charles and Sadie are considering various different events and trying to understand their probabilistic behaviour. They take $S$, $C$, $H$, and $D$ to be the events that a spade, club, heart, or diamond are drawn (respectively), and they take $C_j$ to be the event that a card with denomination $j$ is drawn ($j$ ranging from ace with $1$ through King with $13$). If they consider the union of any two (or more) of these events when canthey leverage properties of additivity? When can't they?


::::{.callout .solution collapse='true'}
## Solution
In order to use the properties of additivity it is required that the two events are disjoint. Note that taking any two (or more) of $S$, $C$, $H$, and $D$ will lead to disjoint events. There is no way to draw a card which has two suits on it at once. Similarly, taking any two (or more) of $C_j$ will lead to disjoint events. However, mixing any of the suited events ($S$, $C$, $H$, and $D$) with any $C_j$ will not be disjoint. 

Consider $S\cap C_1$. The ace of spades is in $S$ since it is a spade and it is in $C_1$ since it is an ace. As a result, $S\cap C_1 = \{\text{Ace of Spades}\}$. Because of this we are not able to say that $P(A \cup C_1) = P(A) + P(C_1)$. However, we can say that $$P(S\cup C\cup H\cup D) = P(S) + P(C) + P(H) + P(D),$$ and could do the same with any subset of these sets. Similarly, we can take $$P\left(\bigcup_{j=1}^{13} C_j\right) = \sum_{j=1}^{13} P(C_j),$$ or any of the subsets there.
::::
:::

These three axioms fully define valid probabilities. Any mechanism that assigns probability values to events which conforms to these rules will assign valid probabilities. While it may seem counter intuitive that such basic rules fully define our notion of a probability, these rules readily give rise to many other properties that are indispensible when working with probabilities.

## Secondary Properties of Probabilities
Using the previously indicated axioms of probability we are able to derive many useful **secondary properties**. These properties will frequently be used to actually compute different probabilities, and are helpful to become familiar with. All of the following properties follow directly from the axioms, though, some are more clear than others. For the following we take $E$ and $E_1,E_2,E_3,\dots$ to be arbitrary events on some well defined sample space.

1. $P(E^C) = 1-P(E)$, and equivalently, $P(E) = 1 - P(E^C)$.
2. $P(\emptyset) = 0$.
3. $P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1\cap E_2)$.
4. $$P(E_1 \cup E_2 \cup E_3) = P(E_1) + P(E_2) + P(E_3) - P(E_1\cap E_2) - P(E_1 \cap E_3) - P(E_2 \cap E_3) + P(E_1 \cap E_2 \cap E_3).$$
5. If $E_1 \subset E_2$ then $P(E_1) \leq P(E_2)$. 

:::{.callout-warning icon="false" collapse="true"}
## Proofs of the Secondary Properties of Probability
It may be instructive to see how these properties are derived. Doing so generates added familiarity with manipulating probability expressions and helps to encourage deeper understanding. 

1. Note that, for any event $E$, by definition we have $E \cup E^C = \mathcal{S}$ and $E \cap E^C = \emptyset$. As a result, we can apply **additivity** to the partition $E, E^C$ which gives $P(E \cup E^C) = P(E) + P(E^C)$. However, since $E\cup E^C = \mathcal{S}$, then we know that $P(E \cup E^C) = P(\mathcal{S}) = 1$ by the **unitary** property. Taken together this tells us that $1 = P(E) + P(E^C)$, and rearranging gives $P(E^C) = 1 - P(E)$, or $P(E) = 1 - P(E^C)$, as required.

2. We know that $\mathcal{S}^C = \emptyset$. Using the first property, $P(E) = 1 - P(E^C)$, and so taking $E = \emptyset$ gives $P(\emptyset) = 1 - P(\mathcal{S}) = 1 - 1 = 0$, by the **unitary** property.

3. Here note that $E_1 \cup E_2$ can be written as $E_1 \cup E_2'$ where $E_2' = E_2\cap E_1^C$. That is, $E_2'$ contains the outcomes from $E_2$ which were not shared by $E_1$. Then $E_1 \cap E_2' = \emptyset$ so we can write $P(E_1 \cup E_2) = P(E_1 \cup E_2') = P(E_1) + P(E_2')$, by **additivity**. Now, if we define $E_2^* = E_2\cap E_1$ then $E_2 = E_2' \cup E_2^*$, and $E_2'\cap E_2^* = \emptyset$. Thus, $P(E_2) = P(E_2'\cup E_2^*) = P(E_2') + P(E_2^*)$. Rearranging this gives $P(E_2') = P(E_2) - P(E_2^*)$, and we know that $P(E_2^*) = P(E_1 \cap E_2)$. Thus, plugging into what we found before we get $$P(E_1 \cup E_2) = P(E_1) + P(E_2') = P(E_1) +  P(E_2) - P(E_1 \cap E_2).$$

4. This follows exactly from the argument for (3). To see, first consider $E_2 \cup E_3$ to be an event itself, say $E_4$. Then we can apply the above result to $E_1 \cup E_4$. And then we need only repeat the process for the remaining terms. 

5. We can rewrite $E_2$ as $E_1 \cup (E_2 \cap E_1^C)$. These two sets are disjoint, since the one is $E_1$ and the other must contain only elements in $E_1^C$. Then, $P(E_2) = P(E_1) + P(E_2 \cap E_1^C)$ by **additivity**. Then, through the **non-negative** property we know that $P(E_2 \cap E_1^C) \geq 0$, and so rearranging we have $P(E_1) = P(E_2) - P(E_2\cap E_1^C) \leq P(E_2)$.  
:::

These properties are immensely useful when computing probabilities. In fact, these secondary properties will be used with more frequency than the basic axioms when manipulating probabilities in practice. It is worth building comfort with these properties, early and often, as they will assist in manipulating all probability expressions in the future. 

:::{#exm-complement-trick}
## Unmatched Six Sided Dice
Charles and Sadie, not all together content with the progress through decks of cards, are considering games with dice. Suppose that they have two, fair, six-sided dice. They are interested in the probability that the two dice show different numbers when they are rolled. What is this probability?

::::{.callout .solution collapse='true'}
## Solution
Here, the key is to realize that the probability is easier to solve when considering the complement rather than the event itself. Notably, rolling two, fair, six-sided dice gives a total of $36$ possible outcomes. Of these, exactly $6$ have equal numbers showing on both dice. Thus, the probability that the two dice show the **same** number is going to be $\frac{6}{36} = \frac{1}{6}$. Then, using the fact that $P(E) = 1 - P(E^C)$, and that taking $E$ to refer to the event where the two dice show different numbers, then $E^C$ refers to the event that the two dice show the *same* number. As a result, the probability that we want is $P(E) = 1 - \frac{1}{6} = \frac{5}{6}$.
::::
:::

:::{#exm-complement-trick-two}
## Unmatched Arbitrary Dice 
Charles and Sadie, working from their intrigue about dice, have decided that instead of using two six-sided dice, they wish to take two dice of possibly different sizes. Suppose that the first die has $d_1$ sides and the second has $d_2$ sides, and that both dice are otherwise fair. They are interested in the probability that the two dice show different numbers when they are rolled. What is this probability?

::::{.callout .solution collapse='true'}
## Solution
This problem is conceptually no different from @exm-complement-trick. There will be a total of $d_1\times d_2$ possible combinations of the two dice to be rolled.^[Note: if this is not yet clear to you, that's okay! In the very next section we begin to discuss how to count the possible combinations in these types of scenarios.] Of these, the dice will match in $\min\{d_1, d_2\}$ events.^[Suppose that $d_1 = 2$ and $d_2 = 4$. Then here, the dice can match when they show either $1$ or $2$, but if the second die shows $3$ or $4$ there is no possibility of having a match at all.] Then, with the same complement trick discussed above, we get $$P(E) = 1 - P(E^C) = 1 - \frac{\min\{d_1,d_2\}}{d_1\times d_2}.$$
::::
:::

While these properties hold in general for all probability models, it is instructive to consider to focus on the equal probability model to begin building familiarity with probability broadly. These properties allow us to take events -- whether compound or simple -- and combine, rewrite, and manipulate expressions to assist in the handling of the computations. Eventually, however, we require the ability to  assign numerical values to these probabilities.

Consider a simple event, $A$. Recall that a simple event is defined as  a possible outcome of an experiment, and so in this case,  $A$ corresponds directly to an event that may be observed. If our sample space is $k$ elements large, then $P(A) = \frac{1}{k}$ in this framework. For instance, if $A$ is the event that a two is rolled on a six-sided fair die, then $P(A) = \frac{1}{6}$. 

Now, suppose that a compound event is defined, $B$. By definition, a compound event can be expressed as a set of possible outcomes from the experiment. Suppose that we enumerate these possible events as $b_1, b_2, \dots, b_\ell$. Then we know that $B$ occurs if any of $b_1,b_2,\dots,b_\ell$ occur. Each $b_j$ are elements of the sample space and correspond to possible outcomes of the experiment. As a result, we know that $P(b_j) = \frac{1}{k}$, based on the equal probability assumption. Now, if we take any two distinct $b_j$, say $b_i$ and $b_j$, we know that they must be disjoint: $b_i \cap b_j = \emptyset$. This is because in an experiment run only one outcome can occur -- there is nothing common between these two events, by definition. Moreover, we can say that $B = b_1 \cup b_2 \cup\cdots\cup b_\ell$.

Using the axioms of probability outlined above we therefore know that $P(B) = \sum_{j=1}^\ell P(b_j) = \sum_{j=1}^\ell \frac{1}{k} = \frac{\ell}{k}$. This holds in general for any compound event in  this setting. If we take $B$ to be the event that an even number is rolled on a six-sided die, then we would have $b_1$ is the event that a two is rolled, $b_2$ is the event that a four is rolled, and $b_3$ is the event that a six is rolled. There are three such events, and so the probability that an even number is rolled must be $\frac{3}{6} = 0.5$, which matches our intuition.

If we consider what this process is doing at its core, we can reframe the calculation as counting up the number of ways that event can happen and dividing by the total number of events. In our previous discussion, there were $\ell$ ways of $B$ occurring, a total of $k$ outcomes, and so the probability becomes $\frac{\ell}{k}$. in the equal probability model, this will always be the case, the probability of any event $A$ occurring is given by $$P(A) = \frac{N_A}{k},$$ where $N_A$ is the number of unique ways that $A$ can occur.  In other words, $N_A$ is the size of the set $A$.

As a result of the realization, a large part of computing probabilities is in the counting of possible outcomes corresponding to different events. If we can determine the count of $A$, $N_A$, and the count of the total number of occurrences, $k$, then we can determine the probability of $A$. This study of counting is known as **combinatorics**, and it is where we will turn our attention next.

## Combinatorics
### The Multiplication Rule
Fundamentally, counting is a matter of assessing the size of a collection of items. Sometimes, this is very straightforward: if you want to count the number of students in a classroom, you  start  at $1$ and enumerate upwards through the integers. To count the number of days until the next Holiday, you do the same thin, sizing up the time that has to pass. If you really need to sleep, perhaps you will count imaginary sleep until you drift off. There is not much to this type of counting, and it is certainly deeply familiar to you all. However, it is also quite limited in its utility.

Imagine that you are interested in determining how many possible ways there are of arranging a deck of $524 cards. You could of course arrange them in a particular order, then count each of those. That would take a tremendous amount of time, so perhaps instead of using an actual deck you just write down the combinations. Still, each combination is going to be $52$ cards long, and keeping track of that all will be a tremendous challenge. This seems like an approachable question, and yet, it illustrates how complicated (and large) these types of "counting" problems can become, very quickly.

Fortunately for us there are some strategies for simplifying these problems down, some of which you are likely already familiar with. Think about trying to form an outfit where you have $4$ different sweaters, $3$$ pairs of pants, and $2$ options for your shoes. Suppose that any combination of these will work well. How many are there? Well, if you have already picked your sweater and pants, then there are going to be $2$ different outfits using these: one with each of the pairs of shoes. This is true for each possible sweater-pant combination, and so we can count $2$ for each one these. In other words, to get the total number of outfits we multiply the number of sweater pant combinations by the number of shoe options ($2$). the same rationale can be applied to count the total number of sweater-pant combinations. For each sweater, there are $3$ pairs of possible pants, and so to get the total number there we can take $3$ for each possible sweater, or in other words, $3\times 4$. 

Taken together then we have $4\times 3\times 2 = 24$ total possible outfits. Another way of framing this is that we have to make three sequential decisions: which of the $4$ sweaters, which of the $3$ pants, and which of the $2$ shoes are to be worn. When we do this we multiply through the number of alternatives at each decision point to get the total number of combinations.

This is known as the **multiplication rule for counting**, and it is a very general rule for counting up total combinations. Any time that we have to make a sequence of $k$ choices, and each choices $j=1,\dots,k$ has $n_j$ options, then the total number of combinations will be $N = n_1\times n_2\times\cdots\times n_k$.

:::{#exm-counting-coffee-orders}
## Counting Coffee Orders
When Charles and Sadie are out for coffee, Sadie enjoys ordering the same thing each time: a black coffee and vegan chocolate chip cookie. Charles, on the other hand, has decided to work through the entire menu of the local coffee shop, each day ordering a drink, with one add-in, and a snack. If there are $10$ different drinks, $8$ possible add-ins, and $12$ different snacks, how many trips to the coffee shop will it take until Charles has tried it all?

::::{.callout .solution collapse='true'}
## Solution
This necessitates an application of the multiplication rule for counting. Specifically, we can view this as three sequential decisions, where the first decision is which drink (with $n_1 = 10$), the second decision is which add-ins (with $n_2 = 8$), and the third decision is which snack (with $n_3 = 12$). Taking the product gives the total number of combinations as $10\times 8\times 12 = 960$. As a result, it will take $960$ visits (assuming that nothing on the menu changes!) to try all combinations. 
::::
:::


:::{#exm-rolling-sequence-of-dice}
## Sequence of Dice Rolls
Charles and Sadie have been enjoying playing with dice, but they lost one of the two they had. As a result, they are trying to come up with games revolving around rolling a single die. They decide to try a game called "six is lava", where they roll a single six-sided die $10$ times in a row. If they get $1$ or more sixes, they lose the game. They are not sure if $10$ is the correct number of rolls to use. What is the probability that they lose on any given round of this game?

::::{.callout .solution collapse='true'}
## Solution
Once again this is a scenario where using the complement simplifies the problem. If we asked "what is the probability that no 6's are rolled, on $10$ rolls of the die" then we can count the number of possibilities through an application of the multiplication rule. In particular, there are going to be $5$ options which are not $6$ at each possible step. We can view this as have $n_1 = n_2 = \cdots = n_{10} = 5$. Thus, the total number of ways of rolling **no** sixes is $5\times5\times5\times\cdots\times5 = 5^{10}$. 

Essentially the same process can be used to count the total number of possible rolls, replacing $5$ with $6$ to get the denominator. This means that there are $6^{10}$ total sequences of $10$ rolls, and $5^{10}$ which contain no sixes. As a result, taking $E$ to represent the probability that we observe no sixes on $10$ rolls of the die, we would get $P(E) = \frac{5^{10}}{6^{10}}$. The question asks for $E^C$ and so we take $P(E^C) = 1 - P(E) = 1 - \left(\frac{5}{6}\right)^{10}$. This is approximately `r round(1 - (5/6)^(10), 4)`.

Note that if instead of $10$ flips they had $n$ flips, the probability would be $1 - \left(\frac{5}{6}\right)^n$. We can plot this over various values of $n$, to see how the number of flips impacts this probability. The probability of $0.5$ is marked and we can see that taking $4$ tosses gives an ever so slightly greater than $0.5$ probability of losing (`r round(1 - (5/6)^4, 4)`).
```{r}
#| echo: false 

n <- 0:20
probs <- 1 - (5/6)^n

plot(probs ~ n, main = "Probability of losing in 'six is lava.'", xlab = "Number of Tosses", ylab = "Probability of Losing", type = 'p')
abline(h = 0.5, lty = 3)
```
::::
:::

### Tree Diagrams
Sometimes it is helpful to express the counting rule graphically. To do so we rely on **tree diagrams**. A tree diagram puts each of the decisions in sequence, and draws a branch for each separate option. You start with the first choice, drawing one branch for each of the $n_1$ alternatives, labelling each. Then, at the second choice, you do the same process at the end of each of the branches you drew for choice $1$, this time drawing $n_2$ branches there (so you will have just drawn $n_1\times n_2$ branches). Then for each of those you draw the $n_3$ further branches, and so on and so forth until the end. 

If you want to know the total number of choices, you simply count the end points at the very end of the diagram. Each branch corresponds to a single option. To determine which combination of choices it corresponds to, you simply read off the branch labels at each branch you take. If you want to know how many possible combinations come with certain options selected, you can look at only those branches which are downstream from the choices that you care about.

![A generic tree diagram. Here the first choice has three different options and the second choice has two. We can see the six total combinations, labelled on the right of the diagram, and can trace the choices required to get there.](/graphics/ch3-tree-diagram-generic){#fig-tree-diagram-general}

While tree diagrams can be quite useful for visualizing a problem, they often grow to be overly complex. As a result, we need to fall back on the numerical representation afforded to us through the product rule for counting. Counting problems, in general, can very quickly become tremendously large and complex. For this reason, we have several tools to assist us in reducing this complexity based on common types of problems that we would like to count.

### The Factorial
The first useful tool for simplifying these problems is the **factorial**. The factorial of an integer, denoted by $x!$ (factorials are exciting because they always look like they are shouting!) is given by the product of all integers from $x$ to $1$. That is, $x! = x(x-1)(x-2)\cdots(2)(1)$. If we consider the product rule for counting then note that if $n_1=1$, $n_2=2$, ... , $n_k = k$, then the total number of options is $k\times(k-1)\times\cdots\times 1 = k!$. The most common reason that this comes up is when we want to order a collection of items. Suppose that you have $10$ books that you want to place on a shelf. You can view this as making $10$ sequential decisions: what book goes first, second, third, and so on. There are $10$ options for the first book, then $9$ for the second (any except for the first one), and then $8$ for the third (any except for the first $2$). This continues down to the last book, and so we conclude that there are $10\times9\times8\times\cdots\times1 = 10!$ ways of arranging these books.

:::{#exm-factorial}
## Seating in a (Full) Coffee Shop
One day Charles and Sadie walk into the coffee shop and find that it is completely full. There are ten seats and ten people sitting in them. They are disappointed that they do not have room to sit themselves, however, they are never ones to pass up an interesting probability question. 

a. How many different ways could these ten people have sat in these ten seats? 
b. If there are ten drinks that have been made, and one is to be passed out to each seat, how many different ways can these ten people sit in these ten seats, with each of these ten drinks? 
c. Alongside the ten drinks, there are ten snacks to be served up as well. How many different ways can these ten people sit in these ten seats, with each of these ten drinks, and each of these ten snacks?

::::{.callout .solution collapse='true'}
## Solution
TODO: 9
::::
:::

### Permutations and Combinations
Sometimes, we want to still order items from a collection, but we want to only use a subset of these times. That is, suppose that you have $20$ books, only $10$ of them will fit on the self, and so you want to know: how many ways can you put $10$ books on the shelf, in order, from your collection of $20$. Using the product rule of counting for this directly  we can recognize that there are $20$ options for the first, then $19$, then $18$, and so on until there are $11$ choices for the $10$th book to place. We can write this out in a seemingly strange way. \begin{align*}
    &\ \frac{20(19)(18)(17)(16)(15)(14)(13)(12)(11)(10)(9)(8)(7)(6)(5)(4)(3)(2)(1)}{10(9)(8)(7)(6)(5)(4)(3)(2)(1)} \\
    &= \frac{20(19)(18)(17)(16)(15)(14)(13)(12)(11)\cancel{(10)}\cancel{(9)}\cancel{(8)}\cancel{(7)}\cancel{(6)}\cancel{(5)}\cancel{(4)}\cancel{(3)}\cancel{(2)}\cancel{(1)}}{\cancel{10}\cancel{(9)}\cancel{(8)}\cancel{(7)}\cancel{(6)}\cancel{(5)}\cancel{(4)}\cancel{(3)}\cancel{(2)}\cancel{(1)}}
\end{align*}

This expression is $20!$ divided by $10!$, and gives the same as our argument from the product rule for counting directly. This is a more general result than our example with books would suggest. If we have $n$ items, and we want to choose $k$ of them and then order those choices, it will always be $n!$ divided by $(n-k)!$. We call this a **permutation**. Formally, we write $$P_{n,k} = \frac{n!}{(n-k)!}.$$

Permutations arise when we select ordered subsets from a collection. We often, in combinatorial problems, talk about ordering, though sometimes what we mean by this is slightly more abstract. Suppose that you want to form a committee with $5$ different people, each of which occupies a different role: the president, vice president, treasurer, note taker, and critic. If there are $30$ people to select for this, then there are $P_{30,5}$ total possible committees that can be formed. While there is not a sequential order here, we talk about this as being ordered since we can differentiate between the five roles. Instead of labelling them with their names, we could label them $1$ through $5$ and make the ordering more explicit. 

:::{#exm-permutations}
## Seating in a (Not Full) Coffee Shop
Still haunted by that time when the coffee shop was full, Sadie and Charles enter the coffee shop at a later date and find that, including themselves, there are only $7$ patrons in the store, and still the $10$ seats to choose from. 

a. How many different ways can the $7$ people sit in the $10$ different chairs?
b. If there are $10$ drinks on the menu, how many different ways can each person choose a chair and a drink? 
c. If the coffee shop can make only one of each drink, how does the previous total change?

::::{.callout .solution collapse='true'}
## Solution
TODO: 10
::::
:::

:::{.remark}
## 0!

TODO: 11
:::

Factorials compute the number of orderings for a set of objects, and permutations compute the number of ordered subsets from a collection of objects. What about when we do not wish to differentiate the order of subsets? Suppose that you still need to form a $5$ person committee, but you do not have explicit roles for the different members of the committee. Here we cannot use a permutation directly, as we know that this takes into account the order. Suppose that we formulate the ordered committee in a separate way: first, we select $5$ people without concern for their order, then we choose which order they will have.

If $M$ represents the number of unordered sets of $5$ from this population, the product rule for counting tells us that the total number of ordered committees will be $M\times 5!$, since there are $5!$ arrangements. Thus, we can write this down as $$P_{30,5} = \frac{30!}{25!} = M\times 5! \implies M = \frac{30!}{25!5!}.$$ Once again, this will be true far more broadly than our committee example: if we want to select $k$ items from a collection of $n$, we will have $n!$ divided by the product of $k!$ and $(n-k)!$. We refer to these as **combinations**. 

We will write $\binom{n}{k}$, which we read as "$n$ choose $k$", which translates to "select $k$ items from a population of $n$ total options, without concern for their order." Formally, a combination is defined as $$\binom{n}{k} = \frac{n!}{k!(n-k)!}.$$

To summarize: factorials allow us to order a complete collection, permutations allow us to select a subset with consideration of the ordering, and combinations allow us to select a subset from the collection without regard to the order. These three techniques can be used in combination with the product rule for counting to allow us to have very complex total summations. 

:::{#exm-combinations}
## Changing the Seating in the Coffee Shop
Some nights, the coffee shop hosts local music acts. Because of the added equipment, the coffee shop owners only keep out the number of seats that are going to be required based on the number of tickets that were sold. 

a. If there are $8$ tickets sold, how many different combinations of the $10$ chairs can get left out?
b. Suppose that only $6$ people end up showing up. How many different ways can the $6$ people sit in the $8$ chairs that are being selected from the $10$ total possibilities?

::::{.callout .solution collapse='true'}
## Solution
TODO: 12
::::
:::

:::{.remark}
## Less Common Counting Techniques
There are other types of counting problems which occasionally need to be considered. TODO: 13
:::

While combinatorics is a field of study on its own, with many intriguing tools and developments surrounding the enumeration of objects, for the purposes of simple probability models these tools will suffice. Ultimately, we care about counting since in the equal probability model, the probability of any event can be determined by counting the number of ways that the event can occur and dividing by the total number of outcomes that are possible. That is, we use these tools to derive $N_A$, the total number of ways that $A$ can occur, and $N$, the total number of experimental outcomes, and then we conclude that $$P(A) = \frac{N_A}{N}.$$

:::{#exm-poker-hands}
## Poker Hand Counts
During one of their conversations, Charles and Sadie were remarking how they never really played poker. As they understand it, in poker you are dealt a hand of $5$ cards and you want to use these $5$ cards to try to match certain sets of cards, some of which are more rare than others. Charles and Sadie start to get hung-up on discussions regarding "straights" and "flushes". 

A straight is any sequence of $5$ cards in ascending order (where aces can be low, or high). For instance, $7, 8, 9, 10, \text{J}$ of any suit. A flush, is any set of $5$ cards belonging to the same suit. Charles just *feels* that straights have to be more rare than flushes. 

a. How many different straights are there from a standard deck of cards?
b. How many different flushes are there from a standard deck of cards?
c. If dealt $5$ cards at random, what is the probability of a flush? What is the probability of a straight?
d. A straight flush occurs when you have $5$ cards in order, of the same suit. What is the probability of a straight flush?
e. If straight flushes were not counted as flushes, and not counted as straights, how do the probabilities of either hand change?

::::{.callout .solution collapse='true'}
## Solution
TODO: 14
::::
:::
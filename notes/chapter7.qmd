# Expectations and Variances with Multiple Random Variables

## Conditional Expectation
Up until this point we have considered the marginal probability distribution when exploring the measures of central tendency and spread. These help to summarize the marginal behaviour of a random quantity, capturing the distribution of $X$ alone. When introducing distributions, we also made a point to introduce the conditional distribution as one which is particularly relevant when there is extra information. The question "what do we expect to happen, given that we have an additional piece of information?" is not only well-defined, but it is an incredibly common type of question to ask.^[For instance, you might ask "how long do we expect a patient to live, given that they received a particular treatment?" or "how much do we expect this house to sell for, given it has a certain square footage?" or "how many goals do we expect this hockey team to score, given their current lineup?" A large number of questions which we may hope to answer using data can be framed as a question of conditional expectation.] To answer it, we require **conditional expectations**.

:::{#def-conditional-expectation}
The conditional expectation of a random variable, $X$, given a second random variable, $Y$, is the average value of $X$ when we know the value of $Y$. Specifically, we write $E[X|Y]$, and define this to be $$E[X|Y] = \sum_{x\in\mathcal{X}} xp_{X|Y}(x|y),$$ which is exactly analogous to the defining relationship for $E[X]$, replacing the marginal probability mass function with the conditional probability mass function.
:::

In principle, a conditional expectation is no more challenging to calculate than a marginal expectation. Suppose we want to know the expected value of $X$ assuming that we know that a second random quantity, $Y$ has taken on the value $y$. We write this as $E[X|Y=y]$, and we replace $p_X(x)$ with $p_{X|Y}(x|y)$ in the defining relationship. That is $$E[X|Y=y] = \sum_{x\in\mathcal{X}}xp_{X|Y}(x|y).$$ We can think of the conditional distribution of $X|Y=y$ as simply being a distribution itself, and then work with that no differently. The conditional variance, which we denote $\text{var}(X|Y=y)$ is defined in an exactly analogous manner, giving $$\text{var}(X|Y) = E[(X - E[X|Y])^2|Y].$$

:::{#exm-conditional-expectation}
## ...
TODO: Include an example computing the conditional expectation, specifically.

::::{.callout .solution collapse='true'}
## Solution

::::
:::

Above we supposed that we knew that $Y=y$. However, sometimes we want to work with the conditional distribution more generally. That is, we want to investigate the behaviour of $X|Y$, without yet knowing what $Y$ equals. We can use the same procedure as above, however, this time we leave $Y$ unspecified. We denote this as $E[X|Y]$, and this expression will be a function of $Y$. Then, whenever a value for $Y$ is observed, we can specify $Y=y$, deriving the specific value. We will typically compute $E[X|Y]$ rather than $E[X|Y=y]$, since once we have $E[X|Y]$ we can easily find $E[X|Y=y]$ for *every* value of $y$.

:::{#exm-conditional-expectation}
## ...
TODO: Include an example computing the conditional expectation.

::::{.callout .solution collapse='true'}
## Solution

::::
:::

## Conditional Expectations as Random Variables
Since $E[X|Y]$ is a function of an unknown random quantity, $Y$, $E[X|Y]$ is also a random variable.^[It is useful to keep in mind that anytime we do *anything* with a random variable, mathematically, we produce an additional random variable. If we think of a random variable as being some mathematical variable whose value depends on the results of an experiment, then if we take that value and apply a function to it we have a *new* value whose results also depend on the results of an experiment.] It is a transformation of $Y$, and as such, it will have some distribution, some expectation, and some variance itself. This is often a confusing concept when it is first introduced, so to recap: 

* $X$ and $Y$ are both random variables; 
* $E[X]$ and $E[Y]$ are both constant, numerical values describing the distribution of $X$ and $Y$; 
* $E[X|Y=y]$ and $E[Y|X=x]$ are each numeric constants which summarize the distribution of $X|Y=y$ and $Y|X=x$ respectively; 
* $E[X|Y]$ and $E[Y|X]$ are functions of $Y$ and $X$, respectively, and can as such be seen as transformations of (and random quantities depending on) $Y$ and $X$ respectively.

We do not often think of the distribution of $E[X|Y]$ directly, however, there are very useful results regarding its expected value and its variance, which will commonly be exploited. If we take the expected value of $E[X|Y]$ we will find that $E[E[X|Y]] = E[X]$. Note that since $E[X|Y] = g(Y)$ for some transformation, $g$, the outer expectation is taken with respect to the distribution of $Y$. Sometimes when this may get confusing we will use notation to emphasize this fact, specifically, $E_Y[E_{X|Y}[X|Y]] = E_X[X]$. This notation is not necessary, but it can clarify when there is much going on, and is a useful technique to fallback on. 

:::{.callout-tip icon="false"}
## The Law of Total Expectation
For any random quantities, $X$ and $Y$, the Law of Total Expectation states that $$E[X] = E[E[X|Y]].$$ That is, if we first compute the conditional expectation of $X$ given $Y$, then take the expected value of this quantity, we compute $E[X]$.
:::

In the same way that it is sometimes easier to first condition on $Y$ in order to compute the marginal distribution of $X$ via applications of the law of total probability, so too can it be easier to first work out conditional expectations, and then take the expected value of the resulting expression. 

:::{.callout-warning icon="false" collapse="true"}
## Proof of the Law of Total Expectation
To prove that law of total expectation, we note that $E[X|Y]$ is a random function of $Y$. As a result, we can apply the LOTUS to $E[X|Y]$ as a function of $Y$ when we take $E[E[X|Y]]$. Doing so yields, \begin{align*}
E_Y[E[X|Y]] &= \sum_{y\in\mathcal{Y}} E[X|Y]p_Y(y) \\
&= \sum_{y\in\mathcal{Y}}\left(\sum_{x\in\mathcal{X}}xp_{X|Y}(x|Y)\right)p_Y(y) \\
&= \sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}}x\frac{p_{X,Y}(x,y)}{p_Y(y)}p_Y(y)\\
&= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}xp_{X,Y}(x,y)\\
&= \sum_{x\in\mathcal{X}} xp_X(x)\\
&= E[X].\end{align*} The remainder of the proof, following an application of the LOTUS relies upon manipulating summations.
:::

:::{#exm-law-of-total-expectation}
## ...
TODO: Include an example computing the conditional expectation.

::::{.callout .solution collapse='true'}
## Solution

::::
:::

## Conditional Variance
While the conditional expectation is used often, the conditional variance is less central to the study of random variables. As discussed, briefly, the conditional variance is given by the same variance relationship, replacing the marginal probability distribution with the conditional one. Just as with expectations $\text{var}(X|Y=y)$ is a numeric quantity given by $E[(X-E[X|Y=y])^2|Y=y]$ and $\text{var}(X|Y)$ is a random variable given by $E[(X-E[X|Y])^2|Y]$. This means that we can consider the distribution, and critically the expected value of, $\text{var}(X|Y)$. A core result relating to conditional expectations and variances connects these concepts. 

:::{.callout-tip icon="false"}
## The Law of Total Variance
For any random variables $X$ and $Y$, we can write $$\text{var}(X) = E[\text{var}(X|Y)] + \text{var}(E[X|Y]).$$ This result can be viewed as decomposing the variance of a random quantity into two separate components, and comes up again in later statistics courses. At this point we can view this as a method for connecting the marginal distribution through the conditional variance nad expectation. 
:::


:::{#exm-law-of-total-variance}
## 
TODO: include example of the law of total variance.

::::{.callout .solution collapse='true'}
## Solution

::::
:::

## Joint Expectations
The final set of techniques to consider^[At least, for now.] relate to making use of the joint distribution between $X$ and $Y$. Specifically, if we have any function of two random variables, say $g(X,Y)$ and we wish to find $E[g(X,Y)]$. This follows in an exactly analogous derivation to what we have seen so far. In this case, we replace the marginal distribution with the joint distribution. The variance extends in the same manner as well.

:::{#def-joint-expectation}
## Joint Expectation
The joint expectation of a function ($g$) of two random variables, $X$ and $Y$, is written $E[g(X,Y)]$. This is an expectation computed with respect to the joint distribution of $X$ and $Y$, giving $$E[g(X,Y)] = \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g(x,y)p_{X,Y}(x,y).$$ The joint expectation captures the location of a multivariate function, and is readily extended to more than two random variables. 
:::

:::{#def-joint-variance}
## Joint Variance
The joint variance of a function ($g$) of two random variables, $X$ and $Y$, is written $\text{var}(g(X,Y))$. This is a variance computed with respect to the joint distribution of $X$ and $Y$, giving $$\text{var}(g(X,Y)) = E[(g(X,Y) - E[g(X,Y)])^2].$$ The joint variance captures the spread of a multivariate function, and is readily extended to more than two random variables. 
:::

For instance, if we want to consider the product of two random variables, we could use this technique to determine $E[XY]$ and $\text{var}(XY)$.

:::{#exm-joint-expectation}
## 
TODO: write example that has the computation of the joint expectation and variance of a random variable.

::::{.callout .solution collapse='true'}
## Solution

::::
:::

It is worth considering, briefly, the ways in which conditional and joint expectations interact. Namely, if we know that $Y=y$, then the transformation $g(X,y)$ only has one random component, which is $X$. As a result, taking $E[g(X,Y)|Y=y] = E[g(X,y)|Y=y]$. If instead we use the conditional distribution without a specific value, we still have that $Y$ is fixed within the expression, it is just fixed to an unknown quantity. That is $E[g(X,Y)|Y]$ will be a function of $Y$. We saw before that $E[E[X|Y]] = E[X]$, and the same is true in the joint case. Thus, one technique for computing the joint expectation, $g(X,Y)$ is to first compute the conditional expectation, and then compute the marginal expectation of the resulting quantity.

:::{#exm-joint-expectation-two}
## 
TODO: do example that has the computation of the joint expectation, again, but this time in two steps.

::::{.callout .solution collapse='true'}
## Solution

::::
:::

### Linear Combinations of Random Variables

With this relationship, we can ask about taking combinations of random variables. For instance, if we have two random variables $X$ and $Y$, we can use this framework to understand how $X + Y$ behaves. An application of these rules with the function $g(X,Y) = X + Y$ gives $E[X+Y] = E[X] + E[Y]$, and that $\text{var}(X + Y) = \text{var}(X) + \text{var}(Y) + 2E[(X-E[X])(Y - E[Y])]$. Thus, we see that expectations are linear over combinations of random variables, however, variances are not. The term $E[(X-E[X])(Y - E[Y])]$ is called the **covariance** of $X$ and $Y$, and it is a measure of how related $X$ and $Y$ happen to be.

:::{#def-covariance}
## Covariance
The covariance of two random variables, $X$ and $Y$, is given by $\text{cov}(X,Y) = E[(X - E[X])(Y - E[Y])]$. The covariance measures the relationship between $X$ and $Y$, where a positive covariance value means that as $X$ increases, $Y$ will also increase on average (and vice versa). A negative covariance means that as $X$ increases, $Y$ will decrease on average (and vice versa).
::: 

The covariance behaves similarly to the variance. We can see directly from the definition that $\text{cov}(X,X) = \text{var}(X)$. Moreover, using similar arguments to those used for the variance, we can show that $$\text{cov}(aX+b,cY+d) = ac\text{cov}(X,Y).$$ Covariances remain linear, so that $$\begin{multline}\text{cov}(X+Y,X+Y+Z)=\text{cov}(X,X)+\text{cov}(X,Y)+\text{cov}(X,Z)\\ +\text{cov}(Y,X)+\text{cov}(Y,Y)+\text{cov}(Y,Z).\end{multline}$$ These make covariances somewhat nicer to deal with than variances, and on occasion it may be easier to think of variances as covariances with themselves.

:::{.callout-warning icon="false" collapse="true"}
## Proofs for the Expectation and Variance of Linear Combinations of Random Variables
With $g(X,Y) = X+Y$, we can consider applying the defining relationship for joint expectations. That is \begin{align*}
E[X+Y] &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}(x+y)p_{X,Y}(x,y) \\
&= \sum_{x\in\mathcal{X}}x\sum_{y\in\mathcal{Y}}p_{X,Y}(x,y) + \sum_{y\in\mathcal{Y}}y\sum_{x\in\mathcal{X}}p_{X,Y}(x,y) \\
&= \sum_{x\in\mathcal{X}}xp_X(x) + \sum_{y\in\mathcal{Y}}yp_Y(y) \\
&= E[X] + E[Y].\end{align*}

For the variances, we apply the variance relationship, giving  \begin{align*}
E[(X+Y-E[X]-E[Y])^2] &= E[((X-E[X])+(Y-E[Y]))^2] \\
&= E[(X-E[X])^2] + E[(Y-E[Y])^2] + 2E[(X-E[X])(Y-E[Y])] \\
&= \text{var}(X) + \text{var}(Y) + 2E[(X-E[X])(Y-E[Y])].\end{align*} Rewriting the covariance in more common terms gives, $$\text{var}(X+Y) = \text{var}(X) + \text{var}(Y) + 2\text{cov}(X,Y).$$
:::

:::{#exm-linear-combination-of-RV}
## 
TODO: Include example with the linear combination of random variables.

::::{.callout .solution collapse='true'}
## Solution

::::
:::

## Expectations when Random Variables are Independent
Whenever we can assume independence of random quantities, we can greatly simplify the expressions we are dealing with. Recall that the key defining relationship with independence is that $p_{X,Y}(x,y) = p_X(x)p_Y(y)$. Suppose then that we can write $g(X,Y) = g_X(X)h_Y(Y)$. For instance, for the covariance we have $g(X,Y)=(x-E[X])(Y-E[Y])$ and so $g_X(X) = X-E[X]$ and $h_Y(Y) = Y-E[Y]$. If we want to compute $E[g(X,Y)]$ then we get \begin{align*}
E[g(X,Y)] &= E[g_X(X)h_Y(Y)] \\
&= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g_X(x)h_Y(y)p_{X,Y}(x,y) \\
&= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g_X(x)h_Y(y)p_X(x)p_Y(y) \\
&=\sum_{x\in\mathcal{X}}g_X(x)p_X(x)\sum_{y\in\mathcal{Y}}h_Y(y)p_Y(y)\\
&= E[g_X(X)]E[h_Y(Y)].\end{align*} Thus, whenever random variables are independent, we have the ability to separate them over their expectations. Stated succinctly, whenever $X\perp Y$, then $$E[g_X(X)h_Y(Y)] = E[g_X(X)]E[h_Y(Y)].$$ 

:::{#exm-independent-rv-expectation}
## 
TODO: Include expectation of independent random variables

::::{.callout .solution collapse='true'}
## Solution

::::
:::

Consider what this means for the covariance between independent random variables. If $X\perp Y$ then $$\text{cov}(X,Y) = E[(X-E[X])(Y-E[Y])] = E[X-E[X]]E[Y-E[Y]].$$ Note that $E[X - E[X]] = E[X] - E[X] = 0$, and the same for $E[Y - E[Y]]$. Thus, if $X \perp Y$ then $\text{cov}(X, Y) = 0$. That is to say, if $X$ and $Y$ are independent, then $\text{cov}(X,Y)=0$. It is critical to note that this relationship does **not** go both ways. You are able to have $\text{cov}(X,Y) = 0$ even if $X\not\perp Y$. 

While the covariance is interesting in and of itself, the result allows us to simplify the expression for the variance of a sum of two random variables. Specifically, for independent random variables $X$ and $Y$ we also must have that $\text{var}(X+Y)=\text{var}(X)+\text{var}(Y)$. This further extends to more than two random variables, where if (for instance) we have $X_1,X_2,\dots,X_n$ all independent, we get both that $$E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i],$$ and that $$\text{var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \text{var}(X_i).$$ These are relationships that we will use **heavily** once we begin to consider statistics. Note, this extension to more than two random variables applies to all of the concepts discussed throughout this chapter. 

In order to do so, the relevant joint distribution, or conditional distribution would need to be substituted into the definitions. Often the complexity here becomes a matter of keeping track of which quantities are random, and which are not. For instance, if we have $X,Y,Z$ as random variables, then $E[X|Y,Z]$ is a random function of $Y$ and $Z$. We will still have that $E[E[X|Y,Z]] = E[X]$, however, the outer expectation is now the joint expectation with respect to $Y$ and $Z$. As a result, we can also write $E[E[X|Y,Z]|Y]$. The first expectation will be with respect to $X|Y,Z$, while the outer expectation is with respect to $Z|Y$. This is a useful demonstration for when making the distribution of the expectation explicit may help clarify what is being computed. In general, the innermost expectations will always have more conditioning variables than the outer ones. Each time we step out, we peel back one of hte conditional variables until the outermost is either a marginal (or joint). This may help to keep things clear.

## Exercises {.unnumbered}

:::{.exr-7.1}
Find the variance and standard deviation of the sum obtained in tossing a pair of standard dice. (Note, this was @exr-6.1 as well; however, now we can use a different technique for it.)
:::
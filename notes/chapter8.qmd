# Conditional and Joint Expectations and Variances
Up until this point we have considered the marginal probability distribution when exploring the measures of central tendency and spread. These help to summarize the marginal behaviour of a random quantity, capturing the distribution of, for instance, $X$ alone. When introducing distributions, we also made a point to introduce the conditional distribution as one which is particularly relevant when there is extra information. The question "what do wwe expect to happen, given that we have an additional piece of information?" is not only well-defined, but it is an incredibly common type of question to ask. To answer it, we require **conditional expectations**.

TODO: Include set of questions relating to conditional expectation.

In principle, a conditional expectation is no more challenging to calculate than a marginal expectation. Suppose we want to know teh expected value of $X$ assuming that we know that a second random quantity, $Y$ has taken on the value $y$. We write this as $E[X|Y=y]$, and all we do is replace $p_X(x)$ with $p_{X|Y}(x|y)$ in the defining relationship. That is $$E[X|Y=y] = \sum_{x\in\mathcal{X}}xp_{X|Y}(x|y).$$ In a sense, we can think of the conditional distribution of $X|Y=y$ as simply being a distribution itself, and then work with that no differently. The conditional variance, which we denote $\text{var}(X|Y=y)$ is also exactly the same.

TODO: Include an example.

Above we supposed that we knew that $Y=y$. However, sometimes we want to work with the conditional distribution more generally. That is, we want to investigate the behaviour of $X|Y$, without yet knowing what $Y$ equals. We can use the same procedure as above, however, this time we leave $Y$ unspecified. We denote this as $E[X|Y]$, and this expression will be (in general) a function of $Y$. Then, whenever a value for $Y$ is observed, we can simply specify $Y=y$, deriving the specific value. In practice, we will typically compute $E[X|Y]$ rather than $E[X|Y=y]$, since once we have $E[X|Y]$ we can easily find $E[X|Y=y]$ for *every* value of $y$.

TODO: Show example.

Since $E[X|Y]$ is a function of an unknown random quantity, $Y$, $E[X|Y]$ is also a random variable. It is a transformation of $Y$, and as such, it will have some distribution, some expectation, and some variance itself. This is often a confusing concept when it is first introduced, so to recap: $X$ and $Y$ are both random variables; $E[X] and $E[Y]$ are both constant, numerical values describing the distribution of $X$ and $Y$; $E[X|Y=y]$ and $E[Y|X=x]$ are each numeric constants which summarize the distribution of $X|Y=y$ and $Y|X=x$ respectively; $E[X|Y]$ and $E[Y|X]$ are functions of $Y$ and $X$, respectively, and can as such be seen as transformations of (and random quantities depending on) $Y$ and $X$ respectively.

We do not often think of the distribution of $E[X|Y]$ directly, however, there is a very useful result about both its expected value and its variance, which will commonly be exploited. Specifically, if we take the expected value of $E[X|Y]$ we will find that $E[E[X|Y]] = E[X]$. Note that since $E[X|Y] = g(Y)$ for some transformation, $g$, the outer expectation is taken with respect to the distribution of $Y$. Sometimes when this may get confusing we will use notation to emphasize this fact, specifically, $E_Y[E_{X|Y}[X|Y]] = E_X[X]$. This notation is not necessary, but it can clarify when there is much going on, and is a useful technique to fallback on. \begin{align*}
E_Y[E[X|Y]] &= \sum_{y\in\mathcal{Y}} E[X|Y]p_Y(y) \\
&= \sum_{y\in\mathcal{Y}}\left(\sum_{x\in\mathcal{X}}xp_{X|Y}(x|Y)\right)p_Y(y) \\
&= \sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}}x\frac{p_{X,Y}(x,y)}{p_Y(y)}p_Y(y)\\
&= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}xp_{X,Y}(x,y)\\
&= \sum_{x\in\mathcal{X}} xp_X(x)\\
&= E[X].\end{align*}

TODO: Include example.

This property, that $E[E[X|Y]] = E[X]$ is important enough that it receives its own name: **the law of total expectation**. In the same way that it is sometimes easier to first condition on $Y$ in order to compute the marginal distribution of $X$ via applications of the law of total probability, so too can it be easier to first work out conditional expectations, and then take the expected value of the resulting expression. This adds on to the so-called "conditioning arguments" that were discussed previously, allowing a technique to work out the marginal mean indirectly.

TODO: Show example use case of LOTE.

While the conditional expectation is used quite prominently, the conditional variance is less central to the study of random variables. As discussed, briefly, the conditional variance is given by the same variance relationship, replacing the marginal probability distribution with the conditional one (just as with expectations). Just as with expectations, $\text{var}(X|Y=y)$ is a numeric quantity given by $E[(X-E[X|Y=y])^2|Y=y]$ and $\text{var}(X|Y)$ is a random variable given by $E[(X-E[X|Y])^2|Y]$. This means that when working with the general, $\text{var}(X|Y)$, we can also consider taking expectations of the resulting transformation.

TODO: Include examples.

A final result relating to conditional expectations and variances connects the two concepts. This is known as **the law of total variance**. For any random variables $X$ and $Y$, we can write $$\text{var}(X) = E[\text{var}(X|Y)] + \text{var}(E[X|Y]).$$ This result can be viewed as decomposing the variance of a random quantity into two separate components, and comes up again in later statistics courses. At this point we can view this as a method for connecting the marginal distribution through the conditional variance nad expectation. 

TODO: Examples of this.

The final set of techniques to consider for now relate to making use of the joint distribution between $X$ and $Y$. Specifically, if we have any function of two random variables, say $g(X,Y)$ and we wish to find $E[g(X,Y)]$. This follows all of the expected derivations that we have used so far, this time replacing the marginal with the joint distribution. That is, $$E[g(X,Y)] = \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g(x,y)p_{X,Y}(x,y).$$ For instance, if we want to consider the product of two random variables, we could use this technique to determine $E[XY]$. The variance extends in the same manner as well.

TODO: Include example.

This defining relationship allows us to work out the expected value of a linear combination of two random variables. That is \begin{align*}
E[X+Y] &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}(x+y)p_{X,Y}(x,y) \\
&= \sum_{x\in\mathcal{X}}x\sum_{y\in\mathcal{Y}}p_{X,Y}(x,y) + \sum_{y\in\mathcal{Y}}y\sum_{x\in\mathcal{X}}p_{X,Y}(x,y) \\
&= \sum_{x\in\mathcal{X}}xp_X(x) + \sum_{y\in\mathcal{Y}}yp_Y(y) \\
&= E[X] + E[Y].\end{align*}

The same property does not apply with variances, at least not in general. To see this, consider that \begin{align*}
E[(X+Y-E[X]-E[Y])^2] &= E[((X-E[X])+(Y-E[Y]))^2] \\
&= E[(X-E[X])^2] + E[(Y-E[Y])^2] + 2E[(X-E[X])(Y-E[Y])] \\
&= \text{var}(X) + \text{var}(Y) + 2E[(X-E[X])(Y-E[Y])].\end{align*} The term that impedes the linear relationship, $E[(X-E[X])(Y-E[Y])]$ can be computed as any joint function can be. This quantity, however, is particularly important when considering the relationship between two random variables. This is called the **covariance** and it is a measure of the relationship between $X$ and $Y$. Typically we write $E[(X-E[X])(Y-E[Y])] = \text{cov}(X,Y)$ so that $$\text{var}(X+Y) = \text{var}(X) + \text{var}(Y) + 2\text{cov}(X,Y).$$

TODO: Include example.

The covariance behaves similarly to the variance. We can see directly from the definition that $\text{cov}(X,X) = \text{var}(X)$. Moreover, using similar arguments to those used for the variance, we can show that $$\text{cov}(aX+b,cY+d) = ac\text{cov}(X,Y).$$ Covariances remain linear, so that $$\text{cov}(X+Y,X+Y+Z)=\text{cov}(X,X)+\text{cov}(X,Y)+\text{cov}(X,Z)+\text{cov}(Y,X)+\text{cov}(Y,Y)+\text{cov}(Y,Z).$$ These make covariances somewhat nicer to deal with than variances, and on occasion it may be easier to think of variances as covariances with themselves.

TODO: Example? Maybe.

It is worth considering, briefly, the ways in which conditional and joint expectations interact. Namely, if we know that $Y=y$, then the transformation $g(X,y)$ only has one random component, which is the $X$. As a result, taking $E[g(X,Y)|Y=y] = E[g(X,y)|Y=y]$. If instead we use the conditional distribution without a specific value, we still have that $Y$ is fixed within the expression, it is just fixed to an unknown quantity. That is $E[g(X,Y)|Y]$ will be a function of $Y$. We saw before that $E[E[X|Y]] = E[X]$, and the same is true in the joint case. Thus, one technique for computing the joint expectation, $g(X,Y)$ is to first compute the conditional expectation, and then compute the marginal expectation of the resulting quantity.

TODO: example.

## Independence in all of this
Whenever we can assume independence of random quantities, this allows us to greatly simplify teh expressions we are dealing with. Recall that the key defining relationship with independence is that $p_{X,Y}(x,y) = p_X(x)p_Y(y)$. Suppose then that we can write $g(X,Y) = g_X(X)g_Y(Y)$. For instance, for the covariance we have $g(X,Y)=(x-E[X])(Y-E[Y])$ and so $g_X(X) = X-E[X]$ and $g_Y(Y) = Y-E[Y]$. If we want to compute $E[g(X,Y)]$ then we get \begin{align*}
E[g(X,Y)] &= E[g_X(X)g_Y(Y)] \\
&= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g_X(x)g_Y(y)p_{X,Y}(x,y) \\
&= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g_X(x)g_Y(y)p_X(x)p_Y(y) \\
&=\sum_{x\in\mathcal{X}}g_X(x)p_X(x)\sum_{y\in\mathcal{Y}}g_Y(y)p_Y(y)\\
&= E[g_X(X)]E[g_Y(Y)].\end{align*} Thus, whenever random variables are independent, we have the ability to separate them over their expectations.

TODO: example.

Consider what this means, in particular, for the covariance between independent random variables. If $X\perp Y$ then \begin{align*}
\text{cov}(X,Y) &= E[(X-E[X])(Y-E[Y])] \\
&= E[(X-E[X])]E[(Y-E[Y])] \\
&= (E[X]-E[X])(E[Y]-E[Y]) \\
&= 0.\end{align*} That is to say, if $X$ and $Y$ are independent, then $\text{cov}(X,Y)=0$. As a result of this, for independent random variables $X$ and $Y$ we also must have that $\text{var}(X+Y)=\text{var}(X)+\text{var}(Y)$. It is critical to note that this relationship does not go both ways: you are able to have $\text{cov}(X,Y) = 0$ even if $X\not\perp Y$.

TODO: Include example of independence.

While we have primarily focused on joint and conditional probabilities with two random variables, the same procedures and ideas apply with three or more as well. The relevant joint distribution, or conditional distribution would simply need to be substituted in definitions. Often the complexity here becomes a matter of keeping track of which quantities are random, and which are not. For instance, if we have $X,Y,Z$ as random variables, then $E[X|Y,Z]$ is a random function of $Y$ and $Z$. We will still have that $E[E[X|Y,Z]] = E[X]$, however, the outer expectation is now the joint expectation with respect to $Y$ and $Z$. As a result, we can also write $E[E[X|Y,Z]|Y]$. The first expectation will be with respect to $X|Y,Z$, while the outer expectation is with respect to $Z|Y$. This becomes a useful demonstration for when making the distribution of the expectation explicit helps to clarify what is being computed. As a general rule of thumb, the innermost expectations will always have more conditioning variables than the outer ones: each time we step out, we peel back one of hte conditional variables until the outermost is either a marginal (or joint). This will help to keep things clear.
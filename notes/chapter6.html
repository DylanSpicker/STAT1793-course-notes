<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.533">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 1793: Course Notes - 6&nbsp; Continuous Random Variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/chapter5.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/chapter1.html">Part 1: Probability</a></li><li class="breadcrumb-item"><a href="../notes/chapter6.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Continuous Random Variables</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">STAT 1793: Course Notes</a> 
        <div class="sidebar-tools-main">
    <a href="../STAT-1793--Course-Notes.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Part 1: Probability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Core Concepts of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Probailities with More than One Event</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Named Discrete Distributtions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter6.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Continuous Random Variables</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Part 2: Statistics</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#continuous-versus-discrete" id="toc-continuous-versus-discrete" class="nav-link active" data-scroll-target="#continuous-versus-discrete"><span class="header-section-number">6.1</span> Continuous Versus Discrete</a></li>
  <li><a href="#cumulative-distribution-functions" id="toc-cumulative-distribution-functions" class="nav-link" data-scroll-target="#cumulative-distribution-functions"><span class="header-section-number">6.2</span> Cumulative Distribution Functions</a></li>
  <li><a href="#the-probability-density-function" id="toc-the-probability-density-function" class="nav-link" data-scroll-target="#the-probability-density-function"><span class="header-section-number">6.3</span> The Probability Density Function</a></li>
  <li><a href="#using-continuous-distributions" id="toc-using-continuous-distributions" class="nav-link" data-scroll-target="#using-continuous-distributions"><span class="header-section-number">6.4</span> Using Continuous Distributions</a></li>
  <li><a href="#the-uniform-distribution" id="toc-the-uniform-distribution" class="nav-link" data-scroll-target="#the-uniform-distribution"><span class="header-section-number">6.5</span> The Uniform Distribution</a></li>
  <li><a href="#the-normal-distribution" id="toc-the-normal-distribution" class="nav-link" data-scroll-target="#the-normal-distribution"><span class="header-section-number">7</span> The Normal Distribution</a>
  <ul class="collapse">
  <li><a href="#the-specification-of-the-distribution" id="toc-the-specification-of-the-distribution" class="nav-link" data-scroll-target="#the-specification-of-the-distribution"><span class="header-section-number">7.1</span> The Specification of the Distribution</a></li>
  <li><a href="#the-standard-normal-distribution" id="toc-the-standard-normal-distribution" class="nav-link" data-scroll-target="#the-standard-normal-distribution"><span class="header-section-number">7.2</span> The Standard Normal Distribution</a></li>
  <li><a href="#the-empirical-rule" id="toc-the-empirical-rule" class="nav-link" data-scroll-target="#the-empirical-rule"><span class="header-section-number">7.3</span> The Empirical Rule</a>
  <ul class="collapse">
  <li><a href="#chebyshevs-inequality" id="toc-chebyshevs-inequality" class="nav-link" data-scroll-target="#chebyshevs-inequality"><span class="header-section-number">7.3.1</span> Chebyshev’s Inequality</a></li>
  </ul></li>
  <li><a href="#closure-of-the-normal-distribution" id="toc-closure-of-the-normal-distribution" class="nav-link" data-scroll-target="#closure-of-the-normal-distribution"><span class="header-section-number">7.4</span> Closure of the Normal Distribution</a></li>
  <li><a href="#normal-approximations" id="toc-normal-approximations" class="nav-link" data-scroll-target="#normal-approximations"><span class="header-section-number">7.5</span> Normal Approximations</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/chapter1.html">Part 1: Probability</a></li><li class="breadcrumb-item"><a href="../notes/chapter6.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Continuous Random Variables</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Continuous Random Variables</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Our discussions of probability distributions, and their summaries have focused entirely on discrete random variables. To recap, a discrete random variable is any random quantity with a countable number of elements in the sample space. Discrete random variables are defined in contrast to continuous random variables, which take on values over the span of intervals in uncountably large sets. Suppose that <span class="math inline">\(X\)</span> can take any real number between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. There is no way to enumerate the set of possible values for this random quantity, and so it must not be discrete.</p>
<p>Many quantities of interest are better treated as a continuous quantity rather than a discrete one, even if this is not technically correct. For instance, time measured in seconds is often best thought of as continuous, even though any stop watch used to grab these measurements will have some limit to the precision with which it can measure. Similarily, lengths (or heights) will often be better treated as continuous quantities, even though any measuring device will necessarily have some minimal threshold afterwhich it cannot discern distances. Deciding whether a quantity is continuous or discrete can thus, sometimes, be a judgment call. In general discrete quantities are harder to work with when the set of possibilities is very large. In these cases, not much is lost by treating the random variables as though they were continuous. This distinction is another area which requires the active development of intuition, but once present, it becomes second nature.</p>
<p>TODO: include examples for discrete versus continuous.</p>
<section id="continuous-versus-discrete" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="continuous-versus-discrete"><span class="header-section-number">6.1</span> Continuous Versus Discrete</h2>
<p>Distinguishing whether a random quantity is continuous or discrete is crucial as, broadly speaking, the two types of quantities are treated differently. The same underlying ideas are present, but the distinctions between the two settings require some careful thought. As a general rule, the use of continuous random variables necessitates an understanding of introductory calculus. This is not a pre-requisite for this course, and as a result, we will not focus as deeply on working with the underlying quantities. However, continuous random variables are also the dominant type of random variables outside of introductory courses. As a result, understanding the distinctions, and beginning to become familair with how they are to be manipulated is an important skill.</p>
<p>The key difference between discrete and continuous random variables is that, for discrete random variables the beahviour is governed by assessing <span class="math inline">\(P(X=x)\)</span> for all possible values of <span class="math inline">\(x\)</span>, and for continuous random variables <span class="math inline">\(P(X=x)=0\)</span> for every value of <span class="math inline">\(x\)</span>. This is a surprising statement, and as such it is worth reiterating. With discrete random variables we discussed how all of the probabalistic behaviour was governed by the probability mass function which was defined as <span class="math inline">\(p_X(x) = P(X=x)\)</span>. If <span class="math inline">\(X\)</span> is continuous, it will aalways be the case that <span class="math inline">\(P(X=x) = 0\)</span>. Correspondingly, continuous random variables do not have probability mass functions, and to understand the behaviour of these random variables we must turn to other quantities.</p>
<p>While the fact that <span class="math inline">\(P(X=x) = 0\)</span> may seem unintuitive at first glance, it is worth exploring this even further. The nature of a continuous random variable is such that there is no possible way to enumerate all of the values which are possible to be realized by the random quantity. Suppose that we take a set of countably many possible observations and gave each of these a probability of greater than <span class="math inline">\(0\)</span> of occuring directly. Even if we take an infinite number of them, there will still be an uncountably infinite number of events in the sample space which we have not accounted for. We know that the total probability of the sample space must be <span class="math inline">\(1\)</span>, and so we must have the total probability of the first set of events being less than <span class="math inline">\(1\)</span> (if it summed to <span class="math inline">\(1\)</span> then this would not be a continuous random variable, since there would only be a countable number of possible events). Now suppose we take another set of countably many events, again giving each of them a positive probability. Once more the sum of all of these probabilities must be less than <span class="math inline">\(1\)</span>, and specifically, the sum of both sets must also be less than <span class="math inline">\(1\)</span>. Even after these two sets there are still uncountably infinite events to go, and so we continue this process. Because we always need the total probability to be <span class="math inline">\(1\)</span> once all events have been accounted for, and because we will always have an uncountably infinite number of events left to account for, we can <strong>never</strong> have a positive probability assigned to each event in a set of events. Even if we made the probability of each of these sets very, very small (say <span class="math inline">\(1\)</span> in a million) after some fixed number of countable eevents the probabilities would be greater than <span class="math inline">\(1\)</span> which cannot happen. As such, each event itself must have <span class="math inline">\(0\)</span> probability.</p>
<p>An alternative technique for understanding this intuition is to think about how uunlikely it really would be to observe any specific value. Suppose that <span class="math inline">\(X\)</span> takes values on the interval <span class="math inline">\([0,1]\)</span>. Recall that when we defined probabilities, we discussed them as being the long run proportion of time that an event occurs. Take some event, say <span class="math inline">\(X=0.5\)</span>. Suppose that we took repeated measurements of <span class="math inline">\(X\)</span> which are independent and identically distributed. Now suppose that at some point we exactly do observe <span class="math inline">\(X=0.5\)</span>. Should we expect that this will ever happen again? The next time we get near <span class="math inline">\(0.5\)</span> might we instead not observe <span class="math inline">\(0.51\)</span> or <span class="math inline">\(0.49\)</span> or <span class="math inline">\(0.5000000000000000001\)</span> or any of the other uncountably infinite values in the very near vicinity of <span class="math inline">\(0.5\)</span>? Each time that we make an observation the denominator of our proportion is growing, but if every value between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> is truly possible, as time goes on the number of times that <span class="math inline">\(X=0.5\)</span> must stay much, much smaller than the total number of trials. If we continue this off to infinity, in the limit, the probability must become <span class="math inline">\(0\)</span>.</p>
<p>This conclusion leads to a few different points. First, impossibility is not the same as probability <span class="math inline">\(0\)</span>. Impossible events do have probability <span class="math inline">\(0\)</span>, but possible events may also have probability <span class="math inline">\(0\)</span>. Events which are outside of the sample space are impossible. Events inside the sample space, even probability zero events, remain possible. Second, we require alternative mathematical tools for discussing the probability of events in a continuous setting. Ideally this would be analogous to a probability mass function, but would somehow function in the case of continuity.</p>
</section>
<section id="cumulative-distribution-functions" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="cumulative-distribution-functions"><span class="header-section-number">6.2</span> Cumulative Distribution Functions</h2>
<p>To begin building up to the continuous analogue to the probability mass function, we will start by focusing on events that are easier to define in the continuous case. Suppose that <span class="math inline">\(X\)</span> is defined on some continuous interval. Instead of thinking of events relating to <span class="math inline">\(X=x\)</span>, we instead turn our focus to events of the form <span class="math inline">\(X \in (a,b)\)</span> for some interval defined by the endpoints <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Now note that, relying only on our knowledge of probabilities relating to generic events, we can rewrite <span class="math inline">\(P(X\in(a,b))\)</span> slightly. Specifically, <span class="math display">\[\begin{align*}
P(X\in(a,b)) &amp;= 1 - P(X\not\in(a,b))\\
&amp;= 1 - P(\{X &lt; a\}\cup\{X &gt; b\}) \\
&amp;= 1 - \left(P(X &lt; a) + P(X &gt; b)\right)\\
&amp;= 1 - P(X &gt; b) - P(X &lt; a)\\
&amp;= P(X &lt; b) - P(X &lt; a).\end{align*}\]</span></p>
<p>TODO: diagram showing this graphically.</p>
<p>In words we know that the probability that <span class="math inline">\(X\)</span> falls into any particular interval is given by the probability that it is less than the upper bound of the interval minus the probability that it is less than the lower bound of the interval. Notice that <span class="math inline">\(X &lt; a\)</span> is an event, and if we knew how to assign probabilities to <span class="math inline">\(X&lt;a\)</span> for arbitrary <span class="math inline">\(a\)</span>, then we could assign probabilities to any interval. Also note that, even in the continuous case, it make sense to talk of <span class="math inline">\(P(X &lt; a)\)</span> for some value <span class="math inline">\(a\)</span>. These intervals will contain an uncountably infinite number of events, and as such, can certainly occur with greater than <span class="math inline">\(0\)</span> probability. Using our common example of <span class="math inline">\(X\)</span> being defined on <span class="math inline">\([0,1]\)</span>, then <span class="math inline">\(P(X&lt;1)=1\)</span>. Note that we could have written <span class="math inline">\(P(X \leq 1) = 1\)</span>, which may have been more obviously true. However, <span class="math inline">\(P(X\leq 1) = P(\{X&lt;1\}\cup\{X=1\}) = P(X&lt;1) + P(X=1)\)</span> and we know that <span class="math inline">\(P(X=1)=0\)</span>. In the continuous case we do not need to worry whether we use <span class="math inline">\(X\leq a\)</span> or <span class="math inline">\(X &lt; a\)</span>, and we will interchange them throughout.</p>
<p>TODO: uniform example</p>
<p>The centrality of events of the form <span class="math inline">\(X&lt;a\)</span> prompts the definition of a mathematical function which we call the <strong>cumulative distribution function</strong>. We will typically denote the cumulative distribution function of a random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(F(x)\)</span>, sometimes using <span class="math inline">\(F_X(x)\)</span> to emphasize that this function relates to <span class="math inline">\(X\)</span> specifically. We may also refer to the cumulative distribution function simply as the <strong>distribution function</strong>. By definition, we take <span class="math inline">\(F_X(x) = P(X\leq x)\)</span>. Once we have defined the distribution function for a random variable, using the above derivation we are able to determine the probability associated with any events based on intervals.</p>
<p>It is worth noting that the cumulative distribution function can also be defined for discrete random variables. In the case of a discrete random variable, we would have <span class="math display">\[F_X(x) = \sum_{k \in\mathcal{X}; k \leq x} p_X(k).\]</span> Since it is simply the summation of the probability mass function it tends to be a less useful quantity. Still, the cumulative distribution functions for discrete random variables do come up on ocassion, and it is worth recognizing that they are defined in exactly the same way.</p>
<p>TODO: Include example with the CDF.</p>
<p>Supposing that, for some continuous random variable <span class="math inline">\(X\)</span> we have the cumulative distribution function, then thisknowledge actually permits us to compute <em>any</em> probability associated with the random variable that we can want. Consider any event associated with <span class="math inline">\(X\)</span> which we may wish to determine the probability of. We know that events are merely subsets of the sample space. Every one of these events can be written using our basic set operations (unions, intersections, and complements) applied to intervals of the form <span class="math inline">\((a,b)\)</span> and <strong>singelton sets</strong> of the form <span class="math inline">\(\{x\}\)</span>. Our basic axioms of probability allow us to compute probabilities across the set operations, and our knowledge of the cumulative distribution, the conversion of <span class="math inline">\(P(X \in (a,b)) = F_X(b) - F_X(a)\)</span>, and the fact that <span class="math inline">\(P(X=0) = 0\)</span> gives all of the results we need to derive probabilities for these events.</p>
<p>TODO: Include simple examples</p>
</section>
<section id="the-probability-density-function" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="the-probability-density-function"><span class="header-section-number">6.3</span> The Probability Density Function</h2>
<p>The distribution function will be the core object used to discuss the probabalistic behaviour of a continuous random variable. All of the beahviour of these random quantities will be described by the distribution function, and as such we will take the distribution function as a function which defines the distribution of a continuous random quantity. This is all that we need in orderto analyze the probabalistic behaviour of these random variables, however, it may be a little unsatisfying in contrast with the discrete case.</p>
<p>We had set out to find a quantity which was a parllel to the probability mass function, and instead concluded that the cumulative distribution function can eb made to play the same role in terms of describing the behaviour of the random quantity. Still, it may be of interest for us to have a function which takes into account the relative likelihood of being near some value. Suppose, for instance, that for a random variable defined on <span class="math inline">\([0,1]\)</span> we wanted to know how likely it was to be in the vicinity of <span class="math inline">\(X=0.5\)</span>. We could take a small number, say <span class="math inline">\(\delta = 0.01\)</span> and calculate <span class="math inline">\(P(X\in(0.5-\delta,0.5+\delta)) = F(0.5+\delta)-F(0.5-\delta)\)</span>. This is perfectly well defined based on our discussions to this point. Now, if we assume that the probability isfairly evenly distributed throughout this interval, then if we wanted to assign a likelihood to each value we could divide this total probability by hte length of the interval, which is <span class="math inline">\(2\delta\)</span>. As a result, we are saying that the probability that <span class="math inline">\(X\)</span> is nearly <span class="math inline">\(0.5\)</span> will be approximately given by the expression <span class="math inline">\(\frac{F(0.5+\delta)-F(0.5-\delta)}{2\delta}\)</span>.</p>
<p>We had taken <span class="math inline">\(\delta=0.01\)</span>, but the same process could be applied for smaller and smaller <span class="math inline">\(\delta\)</span>, say <span class="math inline">\(0.001\)</span> or <span class="math inline">\(0.0001\)</span>. Intuitively, as the size of this itnerval shrinks more and more we are getting a better and better estimate for the likelihood that the random variable is in the immediate vicinity of <span class="math inline">\(0.5\)</span>. Moreover, as <span class="math inline">\(\delta\)</span> gets smaller and smaller our assumption of a uniform probability over the interval becomes more and more reasonable. Now,, we cannot set <span class="math inline">\(\delta=0\)</span> exactly, however, we can ask what happens in the limit as <span class="math inline">\(\delta\)</span> continues to get smaller and smalller. This question is in the purvue of calculus, and can in fact be answered. While working out the answer is beyond the scope of the course, we will provide the result anyway.</p>
<p>The resulting function is called the <strong>probability density function</strong>, and is related to the cumulative distribution function through derivatives (and integrals). If a random variable <span class="math inline">\(X\)</span> has a cumulative distribution function, <span class="math inline">\(F(x)\)</span>, we typically denote the corresponding density function as <span class="math inline">\(f(x)\)</span>. The density function also describes the behaviour of the random variable, and mirrors the behaviour of the probability mass function in the discrete case. Roughly speaking, the density function evaluates how likely (relatively speaking) it is for a continuous quantity to be in a small neighbourhood of the given value. Critically, <strong>probability density functions do not give probabilities directly</strong>. In fact, probability density functions may give values that are greater than <span class="math inline">\(1\)</span>!</p>
<p>TODO: Example of uniform PDF.</p>
<p>Still, if we see the shape of the probability density function, we can state how likely it is to make observations near the results of interest. We will often graph the density function. The high points of the graph indicate regions with more probability than the regions of the graph which are lower. Again, the specific probability of any event <span class="math inline">\(X=x\)</span> will always be <span class="math inline">\(0\)</span>, but some events fall in neighbourhoods which are more likely to observe than others.</p>
<p>TODO: include examples.</p>
</section>
<section id="using-continuous-distributions" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="using-continuous-distributions"><span class="header-section-number">6.4</span> Using Continuous Distributions</h2>
<p>With the exception of the differences indicated until this point, there is otherwise not much difference between continuous and discrete random variables. The tools to analyze them differ (in the continuous case, we cannot sum over the sample space, and so we must use techniques from calculus to mirror this process, for instance), but the fundamentals remain the same. It is still possible to compute expected values (and medians and modes) with roughly the same interpretations. It is still possible to describe the range, interquartile range, and variance, again with corresponding interpretations. The axioms of probability still underpin the manipulation and analysis of these random variables. The distinction is merely that in place of elementary mathematics to complete the calculations, calculus is required.</p>
<p>Just as with discrete distributions, there are continuous named distributions. These are typically governed by either a density function or else a distribution function, alongside the expected value and variance. And just like the named discrete distributions, by matching the underlying scenario to the correct process, we are able to side step a lot of work in understanding the beahviour of the random quantities. Now, because there is no assumed knowledge of calculus, we will not work too widely with continuous distributions. We will introduce only two named continuosu distributions: the uniform distribution, which we have already started to see, and the normal distribution, which is far and away the most important distribution (discrete or continuous) in all of probability and statistics.</p>
</section>
<section id="the-uniform-distribution" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="the-uniform-distribution"><span class="header-section-number">6.5</span> The Uniform Distribution</h2>
<p>The uniform distribution, sometimes called the continuous uniform distribution to distinguish it from the discrete counterpart, is parameterized over a set interval specified as <span class="math inline">\((a,b)\)</span>. On this interval, equal probability density is given to every event, which is to say that the density function is constant. Specifically, for <span class="math inline">\(X\sim\text{Unif}(a,b)\)</span> we note that <span class="math display">\[f(x) = \begin{cases}\frac{1}{b-a} &amp; x \in (a,b) \\ 0 &amp; \text{otherwise}.\end{cases}\]</span> From the density function we can work out that <span class="math display">\[F(x) = \frac{x - a}{b - a}\]</span> for <span class="math inline">\(x \in (a,b)\)</span>, with <span class="math inline">\(F(x) = 0\)</span> for <span class="math inline">\(x &lt; a\)</span> and <span class="math inline">\(F(x) = 1\)</span> for <span class="math inline">\(x &gt; b\)</span>. Moreover, we have <span class="math inline">\(E[X] = \frac{a+b}{2}\)</span> and <span class="math inline">\(\text{var}(X) = \frac{(b-a)^2}{12}\)</span>.</p>
<p>TODO: Include some calculations</p>
<p>The uniform distribution is analogous to the discrete uniform. Any time there is an interval of possible outcomes which are all equally likely, the uniform distribution is the distribution to use. Compared with other distributions it is also fairly straightforward to work with, which makes it a useful demonstration of the concepts relating the continuous probability calculations.</p>
<p>TODO: Include examples.</p>
</section>
<section id="the-normal-distribution" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> The Normal Distribution</h1>
<p>The normal distribution, also sometimes referred toas the Gaussian distribution, is a named continuous distribution function defined on the complete real line. The distribution is far and away the most prominently used distribution in all of probability and statistics. In fact, most people have heard of normal distributions even if they are not aware of this fact. Any time that there is a discussion of a bell curve, for instance, this is in reference to the normal distribution. Normally distributed quantities arise all over the place from measurements of heights, grades, or reaction times through to levels of job satisfaction, reading ability, or blood pressure. There is a tremendous number of normally distributed phenomena naturally occurring in the world, which renders the normal distribution deeply important across a wide range of domains.</p>
<p>Perhaps more important than the places where the normal distribution arises in nature are hte places where it arises mathematically. At the end of this course we will see a result, the central limit theorem, which is one of the core results in all of statistics. Most of the statistical theory that drives scentific inquiry sits ontop of the central limit theorem, and at the core of the central limit theorem is the normal distribution. It is virtually impossible to overstate the importance of the normal distribution, and as a result, we will spend a great deal of time investigating it.</p>
<section id="the-specification-of-the-distribution" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="the-specification-of-the-distribution"><span class="header-section-number">7.1</span> The Specification of the Distribution</h2>
<p>A normal distribution is parameterized by two different parameters: the mean, <span class="math inline">\(\mu\)</span>, and the varaince <span class="math inline">\(\sigma^2\)</span>. We write <span class="math inline">\(X\sim N(\mu,
\sigma^2)\)</span>. These parameters directly correspond to the relevant quantities such that <span class="math inline">\(E[X] =  \mu\)</span> and <span class="math inline">\(\text{var}(X) = \sigma^2\)</span>. The density function is given by <span class="math display">\[f(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).\]</span> This can be quite unwieldy to work with, however, when it is plotted we ssee that the normal distribution takes on a bell curve which is centered at <span class="math inline">\(\mu\)</span>.</p>
<p>TODO: include plots</p>
</section>
<section id="the-standard-normal-distribution" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="the-standard-normal-distribution"><span class="header-section-number">7.2</span> The Standard Normal Distribution</h2>
<p>Normally distributed random variables are particularly well-behaved. One way in which this is true is that if you multiply a normally distributed random variable by a constant, it will remain normally distributed, and if you add a constant to a normally distributed random variable, it will remain normally distributed. Consider then if <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span>, taking the quantity <span class="math inline">\(X - \mu\)</span>. We have seen in our discussions of expected values that <span class="math inline">\(E[X-\mu] = E[X]-\mu = 0\)</span>. Furthermore, adding or subtracting a constant will not change the variance. Thus, <span class="math inline">\(X-\mu\sim N(0,\sigma^2)\)</span>.</p>
<p>Now, consider dividing this by <span class="math inline">\(\sigma\)</span>, or equivalently, multiply by <span class="math inline">\(\frac{1}{\sigma}\)</span>. The expected value of the new quantity will be <span class="math inline">\(\frac{1}{\sigma}\times 0 = 0\)</span>, and the variance of the newquantity will be <span class="math inline">\(\frac{1}{\sigma^2}\times\sigma^2 = 1\)</span>. Taken together then, if <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span> then <span class="math display">\[Z = \frac{X - \mu}{\sigma} \sim N(0,1).\]</span> This holds true for <em>any</em> starting normal distribution, with any mean or variance values. As a result, this straightforward transformation allows us to discuss any normal distribution in terms of <span class="math inline">\(N(0,1)\)</span>. We call this the <strong>standard normal distribution</strong>, and will typically use <span class="math inline">\(Z\)</span> to denote a random variable from the standard normal distriubtion.</p>
<p>TODO: Include example</p>
<p>If <span class="math inline">\(Z\sim N(0,1)\)</span>, then we use a special notation for the density function and distribution function of <span class="math inline">\(Z\)</span>. Specifically, we take the density function to be denoted <span class="math inline">\(\text{var}phi(z)\)</span> and the cumulative distribution function to be given by <span class="math inline">\(P(Z \leq z) = \Phi(z)\)</span>. The cumulative distribution function does not have a nice form to be written down, however, it is a commonly applied enough function that many computing languages have implemented it, including of course R.</p>
<p>TODO: Demonstration of using *norm.</p>
<p>The utility in this is demonstrated by realizing that events can be converted using the same transformations. Specifically, suppose we have <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, and we want to find <span class="math inline">\(P(X \leq x)\)</span>. Note that, <span class="math inline">\(X \leq x\)</span> must also mean that <span class="math display">\[\frac{X-\mu}{\sigma} \leq \frac{x - \mu}{\sigma},\]</span> simply by applying the same transformation to both sides. But we <em>know</em> that the left hand side of this inequality is exactly <span class="math inline">\(Z\)</span>, a standard normal random variable with cumulative distribution function <span class="math inline">\(\Phi(z)\)</span>. Thus, <span class="math display">\[P(X \leq x) = P\left(Z \leq \frac{x - \mu}{\sigma}\right) = \Phi\left(\frac{x-\mu}{\sigma}\right).\]</span> Using this trick of <strong>standardization</strong> any normal probability can be converted into a probability regarding the standard normal, for which we can easily use computer software.</p>
<p>TODO: Include normal calculation TODO: Include discussion of R calculating normal probabilities</p>
<p>As a result, combining our knowledge of continuous random variables, with the process of standardization we are able to calculate normal probabilities for any events relating to normally distributed random quantities. Moreover, since the shape of the normal distribution is so predictable, it is often easy to draw out the density function, and indicate on this graphic the probabilities of interest, which in turn helps with the required probability calculations. Calculating probabilities from normal distributions will remain a central component of working with statistics and probabilities beyond this course. Developing the skills and intuition at this point, through repeated practice is a key step in successfully navigating statistics here and beyond.</p>
<p>TODO: another example.</p>
<p>When you have access to a computer, and your interest is in calculating a normal probability, as described above, there is not properly a need for standardization. However, it rmeains an important skill for several reasons. First, by always working with the same normal distribution, you will develop a much more refined intuition for the likelihoods of different events. It goes beyond working with the same family of distributions, you get very used to working with exactly the same distribution. Second, you will likely become quite familiar with certain key <strong>critical values</strong> of the standard normal distribution. These values arise frequently, and allow you to quickly approximate the likelihood of different events. Finally, as we begin to move away from studying probability and into studying statistics, the standard normal will feature prominently there.</p>
</section>
<section id="the-empirical-rule" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="the-empirical-rule"><span class="header-section-number">7.3</span> The Empirical Rule</h2>
<p>Another way in which the normal distribution is well behaved is summarized in the <strong>emprical rule</strong>. The shape of the distribution is such that, no matter the specific mean or variance, all members of the family remain quite similar. This enables the derivation of an easy, approximate result, to help intuitively gauge the probabilities of normal events. The emprical rule states that, if <span class="math inline">\(X\)</span> has a normal distribution, then the probability of observing a value within <span class="math inline">\(\sigma\)</span> or the mean is approximately <span class="math inline">\(0.68\)</span>, the probability of observing a value within <span class="math inline">\(2\sigma\)</span> of the mean is approximately <span class="math inline">\(0.95\)</span>, and the probability of observing a value within <span class="math inline">\(3\sigma\)</span> of the mean is approximately <span class="math inline">\(0.997\)</span>.</p>
<p>TODO: Include graphic</p>
<p>In words, the empirical simply states that almost all of the observations from a normal distribution will fall within <span class="math inline">\(\mu\pm3\sigma\)</span>. In mathematical terms, the empirical rule is summarized as <span class="math inline">\(P(\mu-\sigma\leq X \leq \mu + \sigma) \approx 0.68\)</span>, <span class="math inline">\(P(\mu - 2\sigma \leq X \leq \mu + 2\sigma) \approx 0.95\)</span>, and <span class="math inline">\(P(\mu - 3\sigma \leq X \leq \mu + 3\sigma) \approx 0.997\)</span>. With the standard normal we can replace <span class="math inline">\(\mu\)</span> with <span class="math inline">\(0\)</span>, and <span class="math inline">\(\sigma\)</span> with <span class="math inline">\(1\)</span> to get a version which is slightly more concise to state. It is then possible to combine these different intervals by recognizing the symmetry in the normal distribution. That is, <span class="math inline">\(P(\mu \leq X \leq \mu + \sigma) \approx \frac{0.68}{2} = 0.34\)</span>.</p>
<p>TODO: Calculations with the empirical rule.</p>
<p>The empirical rule is not exact, and again, when computing probabilities with access to statistical software, it is likely of limiteddirect utility. However, it is another tool to leverage to continue developing a refined intuition for the behaviour of random quantities. It is also a good check to have a sense of the likelihood of different events. If you compute an answer which seems out of line with the empirical rule, take a look moreclosely. If you have someone tell you that they have observed events which are out of line with the empirical rule, be skeptical.</p>
<section id="chebyshevs-inequality" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="chebyshevs-inequality"><span class="header-section-number">7.3.1</span> Chebyshev’s Inequality</h3>
<p>The empirical rule is a useful result to aid in building intuition regarding the normal distribution. However, when quantitiesare not normally distributed, we cannot use the empirical rule. A related, though somewhat weaker result, does hold for <em>any</em> distribution, and it is a useful extension to the empirical rule. Stated in words, Chebyshev’s inequality says that there is a probability of <span class="math inline">\(0.75\)</span> or more of observing an observation within two standard deviations, and a probability of at least <span class="math inline">\(0.8889\)</span> of observing a value within three standard deviations of the mean.</p>
<p>Formally, we can write that <span class="math display">\[P(\mu - k\sigma \leq X \leq \mu + k\sigma) \geq 1 - \frac{1}{k^2}.\]</span> Here <span class="math inline">\(k\)</span> can be any real number which is greater than <span class="math inline">\(0\)</span>. If <span class="math inline">\(k\leq 1\)</span>, this result is uninteresting since the bound simply is <span class="math inline">\(0\)</span>. However, taking <span class="math inline">\(k=2\)</span> gives the <span class="math inline">\(0.75\)</span> lower bound outlined above, which is a more useful result. Additionally, there is no requirement for <span class="math inline">\(k\)</span> to be an integer here, and so, for instance, the probability of observing a value within <span class="math inline">\(\mu\pm\sqrt{2}\sigma\)</span> is at least <span class="math inline">\(0.5\)</span>, for all distributions.</p>
<p>TODO: Include some examples.</p>
</section>
</section>
<section id="closure-of-the-normal-distribution" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="closure-of-the-normal-distribution"><span class="header-section-number">7.4</span> Closure of the Normal Distribution</h2>
<p>We have seen a certain type of “closure property” for the normal distribution when we discussed standardization. That is, adding and multiplying by constants does not change the distribution when working with normally distributed quantities. This is an interesting property which does not hold for most distributions, and makes normally distributed random variables quite nice to work with. The normal distribution has an addition type of closure property which is frequently used, and is also somewhat surprising.</p>
<p>Suppose that <span class="math inline">\(x\)</span> and <span class="math inline">\(Y\)</span> are independent of one another, with <span class="math inline">\(X\sim N(\mu_X, \sigma_X^2)\)</span>, and <span class="math inline">\(Y\sim N(\mu_Y, \sigma_Y^2)\)</span>. In this setting, <span class="math display">\[X+Y\sim N(\mu_X+\mu_Y, \sigma_X^2 + \sigma_Y^2).\]</span> in words, the addition of two independent normally distributed random variables will also be normally distributed. This extends beyond two in the natural way, simply by applying and reapplying the rule (as many times as is required).</p>
<p>TODO: Include example of this.</p>
<p>This becomes particularly useful when we think of generating many iid realizations in an experiment from a normal population. For instance, if <span class="math inline">\(X_1,\dots,X_n\)</span> are all iid from a <span class="math inline">\(N(\mu,\sigma^2)\)</span> distribution, then <span class="math display">\[\sum_{i=1}^n X_i \sim N(n\mu, n\sigma^2).\]</span> If instead we consider the average of these <span class="math inline">\(n\)</span> independent variables, then <span class="math display">\[\frac{1}{n}\sum_{i=1}^n X_i \sim N(\mu, \frac{\sigma^2}{n}),\]</span> through an application of our standard expectation and variance transformation rules. This type of result is central to the practice of statistics, and this closure under addition further aids in the utility of the normal distribution.</p>
</section>
<section id="normal-approximations" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="normal-approximations"><span class="header-section-number">7.5</span> Normal Approximations</h2>
<p>A final utility to the normal distribution is in its ability to approximate other distributions. While several of these approximations exist, we will focus on the normal approximation to the binomial as an illustrative example. Historically, these approximations were critical for computing probabilities by hand in a timely fashion. Owing to the widespread use of statistical software, these usecases are more and more limited, which removes the necessity of these approximations directly. However, there are two major advantages to learning these approximations. First, with an approximation it becomes easier to leverage the intuition you will build regarding the normal distribution in order to better understand the behaviour of other random quantities. Second, the normal approximation has the same “flavour” as many results in statistics, and so it presents an additional path to familiarity with these types of findings.</p>
<p>Suppose that <span class="math inline">\(X\sim\text{Bin}(n,p)\)</span>. Through knowledge of the binomial distribution, we know that <span class="math inline">\(E[X] = np\)</span> and <span class="math inline">\(\text{var}(X) = np(1-p)\)</span>. If <span class="math inline">\(n\)</span> is sufficiently large then it is possible to approximate a binomial distribution using a normal distribution with the corresponding mean and variance. That is, for <span class="math inline">\(n\)</span> large enough, we can take <span class="math inline">\(X\sim\text{Bin}(n,p)\)</span> to have approximately the same distribution as <span class="math inline">\(W\sim N(np, np(1-p))\)</span>.</p>
<p>TODO: Brief example.</p>
<p>One consideration that we need to make when applying this approximation has to do with the fact that the normal distribution is continuous while the binomial distribution is discrete. As a result, the normal distribution can take on any value on the real line, where the binomial is limited to the integers. A question that we must answer is what to do with the non-integer valued numbers. The natural solution is to rely on rounding. That is, for any value between <span class="math inline">\([1.5, 2.5)\)</span> we would round to the nearest integer, which is <span class="math inline">\(2\)</span>.</p>
<p>This natural solution is in fact a fairly useful technique, and it is the one that we will make use of in the normal approximation. While rounding is quite natural, the process for leveraging this idea in probability approximation is somewhat backwards. That is, we typically will need to go from probabilities relating to <span class="math inline">\(X\)</span> and transform those into probabilities relating to <span class="math inline">\(W\)</span>. So, for instance, if we wish to know <span class="math inline">\(P(X \leq 2)\)</span>, then we need to be able to make this a statement regarding the random variable <span class="math inline">\(W\)</span>. In order to do this we need to ask “what is the largest value for <span class="math inline">\(W\)</span> that would get rounded to <span class="math inline">\(2\)</span>?” The answer is <span class="math inline">\(2.5\)</span> and so <span class="math inline">\(P(X \leq 2) \approx P(W \leq 2.5)\)</span>.</p>
<p>A similar adjustment would be required if we instead wanted <span class="math inline">\(P(X \geq 5)\)</span>. Here we would ask “what is the smallest value for <span class="math inline">\(W\)</span> which would get rounded to <span class="math inline">\(5\)</span>?” and note that the answer is <span class="math inline">\(4.5\)</span>. Thus, <span class="math inline">\(P(X \geq 5) \approx P(W \geq 4.5)\)</span>. Once we have expressed the probability of interest in terms of the normla random variable, we can use the standard techniques previously outlined to compute the relevant probabilities.</p>
<p>TODO: Examples</p>
<p>It is very important to keep in mind that the two results discussed above were of the form <span class="math inline">\(X \geq x\)</span> and <span class="math inline">\(X \leq x'\)</span>. If we instead had considered <span class="math inline">\(X &gt; x\)</span> or <span class="math inline">\(X &lt; x'\)</span>, we would need to take an additional step. For continuous random variables whether <span class="math inline">\(X \geq x\)</span> or <span class="math inline">\(X &gt; x\)</span> is considered it makes no difference. However, for discrete random variables this is not the case. As a result we should first convert the event the an equivalent event which contains the equality sign within the inequality, and then apply the continuity correction. (TODO: ensure that continuity correction was referenced before). That is, if we want <span class="math inline">\(P(X &gt; 3)\)</span> first note that for <span class="math inline">\(X &gt; 3\)</span> to hold, we could equivalently write this as <span class="math inline">\(X \geq 4\)</span>. Alternatively, if the event of interest is <span class="math inline">\(X &lt; 8\)</span>, this is the same as <span class="math inline">\(X \leq 7\)</span>.</p>
<p>TODO: Further examples.</p>
<p>When it is not necessary, it rarely makes sense to use an approximation. There will be cases where the approximation is directly useful, and in those moments it is great to be able to use it. This example of using the normal distribution to approximate a discrete random variable serves as a nice bridge from the study of probability to the study of statistics. In statistics we take a different view of the types of problems we have been considering to date, and we require the tools of probability that have been brought forth. As a result, a deep comfort with manipulating probability expressions is required to build a strong foundation while studying statistics.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          trigger: 'click',
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          positionFixed: true,
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notes/chapter5.html" class="pagination-link  aria-label=" &lt;span="" named="" discrete="" distributtions&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Named Discrete Distributtions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.533">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 1793: Course Notes - 6&nbsp; The Expected Value and Distributional Summaries</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/chapter7.html" rel="next">
<link href="../notes/chapter5.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "classic",
  "enableExperimentalNewNoteButton": true,
  "showHighlights": "whenSidebarOpen"
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/chapter1.html">Part 1: Probability</a></li><li class="breadcrumb-item"><a href="../notes/chapter6.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Expected Value and Distributional Summaries</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">STAT 1793: Course Notes</a> 
        <div class="sidebar-tools-main">
    <a href="../STAT-1793--Course-Notes.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Part 1: Probability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Mathematical Foundations of Statistical Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Core Concepts of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Probabilities with More than One Event</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Summarizing Statistical Experiments with Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter6.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Expected Value and Distributional Summaries</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Named Discrete Distributtions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Part 2: Statistics</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#expectation" id="toc-expectation" class="nav-link active" data-scroll-target="#expectation"><span class="header-section-number">6.1</span> Expectation</a></li>
  <li><a href="#conditional-and-joint-expectations-and-variances" id="toc-conditional-and-joint-expectations-and-variances" class="nav-link" data-scroll-target="#conditional-and-joint-expectations-and-variances"><span class="header-section-number">6.2</span> Conditional and Joint Expectations and Variances</a></li>
  <li><a href="#independence-in-all-of-this" id="toc-independence-in-all-of-this" class="nav-link" data-scroll-target="#independence-in-all-of-this"><span class="header-section-number">6.3</span> Independence in all of this</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/chapter1.html">Part 1: Probability</a></li><li class="breadcrumb-item"><a href="../notes/chapter6.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Expected Value and Distributional Summaries</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Expected Value and Distributional Summaries</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="expectation" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="expectation"><span class="header-section-number">6.1</span> Expectation</h2>
<p>Until this point in our discussions of probability we have relied upon characterizing the behaviour of a random variable via the use of probability mass functions. In some sense, a probability mass function captures all of the probabilistic behaviour of a discrete random variable. Using the mass function you are able to characterize how often, in the long-run, any particular value will be observed, and any questions associated with this. As a result, the mass function remains a critical area of focus for understanding how random quantities behave.</p>
<p>However, these functions need to be explored and manipulated in order for useful information to be extracted from them. They do not summarize this behaviour effectively, as they are not intended to be a summary tool, and understandably we often wish to have better numeric quantities which are able to concisely indicate components of the behaviour of a distribution. Put differently, provided with a probability mass function it is hard to immediately answer “what do we expect to happen, with this random variable?” despite the fact that this is a very obvious first question.</p>
<p>To address questions related to expectations, we turn towards the statistical concept of an <strong>expected value</strong>. We refer to expected values as expectations, averages, and means of a distribution, interchangeably. The idea with an expected value is that we are trying to capture, with one single number, what value is expected when we make observations from the random quantity. There are many ways one might think to describe our expectations, and it is worth exploring these concepts in some detail.</p>
<p>One way that we may think to define our expected value is by asking what value is the most probable. This is a question which can be directly answered using the probability mass function. The process for this would require looking at the function and determining which value for <span class="math inline">\(x\)</span> corresponds to the highest probability: this is the value that we are most likely to see. Sometimes this procedure is fairly straightforward, sometimes it is quite complicated. No matter the complexity of the specific situation, the underlying process is the same: what value has the highest probability of being seen, and that is the most likely one.</p>
<p>TODO: Example of mode</p>
<p>As intuitive as this may seem, this is not the value that will be used as the expected value generally. Instead, this quantity is referred to as the <strong>mode</strong>. While the mode is a useful quantity, and for some decisions will be the most pertinent summary value, there are some major issues with it as a general measure which make it less desirable. For starters, consider that our most common probability model considered until this point has been that of equally likely outcomes. Here, there is no well-defined mode (convention tends to be taking it as the set of all the most probable values). Presenting the mode is equivalent to presenting the full mass function in this setting.</p>
<p>While the case of equally likely outcomes is a fairly strong explanation for some issues with the mode, it need not be so dramatic to undermine its utility. It is possible for a distribution to have several modes which are quite distinct from one another, even if it’s not all values. Moreover, it is quite common for the modal value to be not particularly likely itself. Consider a random variable that can take on a million different values. If all of the probabilities are approximately <span class="math inline">\(0.000001\)</span> then presenting the mode as the most probable value does not translate to saying that the mode is particularly probable.</p>
<p>TODO: include example where the mode is slightly more likely than a string of similar values.</p>
<p>If the mode has these shortcomings, what else might work? Another intuitive concept is to try to select the “middle” oof the distribution. One way to define the middle would be to select the value such that half of observations are beneath it, and half of observations are above it. That way, when you are told this value, you immediately know that it is equally likely to observe values on either side of this mark. This is also a particularly intuitive definition for expected value, and is important enough to be named: <strong>the median</strong>.</p>
<p>The median is the midpoint of a distribution, and is very important for describing the behaviour of random variables. Medians are often the most helpful single value to report to indicate the typical behaviour of a distribution, and they are frequently used. When people interpret averages, in general, it is often the median that they are actually interpreting. It is very intuitive to be given a value and know that it is the middle of all the possible values for a distribution.</p>
<p>TODO: include examples on medians</p>
<p>Despite the advantages of medians, they have their own drawbacks as well. For starters, the median can be exceptionally challenging to compute in certain settings. As a result, even when a median is appropriate, it may not be desirable if it is too challenging to determine. Beyond the difficulties in computation, medians have some properties which may be undesirable, depending on the specific use case. One concern which arises frequently is that medians are not translated to totals, which can make them challenging in certain use cases.</p>
<p>Suppose that you are a store and you know that your median quantity of items sold in a day is <span class="math inline">\(50\)</span> and the median cost of these items is <span class="math inline">\(\$10\)</span>, you cannot simply multiply the <span class="math inline">\(50\)</span> and the <span class="math inline">\(\$10\)</span> to suggest that your median revenue in a day is <span class="math inline">\(\$500\)</span>. Doing this type of unit conversion or basic arithmetic with medians can be challenging, and as a result they are not always the most useful when reporting values that are going to be interpreted as rates.</p>
<p>TODO: Expand on the above example.</p>
<p>Beyond the basic manipulation medians have a feature which is simultaneously a major benefit in some settings, and a major fallback in others. Specifically, medians are less influenced by extreme values in the probability distribution. Consider two different distributions: one of them is equally likely to take any value between <span class="math inline">\(1\)</span> and <span class="math inline">\(10\)</span>, where the other is equally likely to take any value between <span class="math inline">\(1\)</span> and <span class="math inline">\(9\)</span> or <span class="math inline">\(1,000,000\)</span>. In both of these settings, we can take the median to be <span class="math inline">\(5.5\)</span> since half of the probability mass falls above <span class="math inline">\(5.5\)</span> and half falls below it.</p>
<p>The median, in some sense, ignores the extreme value in the probability distribution and remains stable throughout it. In certain settings, this can be very desirable. For instance, in the distribution of household incomes, the median may be an appropriate measure seeing as there are a few families who have very extreme incomes which otherwise distort the picture provided by most families. In this sense, the median’s robustness to extreme values is a positive feature of it in terms of a summary measure for distributional behaviour.</p>
<p>Suppose instead that you work for an insurance company and are concerned with understanding the value of insurance claims that your company will need to pay out. The distribution will look quite similar to the income distribution: most of the probability will be assigned to fairly small claims, with a small chance of a very large one. As an insurance company, if you use the median this large claim behaviour will be smoothed over, perhaps leaving you unprepared for the possibility of extremely large payouts. In this setting, the extreme values are informative and important, and as a result the median’s robustness becomes a hindrance to correctly describing the behaviour.</p>
<p>TODO: Another median example?</p>
<p>Between the median and the mode we have two measures which capture some sense of expected value, each with their own set of strengths and drawbacks. Neither capture what it is that is referred to as <em>the</em> expected value. For this, we need to take inspiration from the median, and consider another way that we may think to find the center of the distribution.</p>
<p>If the median gives the middle reading along the values sequentially, we may also wish to think about trying to find the “center of gravity” of the numbers. Suppose you take a pen, or marker, or small box of chocolates, and you wish to balance this object on a finger or an arm. To do so, you do not place the item so that half of it sits on one side of the appendage and half on the other: you adjust the location so that half of the mass sits on either side of the appendage.</p>
<p>Throughout our probability discussions, we have always referred to probability as mass itself. We use the probability mass function to generate our probability values. This metaphor can be extended when we try to find the center of the distribution. If we imagine placing a mass with weight equal to the probability mass functions value at each value that a random variable can take on, we may ask: where would we have to place a fulcrum to have this number line be balanced? The answer to this question serves as another possible measure of center.</p>
<p>It turns out that this notion of center is the one that we are all most familiar with: the simple average. And this simple average is also the conception of expectation which gets bestowed with the name “expected value”. Mathematically, the expected value is desirable for many reasons, some of which we will study in more depth later on. One of these desirable features, which stands in contrast with the median, is the comparative ease with which expected values can be computed. For a random variable, <span class="math inline">\(X\)</span>, we write the expected value of <span class="math inline">\(X\)</span> as <span class="math inline">\(E[X]\)</span>, and assume that <span class="math inline">\(X\)</span> takes values in <span class="math inline">\(\mathcal{X}\)</span> with a probability mass function <span class="math inline">\(p_X(x)\)</span>, we get <span class="math display">\[E[X] = \sum_{x \in \mathcal{X}} xp_X(x).\]</span></p>
<p>TODO: Example compute simple expected value.</p>
<p>In the case of an equally likely probability model, the expected value becomes the standard average that is widely used. Suppose that there are <span class="math inline">\(n\)</span> options in the sample space, denoted <span class="math inline">\(x_1,\dots,x_n\)</span>, then we can write <span class="math display">\[E[X] = \sum_{i=1}^n x_i\frac{1}{n} = \frac{1}{n}\sum_{i=1}^nx_i.\]</span> When the probability models are more complex, the formula is not precisely the standard average - instead, it becomes a weighted average.</p>
<p>TODO: Add basic average example.</p>
<p>While less commonly applied than the simple average, a weighted average is familiar to most students for a crucial purpose: grade calculations. If you view the weight of each item in a course as a probability mass, and the grade you scored as the value, then your final grade in the course is exactly the expected value of this distribution. The frequency with which expected values are used make them attractive as a quick summary for the center of a distribution.</p>
<p>TODO: Include pricing calculation showing mean versus median.</p>
<p>While the mean provides a useful, intuitive measure of center of the distribution, it is perhaps counter intuitive to name it the expected value. To understand the naming convention it is easiest to consider the application which has likely spurred more development of statistics and probability than any other: gambling.</p>
<p>Suppose that there is some game of chance that can pay out different amounts with different probabilities. A critical question for a gambler in deciding whether or not to play such a game is “how much can I expect to earn, if I play?” This is crucial to understanding, for instance, how much you should be willing to pay to participate, or if you are the one running the game, how much you should charge to ensure that you make a profit.</p>
<p>If you want to understand what you expect to earn, the intuitive way of accomplishing this is to weight each possible outcome by how likely it is to occur. This is exactly the expected value formula that has been provided, and so the expected value can be thought of as the expected payout of a game of chance where the outcomes are payouts corresponding to each probability.</p>
<p>TODO: Include expected payout calculation.</p>
<p>This also represents the cost at which a rational actor should be willing to pay to participate. If a game of chance costs more than the expected value to play, in the long run you will lose money. If a game of chance costs less than the expected value, in the long run you will earn money. It is hard to overstate the utility of gambling in developing probability theory, and as such these types of connections are expected.</p>
<p>To interpret the expected value of a random variable, one possibility is using the intuition that we used to derive the result. Notably, the expected value is the center of mass of the distribution, where the masses correspond to probabilities. This means that it is not necessarily an actual central number over the range, but rather that it sits in the weighted middle. While this interpretation is useful in many situations, there are times where the point of balance is a less intuitive description. For these, it can sometimes be useful to frame the expected value as the long-term simple average from the distribution.</p>
<p>If we imagine observing many independent and identically distributed random variables, then as the number of samples tends to infinity, the expected value of <span class="math inline">\(X\)</span> and the simple average will begin to coincide with one another. That is the distance between <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n X_i\)</span> will shrink to <span class="math inline">\(0\)</span>. As a result, we can view the expected value as the average over repeated experiments. This interpretation coincides nicely with the description based on games of chance. Specifically, if you were to repeatedly play the same game of chance, the average payout per game will be equal to the expected value, if you play for long enough.</p>
<p>TODO: Include convergence graphic.</p>
<p>Sometimes the value of a random variable needs to be mapped through a function to give the value which is most relevant to us. Consider, for instance, a situation wherein the side lengths of boxes being manufactured by a specific supplier are random, due to incorrectly calibrated tolerances in the machines. The resulting boxes are cubes, but what is of more interest is the volume of the produced box, not the side length. If a box has side length <span class="math inline">\(x\)</span>, then its volume will be <span class="math inline">\(x^3\)</span>, and so we may desire some way of computing <span class="math inline">\(E[X^3]\)</span> rather than <span class="math inline">\(E[X]\)</span>.</p>
<p>In general, for some function <span class="math inline">\(g(X)\)</span>, we may want to compute <span class="math inline">\(E[g(X)]\)</span>. It is important to recognize that, generally speaking, <span class="math inline">\(E[g(X)] \neq g(E[X])\)</span>. This is a common mistake, and an attractive one, but a mistake nonetheless. If we are unable to simply apply the function to the expected value, then the question of how to compute the expected value remains. Instead of applying the function to overall expected value, instead, we simply apply the function to each value in the defining relationship for the expected value. That is, <span class="math display">\[E[g(X)] = \sum_{x\in\mathcal{X}} g(x)p_X(x).\]</span> This is sometimes referred to as the “law of the unconscious statistician,” a name which may be aggressive enough to help remember the correct way to compute the expectation.</p>
<p>TODO: Move the next thing up. Where the median demonstrated robustness against extreme values in the distribution, the mean (or expected value) does not. For instance, if we consider the distribution of incomes across a particular region, the mean will be much higher than the median, as those families with exceptionally high incomes will not be smoothed over as they were with medians. In this case, the lack of robustness for the expected value will render the mean a less representative summary for the true behaviour of the random quantity.</p>
<p>To see this concretely, consider the difference between a random variable which with equal probability takes a value between <span class="math inline">\(1\)</span> and <span class="math inline">\(10\)</span>. This will have <span class="math inline">\(E[X] = 5.5\)</span>. Now, if the <span class="math inline">\(10\)</span> is made to be <span class="math inline">\(1,000,000\)</span>, the expected value will now be <span class="math inline">\(E[X] = 100,004.5\)</span>. This is a far cry from the median which does not change from <span class="math inline">\(5.5\)</span> in either case. This lack of robustness is desirable in the event of the insurance example from the median discussion, but will be less desirable in other settings.</p>
<p>The mean, median, and mode are the three standard measures of central tendency. They are also referred to as measures of location, and in general, are single values which describe the standard behaviour of a random quantity. Each of the three has merits as a measure, and each has drawbacks for certain settings. The question of which to use and when depends primarily on the question of interest under consideration, rather than on features of the data alone. Often, presenting more than one measure can give a better sense of the distributional behaviour that any one individual will.</p>
<p>TODO: Include example of choosing measures.</p>
<p>Despite the utility of all three measures, the expected value holds a place of more central importance in probability and statistics. A lot of this has to do with further mathematical properties of the mean. Because of its central role, it is worth studying the expected value in some more depth. % END OF MOVING SECTION.</p>
<p>TODO: include example using LOTUS.</p>
<p>These functions applied to random variables are often thought of as “transformations” of the random quantities. For instance, we <em>transformed</em> a side length into a volume. While the law of the unconscious statistician will apply to any transformation for a random variable, we can sometimes use shortcuts to circumvent its application. In particular, when <span class="math inline">\(g(X) = aX + b\)</span>, for constant numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, we can greatly simplify the expected value of the transformation. To see this note <span class="math display">\[\begin{align*}
E[aX + b] &amp;= \sum_{x\in\mathcal{X}}(ax + b)p_X(x) \\
&amp;= \sum_{x\in\mathcal{X}}axp_X(x) + bp_X(x) \\
&amp;= a\sum_{x\in\mathcal{X}}xp_X(x) + b\sum_{x\in\mathcal{X}}p_X(X) \\
&amp;= aE[X] + b.
\end{align*}\]</span> That is, in general, we have that <span class="math inline">\(E[aX + b] = aE[X] + b\)</span>.</p>
<p>This is particularly useful as linear transformations like <span class="math inline">\(aX+b\)</span> arise very commonly. For instance, most unit conversions are simple linear combinations. If a random quantity is measured in one unit then this result can be used to quickly convert expectations to another.</p>
<p>TODO: include example of temperature or weight conversion.</p>
<p>This type of linear transformation also frequently comes up with games of chance and payouts, or with scoring more generally. For instance, suppose you are betting a certain amount on the results of a coin toss, or that you are taking a multiple choice test that gives <span class="math inline">\(2\)</span> points for a correct answer.</p>
<p>Measures of central tendency are important to summarize the behaviour of a random quantity. Whether using the mean, median, or mode, these measures of location describe, on average, what to expect from observations of the random quantity. However, understanding a distribution requires understanding far more than simply the measures of location. As was discussed previously, the probability mass function captures the complete probabilistic behaviour of a discrete random variable, it is only intuitive that some information would be lost with a single numeric summary.</p>
<p>TODO: Example with equivalent mean, median, and mode.</p>
<p>A key characteristic of the behaviour of a random variable which is not captured by the measures of location is the variability of the quantity. If we imagine taking repeated realizations of a random variable, the variability of the random variable captures how much movement there will be observation to observation. If a random variable has low variability, we expect that the various observations will cluster together, becoming not too distant from one another. If a random variable has high variability, we expect the observations to jump around each time.</p>
<p>Just as was the case with measures of location, there are several measures of variability which may be applicable in any given setting. One fairly basic measure of the spread of a random variable is simply the range of possible values: what is the highest possible value, what is the lowest possible value, and how much distance is there between those two points? This is a fairly intuitive notion, and is particularly useful in the equal probability model over a sequence of numbers. Consider, for instance, dice. dice are typically defined by the range of values that they occupy, say <span class="math inline">\(1\)</span> to <span class="math inline">\(6\)</span>, or <span class="math inline">\(1\)</span> to <span class="math inline">\(20\)</span>. Once you know the values present on any die, you have a sense for how much the values can move observation to observation.</p>
<p>TODO: Include example for the range.</p>
<p>While the range is an important measure to consider to determine the behaviour of a random variable, it is a fairly crude measurement. It may be the case that, while the extreme values are possible, they are sufficiently unlikely so as to come up very infrequently and not remain representative of the likely spread of observations. Alternative, many random variables have a theoretically infinite range. In these cases, providing the range will likely not provide much utility.</p>
<p>TODO: Include example.</p>
<p>To remedy these two issues, we can think of some techniques for modifying the range. Instead of taking the start and end points to be the lowest and highest values, we can instead consider ranges of values which remain more plausible. A common way to do this is to extend our concept of a median beyond the half-way point. The median of a random variable <span class="math inline">\(X\)</span>, is the value, <span class="math inline">\(m\)</span>, such that <span class="math inline">\(P(X \leq m) = 0.5\)</span>. While there is good reason to care about the midpoint, we can think of generalizing this to be <em>any</em> probability.</p>
<p>That is, we could find a number <span class="math inline">\(z\)</span>, such that <span class="math inline">\(P(X \leq z) = 0.1\)</span>. We could then use this value to conclude that <span class="math inline">\(10\%\)</span> of observations are below <span class="math inline">\(z\)</span>, and <span class="math inline">\(90\%\)</span> of observations are above <span class="math inline">\(z\)</span>. (TODO: Change this to be “probability of observation”). These values are referred to, generally, as percentiles and they are the natural extension of medians. We will typically denote the <span class="math inline">\(100p\)</span>th percentile as <span class="math inline">\(\zeta(p)\)</span>, which is the value <span class="math inline">\(P(X \leq \zeta(p)) = p\)</span>. Thus, the median of a distribution is <span class="math inline">\(\zeta(0.5)\)</span>.</p>
<p>TODO: Include examples</p>
<p>We can leverage percentiles to remedy some of the issues with the range as a measure of variability. Framed in terms of percentiles, the minimum value is <span class="math inline">\(\zeta(0)\)</span>, and the maximum value is <span class="math inline">\(\zeta(1)\)</span>. Instead of considering the extreme endpoints, if we consider the difference between more moderate percentiles, we can overcome the major concerns outlined with the range. The most common choices would be to take <span class="math inline">\(\zeta(0.25)\)</span> and <span class="math inline">\(\zeta(0.75)\)</span>; these are referred to as the first and third quartiles, respectively. They are named as, taking <span class="math inline">\(\zeta(0.25)\)</span>, <span class="math inline">\(\zeta(0.5)\)</span> and <span class="math inline">\(\zeta(0.75)\)</span>, the distribution is cut into quarters.</p>
<p>TODO: Include examples.</p>
<p>With the first and third quartiles computed, we can compute the interquartile range, which is given by <span class="math inline">\(\zeta(0.75)-\zeta(0.25)\)</span>. Typically, we denote the interquartile range simply as <span class="math inline">\(\text{IQR}\)</span>, and like the overall range, it gives a measure of how much spread there tends to be in the data. Unlike the range, however, we can be more certain that both the first and third quartiles are reasonable values around which repeated observations of the random variable would be observed. Specifically, there is a probability of <span class="math inline">\(0.5\)</span> that a value between the first and third quartile will be observed. The larger the <span class="math inline">\(\text{IQR}\)</span>, the more spread out these moderate observations will be, and as a result, the more variable the distribution is.</p>
<p>TODO: Write examples.</p>
<p>Both the range and the interquartile range give a sense of the variation in the distribution irrespective of the measures of location for that distribution. Another plausible method for assessing the variability of a distribution is to assess how far we expect observations to be from the center. Intuitively, if observations of <span class="math inline">\(X\)</span> are near the center with high probability, then the distribution will be less variable than if the average distance to the center is larger.</p>
<p>This intuitive measure of variability is useful for capturing the behaviour of a random variable, particularly when paired with a measure of location. However, we do have to be careful: not all measures of dispersion based on this notion will be useful. Consider the most basic possibility, to consider <span class="math inline">\(X - E[X]\)</span>. We might ask, for instance, what is the expected value of this quantity. If we take <span class="math inline">\(E[X - E[X]]\)</span> then note that this a linear combination in expectation since <span class="math inline">\(E[X]\)</span> is just some number. Thus, <span class="math inline">\(E[X-E[X]] = E[X] - E[X] = 0\)</span>. In other words, the expected difference between a random variable and its mean is exactly <span class="math inline">\(0\)</span>. We thus need to think harder about how best to turn this intuition into a useful measure of spread as the first idea will result in <span class="math inline">\(0\)</span> for all random quantities.</p>
<p>The issue with this procedure is that some realizations are going to be below the mean, making the difference negative, and some will be above the mean, making the difference positive. Our defining relationship for the mean relied on balancing these two sets of mass. However, when discussing the variability of the random variable, we do not much care whether the observations are lower than expected or higher than expected, we simply care how much variability there is around what is expected. To remedy this, we should consider only the distance between the observation and the expectation, not the sign. That is, if <span class="math inline">\(X\)</span> is <span class="math inline">\(5\)</span> below <span class="math inline">\(E[X]\)</span> we should treat that the same as if <span class="math inline">\(X\)</span> is <span class="math inline">\(5\)</span> above <span class="math inline">\(E[X]\)</span>.</p>
<p>There are two common ways to turn value into its magnitude in mathematics generally: squaring the number and using absolute values. Both of these tactics are useful approaches to defining measures of spread, and they result in the <strong>variance</strong> when using the expected value of the squared deviations, and the <strong>mean absolute deviation</strong> when using the absolute value. While <span class="math inline">\(E[|X-E[X]|]\)</span> is perhaps the more intuitive quantity to consider, generally speaking it will not be the one that we use.</p>
<p>In general when we need a positive quantity in mathematics it will typically be preferable to consider the square to the absolute value. The reasons for this are plentiful, but generally squares are easier to handle than absolute values, and as a result become more natural quantities to handle. The variance is the central measure of deviation for random variables, so much so that we give it its own notation, <span class="math display">\[\text{var}(X) = E[(X-E[X])^2].\]</span></p>
<p>Note that if we take <span class="math inline">\(g(X) = (X-E[X])^2\)</span>, then the variance of <span class="math inline">\(X\)</span> is the expected value of a transformation. We have seen that to compute these we apply the law of the unconscious statistician, and substitute <span class="math inline">\(g(X)\)</span> into the defining relationship for the expected value, which for the variance gives <span class="math display">\[\text{var}(X) = \sum_{x\in\in\mathcal{X}} (x-E[X])^2p_X(x).\]</span> Prior to computing the variance, we must first work out the mean as the function <span class="math inline">\(g(X)\)</span> relies upon this value.</p>
<p>TODO: Include example for calculating variance.</p>
<p>The higher that an individual random variables variance is, the more spread we expect there to be in repeatedly realizations of that quantity. Specifically, the more spread out around the mean value the random variable will be. A random variable with a low variance will concentrate more around the mean value than one with a higher variance. One confusing part of the variance of a random variable is in trying to assess the units. Suppose that a random quantity is measured in a particular set of units - dollars, seconds, grams, or similar. In this case, our interpretations of measures of location will all be in the same units, which aids in drawing connections to the underlying phenomenon that we are trying to study. However, because the variance is squared, we cannot make the same extensions to it: variance is not measured in the regular units, but in the regular units squared.</p>
<p>Suppose you have a random time being measured, perhaps the reaction time for some treatment to take effect in a treated patient. Finding the mean or median will give you a result that you can read off in seconds as well. The range and interquartile range both give you the spread in seconds. However, if you work out the variance of this quantity it will be measured in seconds squared - a unit that is challenging to have much intuition about. To remedy this we will often use a transformed version of the variance, called the <strong>standard deviation</strong>, returning the units to be only the original scale. The standard deviation of a random variable is simply given by the square root of the variance, which is to say <span class="math display">\[\text{SD}(X) = \sqrt{\text{var}(X)}.\]</span> We do not often consider computing the standard deviation directly, and so will most commonly refer to the variance when discussing the behaviour of a random variable, but it is important to be able to move seamlessly between these two measures of spread.</p>
<p>TODO: Standard deviation.</p>
<p>When computing the variance of a random quantity, we often use a shortcut for the formula. Consider <span class="math display">\[\begin{align*}
\text{var}(X) &amp;= \sum_{x\in\mathcal{X}} (x-E[X])^2p_X(x) \\
&amp;= \sum_{x\in\mathcal{X}} (x^2 - 2xE[X] + E[X]^2)p_X(x) \\
&amp;=\sum_{x\in\mathcal{X}} x^2p_X(x) - 2E[X]\sum_{x\in\mathcal{X}}xp_X(x) + E[X^2]\sum_{x\in\mathcal{X}}p_X(x)\\
&amp;= E[X^2] - 2E[X]E[X] + E[X]^2\\
&amp;= E[X^2] - E[X]^2.
\end{align*}\]</span></p>
<p>This result gives us the identity that the variance of <span class="math inline">\(X\)</span> can be found via <span class="math inline">\(E[X^2] - E[X]^2\)</span>. Generally, this is moderately more straightforward to calculate since <span class="math inline">\(X^2\)</span> is an easier transformation than <span class="math inline">\((X-E[X])^2\)</span>. This identity will come back time and time again, with a lot of versatility in the ways that it can be used. Typically, when a variance is needed to be calculated the process is to simply compute <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(E[X^2]\)</span>, and then apply this relationship.</p>
<p>TODO: include variance calculation example.</p>
<p>With expectations, we saw that <span class="math inline">\(E[g(X)]\)</span> needed to be directly computed from the definition. The same is true for variances of transformations. Specifically, <span class="math inline">\(\text{var}(g(X))\)</span> is given by <span class="math inline">\(E[(g(X) - E[g(X)])^2]\)</span> which can be simplified with the previous relationship as <span class="math inline">\(E[g(X)^2] - E[g(X)]^2\)</span>. Just as with expectations, it is important to realize that <span class="math inline">\(\text{var}(g(X)) \neq g(\text{var}(X))\)</span>, and so dealing with transformations requires further work.</p>
<p>TODO: Transformation example. TODO: Move the following discussion up. Beyond being linear over simple transformations, summations in general behave nicely with expectations. Specifically, for any quantities separated by addition, say <span class="math inline">\(g(X) + h(X)\)</span>, the expected value will be the sum of each expected value. Formally, <span class="math display">\[\begin{align*}
E[g(X) + h(X)] &amp;= \sum_{x\in\mathcal{X}} (g(X) + h(X))p_X(x) \\
&amp;= \sum_{x\in\mathcal{X}} g(X)p_X(x) + h(x)p_X(x) %todo fix capitals\\
&amp;= \sum_{x\in\mathcal{X}} g(x)p_X(x) + \sum_{x\in\mathcal{X}} h(x)p_X(x) \\
&amp;= E[g(X)] + E[h(X)].
\end{align*}\]</span> Behaving well under linearity is one of the very nice properties of expectations. It will come in useful when dealing with a large variety of important quantities, and as we will see shortly, this linearity will also extend to multiple different random quantities.</p>
<p>TODO: add example of linearity. %End section to move.</p>
<p>With expectations, we highlighted linear transformations as a special case, with <span class="math inline">\(g(X) = aX + b\)</span>. For the variance, the linear transformations are also worth distinguishing from others. To this end, we can apply the standard identity for the variance, giving <span class="math display">\[\begin{align*}
E[(aX+b)^2] &amp;= E[a^2X^2 + 2abX + b^2] \\
&amp;= E[a^2X^2] + E[2abX] + E[b^2]\\
&amp;= a^2E[X^2] + 2abE[X] + b^2.
\end{align*}\]</span></p>
<p>TODO:Move upwards Note that part of the property of the linearity of expectation that we can immediately see if that the expected value of any constant is always that constant. If we take <span class="math inline">\(a = 0\)</span>, then we see that <span class="math inline">\(E[aX + b] = E[b] = b\)</span>. Thus, any time that we need to take the expected value of any constant number, we know that it is just that number. %End upwards movement</p>
<p>Next, we note that <span class="math inline">\(E[aX + b] = aE[X] + b\)</span> and so <span class="math display">\[\begin{align*}
E[aX + b]^2 &amp;= (aE[X] + b)^2 \\
&amp;= a^2E[X]^2 + 2abE[X] + b^2.\end{align*}\]</span> Differencing these two quantities gives <span class="math display">\[a^2E[X^2] + 2abE[X] + b^2 - a^2E[X]^2 - 2abE[X] - b^2 = a^2(E[X^2] - E[X]^2).\]</span> By noting that <span class="math inline">\(E[X^2] - E[X]^2\)</span>, we can complete the statement that <span class="math display">\[\text{var}(aX + b) = a^2\text{var}(X).\]</span></p>
<p>Thus, when applying a linear transformation, only the multiplicative constant matters , and it transforms the variance by a squared factor. This should make some intuitive sense that the additive constant does not change anything. If we consider that variance is a measure of spread, adding a constant value to our random quantity will not make it more or less spread out, it will simply shift where the spread is located. This is not true of the mean, which measures where the center of the distribution is, which helps explain why the result identities are different.</p>
<p>TODO: Example using this.</p>
<p>In the same way that the linearity of expectation demonstrates that the expected value of any constant is that constant, we can use this identity to show that the variance of constant is zero. However, we can also reason to this based on our definitions so far. Suppose that we have a random variable which is constant. This seems to be an oxymoron, but it is perfectly well defined. A constant <span class="math inline">\(b\)</span> can be seen as a random variable with probability distribution <span class="math inline">\(p_X(x) = 1\)</span> if <span class="math inline">\(x=b\)</span> and <span class="math inline">\(p_X(x) = 0\)</span> otherwise. In this case, the expected value is going to be <span class="math inline">\(E[X] = 1(b) = b\)</span>, and <span class="math inline">\(E[X^2] = 1(b)^2 = b^2\)</span>. As a result, we see that <span class="math inline">\(E[b] = b\)</span>, as previously stated, and <span class="math inline">\(\text{var}(b) = E[X^2] - E[X]^2 = b^2 - b^2 = 0\)</span>. From an intuitive perspective, there is no variation around the mean of a constant: it is always the same value. As a result, when taking the variance, we know that it should be <span class="math inline">\(0\)</span>.</p>
<p>Unlike the expectation, the variance of additive terms will not generally be the addition of the variances themselves. That is, we cannot say that <span class="math inline">\(\text{var}(g(X) + h(X)) = \text{var}(g(X)) + \text{var}(h(X))\)</span>, as a general rule. Writing out the definition shows issue with this: <span class="math display">\[E[(g(X) + h(X))^2] = E[g(X)^2] + 2E[g(X)h(X)] + E[h(X)^2].\]</span> The first and third terms here are nicely separated and behave well. However, the central term is not going to be easy to simplify, in general. You can view <span class="math inline">\(g(X)h(X)\)</span> as a function itself, and so <span class="math inline">\(E[g(X)h(X)] \neq E[g(X)]E[h(X)],\)</span>$ in general. Instead, this will typically need to be worked out for any specific set of functions.</p>
</section>
<section id="conditional-and-joint-expectations-and-variances" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="conditional-and-joint-expectations-and-variances"><span class="header-section-number">6.2</span> Conditional and Joint Expectations and Variances</h2>
<p>Up until this point we have considered the marginal probability distribution when exploring the measures of central tendency and spread. These help to summarize the marginal behaviour of a random quantity, capturing the distribution of, for instance, <span class="math inline">\(X\)</span> alone. When introducing distributions, we also made a point to introduce the conditional distribution as one which is particularly relevant when there is extra information. The question “what do wwe expect to happen, given that we have an additional piece of information?” is not only well-defined, but it is an incredibly common type of question to ask. To answer it, we require <strong>conditional expectations</strong>.</p>
<p>TODO: Include set of questions relating to conditional expectation.</p>
<p>In principle, a conditional expectation is no more challenging to calculate than a marginal expectation. Suppose we want to know teh expected value of <span class="math inline">\(X\)</span> assuming that we know that a second random quantity, <span class="math inline">\(Y\)</span> has taken on the value <span class="math inline">\(y\)</span>. We write this as <span class="math inline">\(E[X|Y=y]\)</span>, and all we do is replace <span class="math inline">\(p_X(x)\)</span> with <span class="math inline">\(p_{X|Y}(x|y)\)</span> in the defining relationship. That is <span class="math display">\[E[X|Y=y] = \sum_{x\in\mathcal{X}}xp_{X|Y}(x|y).\]</span> In a sense, we can think of the conditional distribution of <span class="math inline">\(X|Y=y\)</span> as simply being a distribution itself, and then work with that no differently. The conditional variance, which we denote <span class="math inline">\(\text{var}(X|Y=y)\)</span> is also exactly the same.</p>
<p>TODO: Include an example.</p>
<p>Above we supposed that we knew that <span class="math inline">\(Y=y\)</span>. However, sometimes we want to work with the conditional distribution more generally. That is, we want to investigate the behaviour of <span class="math inline">\(X|Y\)</span>, without yet knowing what <span class="math inline">\(Y\)</span> equals. We can use the same procedure as above, however, this time we leave <span class="math inline">\(Y\)</span> unspecified. We denote this as <span class="math inline">\(E[X|Y]\)</span>, and this expression will be (in general) a function of <span class="math inline">\(Y\)</span>. Then, whenever a value for <span class="math inline">\(Y\)</span> is observed, we can simply specify <span class="math inline">\(Y=y\)</span>, deriving the specific value. In practice, we will typically compute <span class="math inline">\(E[X|Y]\)</span> rather than <span class="math inline">\(E[X|Y=y]\)</span>, since once we have <span class="math inline">\(E[X|Y]\)</span> we can easily find <span class="math inline">\(E[X|Y=y]\)</span> for <em>every</em> value of <span class="math inline">\(y\)</span>.</p>
<p>TODO: Show example.</p>
<p>Since <span class="math inline">\(E[X|Y]\)</span> is a function of an unknown random quantity, <span class="math inline">\(Y\)</span>, <span class="math inline">\(E[X|Y]\)</span> is also a random variable. It is a transformation of <span class="math inline">\(Y\)</span>, and as such, it will have some distribution, some expectation, and some variance itself. This is often a confusing concept when it is first introduced, so to recap: <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are both random variables; $E[X] and <span class="math inline">\(E[Y]\)</span> are both constant, numerical values describing the distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>; <span class="math inline">\(E[X|Y=y]\)</span> and <span class="math inline">\(E[Y|X=x]\)</span> are each numeric constants which summarize the distribution of <span class="math inline">\(X|Y=y\)</span> and <span class="math inline">\(Y|X=x\)</span> respectively; <span class="math inline">\(E[X|Y]\)</span> and <span class="math inline">\(E[Y|X]\)</span> are functions of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, respectively, and can as such be seen as transformations of (and random quantities depending on) <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> respectively.</p>
<p>We do not often think of the distribution of <span class="math inline">\(E[X|Y]\)</span> directly, however, there is a very useful result about both its expected value and its variance, which will commonly be exploited. Specifically, if we take the expected value of <span class="math inline">\(E[X|Y]\)</span> we will find that <span class="math inline">\(E[E[X|Y]] = E[X]\)</span>. Note that since <span class="math inline">\(E[X|Y] = g(Y)\)</span> for some transformation, <span class="math inline">\(g\)</span>, the outer expectation is taken with respect to the distribution of <span class="math inline">\(Y\)</span>. Sometimes when this may get confusing we will use notation to emphasize this fact, specifically, <span class="math inline">\(E_Y[E_{X|Y}[X|Y]] = E_X[X]\)</span>. This notation is not necessary, but it can clarify when there is much going on, and is a useful technique to fallback on. <span class="math display">\[\begin{align*}
E_Y[E[X|Y]] &amp;= \sum_{y\in\mathcal{Y}} E[X|Y]p_Y(y) \\
&amp;= \sum_{y\in\mathcal{Y}}\left(\sum_{x\in\mathcal{X}}xp_{X|Y}(x|Y)\right)p_Y(y) \\
&amp;= \sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}}x\frac{p_{X,Y}(x,y)}{p_Y(y)}p_Y(y)\\
&amp;= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}xp_{X,Y}(x,y)\\
&amp;= \sum_{x\in\mathcal{X}} xp_X(x)\\
&amp;= E[X].\end{align*}\]</span></p>
<p>TODO: Include example.</p>
<p>This property, that <span class="math inline">\(E[E[X|Y]] = E[X]\)</span> is important enough that it receives its own name: <strong>the law of total expectation</strong>. In the same way that it is sometimes easier to first condition on <span class="math inline">\(Y\)</span> in order to compute the marginal distribution of <span class="math inline">\(X\)</span> via applications of the law of total probability, so too can it be easier to first work out conditional expectations, and then take the expected value of the resulting expression. This adds on to the so-called “conditioning arguments” that were discussed previously, allowing a technique to work out the marginal mean indirectly.</p>
<p>TODO: Show example use case of LOTE.</p>
<p>While the conditional expectation is used quite prominently, the conditional variance is less central to the study of random variables. As discussed, briefly, the conditional variance is given by the same variance relationship, replacing the marginal probability distribution with the conditional one (just as with expectations). Just as with expectations, <span class="math inline">\(\text{var}(X|Y=y)\)</span> is a numeric quantity given by <span class="math inline">\(E[(X-E[X|Y=y])^2|Y=y]\)</span> and <span class="math inline">\(\text{var}(X|Y)\)</span> is a random variable given by <span class="math inline">\(E[(X-E[X|Y])^2|Y]\)</span>. This means that when working with the general, <span class="math inline">\(\text{var}(X|Y)\)</span>, we can also consider taking expectations of the resulting transformation.</p>
<p>TODO: Include examples.</p>
<p>A final result relating to conditional expectations and variances connects the two concepts. This is known as <strong>the law of total variance</strong>. For any random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we can write <span class="math display">\[\text{var}(X) = E[\text{var}(X|Y)] + \text{var}(E[X|Y]).\]</span> This result can be viewed as decomposing the variance of a random quantity into two separate components, and comes up again in later statistics courses. At this point we can view this as a method for connecting the marginal distribution through the conditional variance nad expectation.</p>
<p>TODO: Examples of this.</p>
<p>The final set of techniques to consider for now relate to making use of the joint distribution between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Specifically, if we have any function of two random variables, say <span class="math inline">\(g(X,Y)\)</span> and we wish to find <span class="math inline">\(E[g(X,Y)]\)</span>. This follows all of the expected derivations that we have used so far, this time replacing the marginal with the joint distribution. That is, <span class="math display">\[E[g(X,Y)] = \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g(x,y)p_{X,Y}(x,y).\]</span> For instance, if we want to consider the product of two random variables, we could use this technique to determine <span class="math inline">\(E[XY]\)</span>. The variance extends in the same manner as well.</p>
<p>TODO: Include example.</p>
<p>This defining relationship allows us to work out the expected value of a linear combination of two random variables. That is <span class="math display">\[\begin{align*}
E[X+Y] &amp;= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}(x+y)p_{X,Y}(x,y) \\
&amp;= \sum_{x\in\mathcal{X}}x\sum_{y\in\mathcal{Y}}p_{X,Y}(x,y) + \sum_{y\in\mathcal{Y}}y\sum_{x\in\mathcal{X}}p_{X,Y}(x,y) \\
&amp;= \sum_{x\in\mathcal{X}}xp_X(x) + \sum_{y\in\mathcal{Y}}yp_Y(y) \\
&amp;= E[X] + E[Y].\end{align*}\]</span></p>
<p>The same property does not apply with variances, at least not in general. To see this, consider that <span class="math display">\[\begin{align*}
E[(X+Y-E[X]-E[Y])^2] &amp;= E[((X-E[X])+(Y-E[Y]))^2] \\
&amp;= E[(X-E[X])^2] + E[(Y-E[Y])^2] + 2E[(X-E[X])(Y-E[Y])] \\
&amp;= \text{var}(X) + \text{var}(Y) + 2E[(X-E[X])(Y-E[Y])].\end{align*}\]</span> The term that impedes the linear relationship, <span class="math inline">\(E[(X-E[X])(Y-E[Y])]\)</span> can be computed as any joint function can be. This quantity, however, is particularly important when considering the relationship between two random variables. This is called the <strong>covariance</strong> and it is a measure of the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Typically we write <span class="math inline">\(E[(X-E[X])(Y-E[Y])] = \text{cov}(X,Y)\)</span> so that <span class="math display">\[\text{var}(X+Y) = \text{var}(X) + \text{var}(Y) + 2\text{cov}(X,Y).\]</span></p>
<p>TODO: Include example.</p>
<p>The covariance behaves similarly to the variance. We can see directly from the definition that <span class="math inline">\(\text{cov}(X,X) = \text{var}(X)\)</span>. Moreover, using similar arguments to those used for the variance, we can show that <span class="math display">\[\text{cov}(aX+b,cY+d) = ac\text{cov}(X,Y).\]</span> Covariances remain linear, so that <span class="math display">\[\text{cov}(X+Y,X+Y+Z)=\text{cov}(X,X)+\text{cov}(X,Y)+\text{cov}(X,Z)+\text{cov}(Y,X)+\text{cov}(Y,Y)+\text{cov}(Y,Z).\]</span> These make covariances somewhat nicer to deal with than variances, and on occasion it may be easier to think of variances as covariances with themselves.</p>
<p>TODO: Example? Maybe.</p>
<p>It is worth considering, briefly, the ways in which conditional and joint expectations interact. Namely, if we know that <span class="math inline">\(Y=y\)</span>, then the transformation <span class="math inline">\(g(X,y)\)</span> only has one random component, which is the <span class="math inline">\(X\)</span>. As a result, taking <span class="math inline">\(E[g(X,Y)|Y=y] = E[g(X,y)|Y=y]\)</span>. If instead we use the conditional distribution without a specific value, we still have that <span class="math inline">\(Y\)</span> is fixed within the expression, it is just fixed to an unknown quantity. That is <span class="math inline">\(E[g(X,Y)|Y]\)</span> will be a function of <span class="math inline">\(Y\)</span>. We saw before that <span class="math inline">\(E[E[X|Y]] = E[X]\)</span>, and the same is true in the joint case. Thus, one technique for computing the joint expectation, <span class="math inline">\(g(X,Y)\)</span> is to first compute the conditional expectation, and then compute the marginal expectation of the resulting quantity.</p>
<p>TODO: example.</p>
</section>
<section id="independence-in-all-of-this" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="independence-in-all-of-this"><span class="header-section-number">6.3</span> Independence in all of this</h2>
<p>Whenever we can assume independence of random quantities, this allows us to greatly simplify teh expressions we are dealing with. Recall that the key defining relationship with independence is that <span class="math inline">\(p_{X,Y}(x,y) = p_X(x)p_Y(y)\)</span>. Suppose then that we can write <span class="math inline">\(g(X,Y) = g_X(X)g_Y(Y)\)</span>. For instance, for the covariance we have <span class="math inline">\(g(X,Y)=(x-E[X])(Y-E[Y])\)</span> and so <span class="math inline">\(g_X(X) = X-E[X]\)</span> and <span class="math inline">\(g_Y(Y) = Y-E[Y]\)</span>. If we want to compute <span class="math inline">\(E[g(X,Y)]\)</span> then we get <span class="math display">\[\begin{align*}
E[g(X,Y)] &amp;= E[g_X(X)g_Y(Y)] \\
&amp;= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g_X(x)g_Y(y)p_{X,Y}(x,y) \\
&amp;= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g_X(x)g_Y(y)p_X(x)p_Y(y) \\
&amp;=\sum_{x\in\mathcal{X}}g_X(x)p_X(x)\sum_{y\in\mathcal{Y}}g_Y(y)p_Y(y)\\
&amp;= E[g_X(X)]E[g_Y(Y)].\end{align*}\]</span> Thus, whenever random variables are independent, we have the ability to separate them over their expectations.</p>
<p>TODO: example.</p>
<p>Consider what this means, in particular, for the covariance between independent random variables. If <span class="math inline">\(X\perp Y\)</span> then <span class="math display">\[\begin{align*}
\text{cov}(X,Y) &amp;= E[(X-E[X])(Y-E[Y])] \\
&amp;= E[(X-E[X])]E[(Y-E[Y])] \\
&amp;= (E[X]-E[X])(E[Y]-E[Y]) \\
&amp;= 0.\end{align*}\]</span> That is to say, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\text{cov}(X,Y)=0\)</span>. As a result of this, for independent random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> we also must have that <span class="math inline">\(\text{var}(X+Y)=\text{var}(X)+\text{var}(Y)\)</span>. It is critical to note that this relationship does not go both ways: you are able to have <span class="math inline">\(\text{cov}(X,Y) = 0\)</span> even if <span class="math inline">\(X\not\perp Y\)</span>.</p>
<p>TODO: Include example of independence.</p>
<p>While we have primarily focused on joint and conditional probabilities with two random variables, the same procedures and ideas apply with three or more as well. The relevant joint distribution, or conditional distribution would simply need to be substituted in definitions. Often the complexity here becomes a matter of keeping track of which quantities are random, and which are not. For instance, if we have <span class="math inline">\(X,Y,Z\)</span> as random variables, then <span class="math inline">\(E[X|Y,Z]\)</span> is a random function of <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>. We will still have that <span class="math inline">\(E[E[X|Y,Z]] = E[X]\)</span>, however, the outer expectation is now the joint expectation with respect to <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>. As a result, we can also write <span class="math inline">\(E[E[X|Y,Z]|Y]\)</span>. The first expectation will be with respect to <span class="math inline">\(X|Y,Z\)</span>, while the outer expectation is with respect to <span class="math inline">\(Z|Y\)</span>. This becomes a useful demonstration for when making the distribution of the expectation explicit helps to clarify what is being computed. As a general rule of thumb, the innermost expectations will always have more conditioning variables than the outer ones: each time we step out, we peel back one of hte conditional variables until the outermost is either a marginal (or joint). This will help to keep things clear.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          trigger: 'click',
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          positionFixed: true,
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notes/chapter5.html" class="pagination-link  aria-label=" &lt;span="" statistical="" experiments="" with="" random="" variables&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Summarizing Statistical Experiments with Random Variables</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/chapter7.html" class="pagination-link" aria-label="<span class='chapter-number'>7</span>&nbsp; <span class='chapter-title'>The Named Discrete Distributtions</span>">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Named Discrete Distributtions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>
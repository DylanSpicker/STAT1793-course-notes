# Introduction

## What is this Course?
TODO: Write this.


## What is Probability?
At its core, statistics is the study of uncertainty. Uncertainty permeates the world around us, and in order to make sense of the world, we need to make sense of uncertainty. The language of uncertainty is **probability**. Probability is a concept which we all likely have some intuitive sense of: if there was a 90% probability of rain today, you likely considered grabbing an umbrella; you are not likely to wager your life savings on a game that has only a 1% probability of paying out; etc. We have a sense that probability provides a measure of *likelihood*. Defining probability is a non-trivial task, and there have been many attempts to formalize it throughout history. While we will spend a good deal of time formalizing notions of probability in this course, let us pause and emphasize the intuitive basis you are starting with.

Suppose that two friends, Charles and Sadie, meet for coffee once a week. During their meetings they have wide-ranging, deep, philosophical conversations spanning the important issues from "do we all really see green as the same colour" to "why is it that 'q' comes as early in the alphabet as it does? it deserves to be with UVWXYZ." Beyond making progress on the most pressing issues of our time, Charles and Sadie each adore probability. As a result, at the end of each of their meetings, they play a game to decide who will pay: they flip a coin three times. If two or more heads come up, Charles pays, otherwise, Sadie pays.

We can think about the strategy that they are using here and intuitively feel that this is going to be "fair". With two or more heads, Charles pays; with two or more tails, Sadie pays; there always has to be either two or more heads *or* two or more tails, and each is equally likely to come up [footnote: TODO: recent literature on fair coins]. The outcome of their game is uncertain before it begins, but we know that in the long-run neither of the friends is going to be disadvantaged relative to the other. We can say that the probability that either of them pays is equal, it's 50-50, everything is balanced.

One day, in the middle of their game, Charles gets a very important phone call (someone has just pointed out the irony in the fact that there is no synonym for synonym [footnote: TODO: is this actually ture?]) and he leaves abruptly after the first coin has been tossed. The first coin toss showed a heads. Sadie, recognizing the gravity of the phone call, pays for the both of them, but she also recognizes that Charles was well on the way to having to pay.

:::{.callout-tip icon=false}
## [{{<fa question-circle>}}]{.text-success} Example: Basic Probability Enumeration

[What is the probability that Sadie would have had to pay in the aforementioned scenario? That is, assuming that the first coin shows a head, what is the probability that at least two heads are shown on the first three coin tosses?]{.lead}

::::{.callout .solution collapse="true"}
## Solution
Sadie figures that any coin toss is equally likely to show heads or tails. because the first coin showed heads, then there are four possible sequences that could have shown up: $H,H,H$, $H,H,T$, $H,T,H$, $H,T,T$.

In three of these situations ($H,H,H$, $H,H,T$, $H,T,H$) there are two heads and so Charles would have to pay. In one of them there are two tails, and so Sadie would have to pay. As a result, Charles would have to pay with probability $0.25$ and Sadie with probability $0.75$.
::::
:::

We can see from this example that Sadie should likely have not paid. Only one times out of four would she have had to, given the first head. However, we can not be certain that, had all three tosses been observed, Sadie would not have paid. It is possible that we would have observed two tails, making her responsible for the bill. This possibility happens one time out of four, which is more likely than  the probability of rolling a 4 on a six-sided die. Of course, 4s are rolled on six-sided dice quite frequently (about one out of every six rolls), and so it is not all together unreasonable for her to have paid.

This seemingly simple concept is the core of probability. Probability serves as a method for quantifying uncertainty. It allows us to make numeric statements regarding the set of outcomes that we can observe, quantifying the freqneucy with which we expect to observe those outcomes. However, probability does not *remove* the uncertainty. We still need to flip the coin or roll the die to understand what will happen. All probability has given us is a set of tools to exactly quantify this uncertainty. It turns out that these tools are critical for decision making in the face of the ever present uncertainty around us.

## How to Interpret Probabilities (like a frequentist)
We indicated that, intuitively, probability is a measure of the frequency with which a particular outcome occurs. This intuition can be codified exactly with the **frequentist interpretation** of probability. According to the frequentist interpretation (or frequentism, as it is often called), probabilities correspond to the proportion of time that whatever the event of interest is (be that flipping a heads, rolling a four, or observing rain on a given day) actually occcurs, in the long run. For a frequentist, you imagine yourself performing the experiment you are running over, and over, and over, and over, and over again. Each time you record "did the event happen?" and you count up those occurences. As you do this more and more and more, wherever that proportion lands corresponds to the probability.

TODO: Insert R Diagram for This (Coin Tosses)

To formalize this in terms of mathematics, we have to define several important terms. An **experiment** is any action whose outcome cannot be known with ceertainty, before it is performed. An **event** is a specified result that may or may not occur when an experiment is performed.

Suppose that an experiment is able to be performed as many times as one likes, limited only by your boredom. If you take $k_N$ to represent the number of times that the event of interest occurs when you perform the experiment $N$ times, then a frequentist would define the probability of the event as $$\text{probability} = \lim_{N\to\infty}\frac{k_N}{N}.$$

In practice this means that, in order to interpret probabilities, we think about repeating an experiment many, many times over. As we do that, we observe the proportion of times that any particular outcome occurs, and take that to be the defining relation for probabilities. The reason that we say the probability of fllipping a heads is $0.5$ is because if we were to sit around and flip a coin (experiment) over, and over, and over again, then in the long-run we would observea head (event) in $0.5$ of cases.

Many situations in the real world are not able to be run over and over again. Think about, for instance, the probability that a particular candidate wins in a particualr election. There is uncertainty there, of course, but the election can only be run once. What then? There are several ways through these types of events. 

First, we can rely on the **power of imagination**. There is nothing stopping us from envisioning the hypothetical possibility of running the election over, and over, and over, and over again. If we step outside of reality for a moment, we can ask "if we could play the day of the election many, many, many times, what proportion of those days would end with the candidate being elected?" If we say that the candidate has a 75% chance of being elected, then we mean that in $0.75$ of those imaginaed worlds, the candidate wins. It is crucial to stress that in our imagination here, we need to be thinking about the **exact same day** over and over again. We cannot imagine a different path leading to the election, different speeches being given in advance, or different opposition candidates. If we start from the same place, and play it out many times over, what happens in each of those worlds?

The repeated imagination is not for everyone. In light of these types of shortcomings, alternative proposals to the definition of probability had been made. Most popularly, the **Bayesian interpretation** has become prominent in recent-ish years. To Bayesians, probability is a measure of subjective belief. To say that there is a $50\%$ chance of a coin coming up is a statement about one's knowledge in the world: typically, coins show up heads half the time, so that's our belief about heads. The Bayesian view, though built around subjective confidence in the state of the world, can be formalized mathematically as well. A Bayesian considers the prior evidence that they have about the world, whatever evidence has been collected, and then they update their subject beliefs balancing these two sources of information. 

:::{.callout-note}
## Bayesian Probabilities and Belief Updating

Suppose that a Bayesian is flipping a coin. Before any flips have been made the Bayesian, understandably believes that the coin will come up heads $50\%$ of the time. However, when the coin starts to be flipped, the observations are a string of tails in a row.

After having flipped the coin five times, the individual has observed five tails. Of course, it is totally possible to flip a fair coin five times and see five tails, but there is a level of skepticism growing.

After 10 flips, the Bayesian has still not seen a head. At this point, the subjective belief is that there is likely something unfair about this coin: even though the experiment started with a baseline assumption that it was fair, the Bayesian no longer believes that the enxt flip will be a head.

As this goes on, you can imagine the Bayesian continuing to update their view of the world. The probability is an evolving concept, capturing what was thought, and what has been observed.
:::

For the election example, the Bayesian interpretation is easier to write out: based on everything that has been observed, and any prior beliefs about the viability of the candidates, the probability that a candidate wins the election is the subjective likelihood assigned to that outcome. Of course, if we disagree about prior beliefs, or have experienced different pieces of information, then we may disagree on the probability: that is okay.

In this course, we will focus on the frequentist interpretation. In part this is because frequentist probability is an easier introduction to the concepts that are necessary for taming uncertainty. In part this is because frequentist interpretations have been shown to give individuals a better grasp on understanding probabilities [CITE?]. However, it is important to know and recognize that there is a world beyond frequentist probability and statistics, one which can be very powerful once it is unlocked. (More on that in later years, if you so desire!)

If probability measures the long-term frequency of times that a particular event occurs, then how can we go about computing probabilities? Do we require to perform an experiment over and over again? Fortunately, the answer is no. The tools of probability we will cover allow us to make concrete statements about the probabilities of events without the need for repeated experimental runs. However, before dismissing the idea of repeatedly running an experiment at face value it is worth considering a tool we have at our disposal that renders this more possible than it has ever been: computers.

## R Programming for Probability and Statistics
Throughout this course we will make use of a programming language for statistical computing, called `R`. While clasically introductory statistics involved heavy computation of particular quantities, by hand, the use of a programming language (like R) frees us from the tedium of calculation, allowing for a far deeper focus on understanding, explanation, decision-making, and complex problem solving. While you will not be expected to write large R programs on your own, you will be expected to read simple scripts, make basic modifications, and to run code that is given to you.

Throughout these course notes, where relevant, R code will be provided to demonstrate the ideas being discussed. It may be useful to have R open alongside the notes to ensure that you can get the same results that are preinted throughout the notes. In this section we will cover some of the very basics of using R, and reading R code. If you are interested, there are plenty of resources to becoming a more proficient R programmer: this is a skill that will benefit you not only in this course, but in many courses to come, and far beyond your university training. If you have any programming in your background, R is a fairly simple language to learn; if not, R can be quite beginner friendly. 

TODO: Write some basic intro to R stuff.

Recall that the motivation for the discussion of R was the frequentist interpretation of probability. One task that computers are very effective at is repeatedly performing some action. As a result, we can use computers to mimick the idea of repeatedly performing some experiment. Consider the simple case of flipping a coin over and over again.

We can use `sample(options, size)` as a function to select  `size` realizations from the set contained in `options`. Thus, if we take `{r} sample(c("H","T"), size=1)` we can view this as flipping a coin one time. If we use the loop structure  we talked before, then we can simulate the experience of repeatedly flipping a coin. Consider the following R code.

```{r}
#| eval: true

set.seed(3141592) #<1> 

number_of_runs <- 1000 # <2> 
tosses <- c() # <3> 

for(idx in 1:number_of_runs){ # <4> 
    toss <- sample(c("H","T"), 1) # <5> 
    if(toss == "H"){  # <6> 
        tosses <- c(tosses, 1) # <6> 
    } else { # <6> 
        tosses <- c(tosses, 0) # <6> 
    }
}

mean(tosses) # <7> 
```
1. A seed ensures that the random numbers generated by the program are always the same. This helps to be able to reproduce our work.
2. This is how many times we want to repeat the experiment.
3. This is where we are going to store the results of our tosses. It creates an empty list for us.
4. Here we are going to loop over the experiments, one for each run.
5. This is our coin toss. We are going to sample 1 from either 'H' or 'T'
6. If the coin toss is heads, then we add a 1 to the list. Otherwise, we add a zero to the list.
7. Return the mean of all of the tosses.

It is worth adjusting some of the parameters within the simulation, and seeing what happens. What if you ran the experiment only 5 times? Ten thousand times? What if instead of counting the number of heads, we wanted to count the number of tails? What if we wanted to count the number of times that a six-sided die rolled a 4? All of these settings can be investigated with simple modifications to the provided script.

::: {.content-visible when-format="html"}

When viewing the notes on the web, you will have the capacity to actually run the code, directly in your browser, and play around with the values that it gives. The code running will be somewhat limited, and so there may be cases where you are better copying the code onto your own computer, but this provides a really excellent method for getting used to working with R code directly. Consider the following code block and this time actually see if you can update the sample sizes, or change the generation process at all. There is no harm in trying: anything you do can be undone simply be reloading the page! 
```{webr-r}
set.seed(3141592) 

number_of_runs <- 1000 
tosses <- c() 

for(idx in 1:number_of_runs){ 
    toss <- sample(c("H","T"), 1) 
    if(toss == "H"){  
        tosses <- c(tosses, 1) 
    } else { 
        tosses <- c(tosses, 0) 
    }
}

mean(tosses) 
```
:::
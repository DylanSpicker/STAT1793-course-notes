# Introduction

## What is Probability?
At its core, statistics is the study of uncertainty. Uncertainty permeates the world around us, and in order to make sense of the world, we need to make sense of uncertainty. The language of uncertainty is **probability**. Probability is a concept which we all likely have some intuitive sense of. If there was a 90% probability of rain today, you likely considered grabbing an umbrella. You are not likely to wager your life savings on a game that has only a 1% probability of paying out. We have a sense that probability provides a measure of *likelihood*. Defining probability mathematically is a non-trivial task, and there have been many attempts to formalize it throughout history. While we will spend a good deal of time formalizing notions of probability in this course, we first pause to emphasize the familiarity with probability that you are likely starting with.

Suppose that two friends, Charles and Sadie, meet for coffee once a week. During their meetings they have wide-ranging, deep, philosophical conversations spanning across many important topics.^[For instance they ask "do we all really see green as the same colour?" or "why is it that 'q' comes as early in the alphabet as it does? it deserves to be with 'UVWXYZ'?"] Beyond making progress on some of the most pressing issues of our time, Charles and Sadie each adore probability. As a result, at the end of each of their meetings, they play a game to decide who will pay. The game proceeds by having them flip a coin three times. If two or more heads come up Charles pays, and otherwise Sadie pays.

We can think about the strategy that they are using here and *feel* that this is going to be "fair". With two or more heads, Charles pays. With two or more tails, Sadie pays. There always has to be either two or more heads *or* two or more tails, and each is equally likely to come up^[There has been some recent literature, see @fairCoin, which may suggest that a coin is ever so slightly more likely to land on the same side it started on, perhaps undermining this assertion.]. The outcome of their game is uncertain before it begins, but we know that in the long run neither of the friends is going to be disadvantaged relative to the other. We can say that the probability that either of them pays is equal. It's 50-50. Everything is balanced.

Now imagine that one day, in the middle of their game, Charles gets a very important phone call ^[Someone has just pointed out the irony in the fact that there is no synonym for synonym. Technically, there are the words *metonym* and *poecilonym* and *polyonym*, but these are rarely used and Charles would wager that there is a very high probability you have never seen them.] and he leaves abruptly after the first coin has been tossed. The first coin toss showed a heads. Sadie, recognizing the gravity of the phone call, pays for the both of them, but she also realizes that Charles was well on the way to having to pay.

:::{#exm-basic-prob}
## Basic Probability Enumeration

What is the probability that Sadie would have had to pay in the aforementioned scenario? That is, assuming that the first coin shows a head, what is the probability that at least two heads are shown on the first three coin tosses?

::::{.callout .solution collapse="true"}
## Solution
Sadie figures that any coin toss is equally likely to show heads or tails. because the first coin showed heads, then there are four possible sequences that could have shown up:

1. $H,H,H$;
2. $H,H,T$;
3. $H,T,H$;
4. $H,T,T$.

In three of these situations ((1) $H,H,H$, (2) $H,H,T$, and (3) $H,T,H$) there are two heads and so Charles would have to pay. In one of them there are two tails, and so Sadie would have to pay. As a result, Charles would have to pay in $3$ of the $4$ (with probability $0.75$) and Sadie in $1$ of the $4$ (with probability $0.25$).
::::
:::

Looking at @exm-basic-prob, we can see that Sadie should likely have not paid. Only one out of every four times would she have had to pay, given the first coin being heads. However, we can not be certain that, had all three tosses been observed, Sadie would *not* have paid. It is possible that we would have observed two tails, making her responsible for the bill. This possibility happens one time out of four, which is more likely than the probability of rolling a four on a six-sided die^[This event happens one time out of every six.]. Fours are rolled on six-sided dice quite frequently^[Again, about one out of every six rolls], and so it is not all together unreasonable for her to have paid.

This seemingly simple concept is the core of probability. Probability serves as a method for quantifying uncertainty. It allows us to make numeric statements regarding the set of outcomes that we can observe, by quantifying the frequency with which we expect to observe those outcomes. Probability does not *remove* the uncertainty. We still need to flip the coin or roll the die to know what will happen. All probability gives us is a set of tools to quantify this uncertainty. These tools are critical for decision making in the face of the ever-present uncertainty around us.

## How to Interpret Probabilities (like a Frequentist)
We indicated that, intuitively, probability is a measure of the frequency with which a particular outcome occurs. This intuition can be codified exactly with the **Frequentist interpretation** of probability. According to the Frequentist interpretation (or frequentism, as it is often called), probabilities correspond to the proportion of time any event of interest is^[Be that flipping a heads, rolling a four, or observing rain on a given day.] actually occurs in the long run. For a Frequentist, you imagine yourself performing the experiment you are running over, and over, and over, and over, and over again. Each time you answer "did the event happen?" and you count up those occurrences. As you do this more and more and more, wherever that proportion lands corresponds to the probability.

```{r}
#| echo: false
#| label: fig-plot
#| fig-cap: "This plot simulates the repeated tossing of a coin. The x-axis represents the number of coins being tossed, and the y-axis plots the proportion of times that heads has shown up cumulatively over the tosses thus far. We can see in the long run that this proportion tends towards $0.5$."

set.seed(31415)

draws <- 5000
sequence <- rbinom(draws, 1, 0.5)

x <- 1:draws
y <- cumsum(sequence)/x

plot(y ~ x, main = "Long run proportion of heads observed tossing a fair coin.", ylab = "Proportion", xlab = "Number of Tosses", type = 'l')
abline(h=0.5, lty=3)
```

To formalize this mathematically, we first define several important terms. 

:::{#def-experiment}
## Experiment 
Any action to be performed whose outcome is not (or cannot) be known with certainty, before it is performed. 
:::
:::{#def-event}
## Event [Informally]
A specified result that may or may not occur when an experiment is performed.
:::

Suppose that an experiment is able to be performed as many times as one likes, limited only by your boredom. If you take $k_N$ to represent the number of times that the event of interest occurs when you perform the experiment $N$ times, then a Frequentist would define the probability of the event as $$\text{probability} = \lim_{N\to\infty}\frac{k_N}{N}.$$ This course does not assume that you have any familiarity with calculus, and yet, this definition relies on limits, a concept taken directly from calculus. However, we will not actually require the ability to work with limits for this course. Instead, when you see a statement of the form $$\lim_{x\to\infty} f(x),$$ simply think "what is happening to the function $f(x)$ as $x$ grows and grows (off to $\infty$)?"

In practice this means that, in order to interpret probabilities, we think about repeating an experiment many, many times over. As we do that, we observe the proportion of times that any particular outcome occurs, and take that to be the defining relation for probabilities. The reason that we say the probability of flipping a heads is $0.5$ is because if we were to sit around and flip a coin^[Our experiment.] over, and over, and over again, then in the long-run we would observe a head^[Our event.] in $0.5$ of cases.

:::{#exm-basic-prob}
## Probability Interpretation

How do we interpret the statement "the probability that Sadie would have had to pay, given a head on the first toss, is $0.25$"? Recall that in the game, they toss three coins, and Sadie pays if two of them show tails.

::::{.callout .solution collapse="true"}
## Solution
This statement means that, if Sadie were to repeatedly be in the situation where one head has shown and there are two coins left to toss, then in $0.25$ of these situations (in the limit, as this is repeated an infinite number of times) will end up showing two tails.
::::
:::

Many situations in the real world are not able to be run over and over again. Think about, for instance, the probability that a particular candidate wins in a particular election. There is uncertainty there, of course, but the election can only be run once. What then? There are several ways through these types of events. 

First, we can rely on the **power of imagination**. There is nothing stopping us from envisioning the hypothetical possibility of running the election over, and over, and over, and over again. If we step outside of reality for a moment, we can ask "if we could play the day of the election many, many, many times, what proportion of those days would end with the candidate being elected?" If we say that the candidate has a 75% chance of being elected, then we mean that in $0.75$ of those imagined worlds, the candidate wins. It is crucial to stress that in our imagination here, we need to be thinking about the **exact same day** over and over again. We cannot imagine a different path leading to the election, different speeches being given in advance, or different opposition candidates. If we start from the same place, and play it out many times over, what happens in each of those worlds?

This repeated imagining is not for everyone. As a result, alternative proposals to the interpretation of probability have been made. Most popularly, the **Bayesian interpretation** has recently become prominent. To Bayesians, probability is a measure of subjective belief. To say that there is a $50\%$ chance of a coin coming up is a statement about one's knowledge of the world. Typically, coins show up heads half the time, so that's our belief about heads. The Bayesian view, built around subjective confidence in the state of the world, can be formalized mathematically as well. A Bayesian considers the *prior evidence* that they have about the world^[Any relevant evidence that has previously been collected.] and combine this with current observations in order to update their subject beliefs, balancing these two sources of information. 

:::{.remark}
## Bayesian Probabilities and Belief Updating

Suppose that a Bayesian is flipping a coin. Before any flips have been made the Bayesian understandably believes that the coin will come up heads $50\%$ of the time. However, when the coin starts to be flipped, the observations are a string of tails in a row.

After having flipped the coin five times, the individual has observed five tails. Of course, it is totally possible to flip a fair coin five times and see five tails, but there is a level of skepticism growing.

After 10 flips, the Bayesian has still not seen a head. At this point, the subjective belief is that there is likely something unfair about this coin. Even though the experiment started with a baseline assumption that the coin was fair the Bayesian no longer believes that the next flip will be a head.

As this goes on, you can imagine the Bayesian continuing to update their view of the world. To them, the probability is an evolving concept, capturing what was believed and what has been observed.
:::

For the election example, the Bayesian interpretation is somewhat easier to think through. To say that a candidate has a $75\%$ chance of winning the election means that "based on everything that has been observed, and any prior beliefs about the viability of the candidates, the subjective likelihood that the candidate wins the election is $0.75$". If we disagree about prior beliefs, or have experienced different pieces of information, then we may disagree on the probability. That is okay.

In this course, we focus on the Frequentist interpretation. This is, in part, because Frequentist probability is an easier introduction to the concepts that are necessary for grasping uncertainty. Additionally, there is some research to suggest that Frequentist interpretations are fairly well understood by the general public ^[See, for instance @rainPaper, where they study how people interpret the statement "there is a $30\%$ chance of rain tomorrow." Interestingly, most people can convert this into a frequency statement ($3$ out of $10$, say), even if the specific meaning is sometimes lost. They conclude that there are other issues in attempting to understanding this statement, issues which we will address later on.]. However, it is important to know and recognize that there is a world beyond Frequentist Probability and Statistics, one which can be very powerful once it is unlocked. ^[More on this in later years, if you so desire! Come have a chat, if that sounds interesting.]

If probability measures the long term proportion of times that a particular event occurs, how can we go about computing probabilities? Do we require to perform an experiment over and over again? Fortunately, the answer is no. The tools of probability we will cover allow us to make concrete statements about the probabilities of events without the need for repeated experimental runs. However, before dismissing the idea of repeatedly running an experiment at face value, it is worth considering a tool we have at our disposal that renders this more possible than it has ever been: computers.

## R Programming for Probability and Statistics
Throughout this course we will make use of a programming language for statistical computing, called R. Classically, introductory statistics courses involved heavy computation of particular quantities by hand. The use of a programming language (like R) frees us from the tedium of these calculations, allowing for a deeper focus on understanding, explanation, decision-making, and complex problem solving. This is **not** to say we will *never* do problems by hand^[To the contrary, some amount of by-hand problem solving helps to solidify these concepts.], however, we will emphasize the use of statistical computing often. While you will not be expected to write R programs on your own, you will be expected to read simple scripts, make basic modifications, and to run code that is given to you.

::: {.content-visible when-format="html"}
Throughout these course notes, where relevant, R code will be provided to demonstrate the ideas being discussed. When viewing the notes on the web, you will have the capacity to actually run the code, directly in your browser, and play around with the values that it gives. The code running will be somewhat limited, and so there may be cases where you are better copying the code onto your own computer, but this provides a really excellent method for getting used to working with R code directly. In this section we will cover some of the very basics of using R, and reading R code. If you are interested, there are plenty of resources to becoming a more proficient R programmer. This is a skill that will benefit you not only in this course, but in many courses to come, and far beyond your university training. If you have any programming in your background, R is a fairly simple language to learn; if not, R can be quite beginner friendly. 
:::
::: {.content-visible when-format="pdf"}
Throughout these course notes, where relevant, R code will be provided to demonstrate the ideas being discussed. It may be useful to have R open alongside the notes to ensure that you can get the same results that are printed throughout. In this section we will cover some of the very basics of using R, and reading R code. If you are interested, there are plenty of resources to becoming a more proficient R programmer. This is a skill that will benefit you not only in this course, but in many courses to come, and far beyond your university training. If you have any programming in your background, R is a fairly simple language to learn; if not, R can be quite beginner friendly. 
:::

### Basic Introduction to R Programming
When programming, the basic idea is that we are going to write instructions in a **script** which we will tell our computer to execute. These instructions are^[Typically. There are some exceptions to this, but if this is your first time programming, you need not worry about that!] performed one-by-one, from the top to the bottom of the script. We can have instructions which operate on their own, or which interact with previous (or future) instructions to add to the complexity. The trick with programming then is to determine which actions you need the computer to perform, in which order, to accomplish the task that you are setting out to do.

To begin, we may consider a very simple R program, one which uses the programming language as a basic calculator. 

::: {.content-visible when-format="pdf"}
```{r}
5 + 3 - (10*2) + 8^(25/3)
```
:::
::: {.content-visible when-format="html"}
```{webr-r}
5 + 3 - (10*2) + 8^(25/3)
```
:::

Here, the we ask the computer to perform some simple arithmetic operations. We use `+` for addition, `-` for subtraction, `*` for multiplication, `/` for division, and `^` for exponentiation. With the use of parentheses, any expressions relating to these basic operations can be performed. Note that here the result is simply output after it is computed. Try modifying the exact expressions being computed, allowing you to feel comfortable with these types of mathematical operations.


::: {.content-visible when-format="pdf"}
```{r}
5 + 3 - (10*2) + 8^(25/3)
8 * 5
```
:::
::: {.content-visible when-format="html"}
```{webr-r}
5 + 3 - (10*2) + 8^(25/3)
8 * 5
```
:::

Here, we have two lines of math running with simple operations. Each is simply output when it is computed. These two results have no ability to interact with one another, and if we were to add more and more lines beneath, the same would continue to happen. If we want different commands to be able to interact with one another, we need a method for storing the results. To do so, we can define **variables**. In R, to define a variable, we use the syntax `variable_name <- variable_value`. We can choose *almost* anything that we want for the variable name^[The variable name must start with a letter and can be a combination of letters, numbers, periods, and underscore.] and the variable value can also be of many types.^[more on this later] The arrow between the two is the **assignment operator** and it simply tells R to assign the `variable_value` to be accessible from the `variable_name`. 

::: {.content-visible when-format="pdf"}
```{r}
my_5 <- 5
my_8 <- 8
my_5
my_8
```
:::
::: {.content-visible when-format="html"}
```{webr-r}
my_5 <- 5
my_8 <- 8
my_5
my_8
```
:::
In this code we assign the variable `my_5` to contain the value `r my_5`, and the variable `my_8` to contain the value `r my_8`. We can output these as expressions themselves, simply by typing the variable name. Simply outputting these variables is not of particular interest, however, we can use the variables in later statements by simply including the variable name in them.

::: {.content-visible when-format="pdf"}
```{r}
my_5 <- 5
my_8 <- 8
my_5*my_8
```
:::
::: {.content-visible when-format="html"}
```{webr-r}
my_5 <- 5
my_8 <- 8
my_5*my_8
```
:::
Here, instead of simply outputting the variables, we multiply them together. We could have used `5 * 8` in this case for the same result, however, we are afforded a lot more flexibility with this approach. Much of this flexibility comes from our capacity to *change* the values of variables over time. Consider the following script, and try to understand why the output is the way that it is.

::: {.content-visible when-format="pdf"}
```{r}
my_5 <- 5
my_8 <- 8
my_5*my_8

my_5 <- 10
my_5*my_8
```
:::
::: {.content-visible when-format="html"}
```{webr-r}
my_5 <- 5
my_8 <- 8
my_5*my_8

my_5 <- 10
my_5*my_8
```
:::
At the top point in the script, before the first `my_5*my_8` call, The variable `my_5` has the value `5`. However, after this is called, the value is updated to be `10`. Then, when we call `my_5*my_8` again, this is now simplified to `10 * 8`, giving the result. Perhaps more importantly, we can take the value of an expression and assign that to a variable itself. 

::: {.content-visible when-format="pdf"}
```{r}
my_5 <- 5
my_8 <- 8
result <- my_5*my_8
result 
```
:::
::: {.content-visible when-format="html"}
```{webr-r}
my_5 <- 5
my_8 <- 8
result <- my_5*my_8
result 
```
:::
Here, we define another variable. This time, `result` now contains the result of multiplying our previous two variables. Thus, when we output it, we get the same value. Take a moment to read through the following script, and try to understand what will happen at the end. 

::: {.content-visible when-format="pdf"}
```{r}
#| output: false 
my_5 <- 5
my_8 <- 8
result <- my_5*my_8

my_5 <- result
my_8 <- my_5
result <- my_5 * my_8

result 
```
:::
::: {.content-visible when-format="html"}
```{webr-r}
my_5 <- 5
my_8 <- 8
result <- my_5*my_8

my_5 <- result
my_8 <- my_5
result <- my_5 * my_8

result 
```
:::
The result is `r result`. Why? We can read through this step-by-step in order to understand this. First we set our variables to have the values of `5` and `8`. Then, `result` is made to be the product of our two variables, which in this case is `40`. After that, we set the value of `my_5` to be the same as the value of `result`, which gives `my_5` equal to `40`. At this point, `result` equals `40`, `my_5` equals `40`, and `my_8` is equal to `8`. The next line updates `my_8` to be the same as the value of `my_5`, which we just clarified was `40`. As a result, all of the variables we have defined now take on the value of `40`. The final line before output, `result <- my_5 * my_8` updates the value of `result` to be the product of the two variables again, this time giving `40 * 40` which gives `r result`. 

### Function Calls in R
Up until this point we have simply used numerical operations and variable assignment. While this allows R to serve as a very powerful calculator, we often want computers to do much more than arithmetic. As a result, we need to explore **functions** in R. A function is a piece of code which takes in various arguments and outputs some value (or values). Most of the way that we will use R in this course is through the use of function calls.^[Note: when you begin to write programs for yourself, a lot of your time will be spent writing custom functions. If this is of interest to you, I suggest looking into programming more! For this course we will not need to define our own functions, except perhaps ones that will be defined for you.] This is exactly analogous to a mathematical function: it is simply some rule which maps input to output. In fact, some of the most basic functions in R are functions which relate to mathematical functions. 

::: {.content-visible when-format="pdf"}
```{r}
x <- 10 

exp(x)  # <1>
sqrt(x) # <2>
log(x)  # <3>

```
1. Computes the exponential function applied to `x`, that is $e^x = e^{10}$.
2. Computes the square root of `x`, that is $\sqrt{x} = \sqrt{10}$.
3. Computes the natural logarithm of `x`, that is $\log(x) = \log(10)$.
:::
::: {.content-visible when-format="html"}
```{webr-r}
x <- 10 

exp(x)  # Computes the exponential, e to the power of x.
sqrt(x) # Computes the square root of x.
log(x)  # Computes the natural logarithm of x.
```
:::
The basic format of a function call will always be `function_name(param1, param2, ...)`. Each of these functions required only a single parameter, however, there are some functions which take more than one parameter. If we have decimal numbers, for instance, we may wish to round them. To do so, we can use the `round` function in R, which takes two parameters: the number we wish to round, and how many digits we wish to keep. 

::: {.content-visible when-format="pdf"}
```{r}
unrounded_value <- 3.141592
rounded_value <- round(unrounded_value, digits = 3)

rounded_value
```
:::
::: {.content-visible when-format="html"}
```{webr-r}
unrounded_value <- 3.141592
rounded_value <- round(unrounded_value, digits = 3)

rounded_value
```
:::
There are a few things to note about this sequence of function calls. First, note that we assign the output of a function to a new variable. This behaves exactly as we saw above with simple numeric calculations. Next, consider that the output of the function^[Which we have stored in the variable `rounded_value`] has a value of $3.142$. That is: we rounded the value to $3$ decimal points, exactly as would be expected. The final part to note is that the second parameter passed to the function call is **named**. That is, instead of simply calling `round(unrounded_value, 3)`, we have `round(unrounded_value, digits = 3)`. If you had made the first call this would have worked perfectly.^[Try it out to convince yourself!] However, R also provides the ability to pass in parameter names alongside the parameter values with the syntax `function_name(param_name = param_value, ...)`. The benefit to doing this is two-fold. First, it is easier to read what is happening, especially for function calls that you have never seen before. Second, it removes the need to have the parameters ordered correctly. It is best practice to **always** include parameter names where you can. 

::: {.content-visible when-format="pdf"}
```{r}
unrounded_value <- 3.141592
rounded_value <- round(digits = 3, x = unrounded_value)

rounded_value
```
:::
::: {.content-visible when-format="html"}
```{webr-r}
unrounded_value <- 3.141592
rounded_value <- round(digits = 3, x = unrounded_value)

rounded_value
```
:::
This code produces the exact same output, where now both parameters are named. Specifically, we could read this function call as "round the value of `x` to have `digits` decimal points." If you had instead written `round(3, unrounded_value)`, you would get the value `3`, since now we are rounding `3` to `3.141592` decimal points. 

### Moving Beyond Numeric Data
So far, everything that we have looked at has been numeric data. We have seen integers, and decimals. You can have negative results, say by taking `my_var <- -5`. And while numbers are frequently useful, we will require further types of data to write useful computer programs. For this course we will focus on three additional data types: textual data which (referred to as **strings**), true and false binary data (referred to as **booleans**), and lists of the same data type (referred to as **vectors**). They will behave in much the same way as numeric data, 

### R Programming for Probability Interpretations
Recall that the motivation for the discussion of R was the Frequentist interpretation of probability. One task that computers are very effective at is repeatedly performing some action. As a result, we can use computers to mimic the idea of repeatedly performing an experiment. Consider the simple case of flipping a coin over and over again.

We can use `sample(x, size)` as a function to select `size` realizations from the set contained in `x`. Thus, if we take `sample(x = c("H","T"), size=1)` we can view this as flipping a coin one time. If we use the loop structure we talked before, then we can simulate the experience of repeatedly flipping a coin. Consider the following R code.

::: {.content-visible when-format="pdf"}
```{r}
set.seed(3141592) # <1> 

number_of_runs <- 1000 # <2> 
tosses <- c() # <3> 

for(idx in 1:number_of_runs){ # <4> 
  toss <- sample(x = c("H","T"), size = 1) # <5> 
  if(toss == "H"){ # <6> 
    tosses <- c(tosses, 1) # <6> 
  } else { # <6> 
    tosses <- c(tosses, 0) # <6> 
  }
}

mean(tosses) # <7> 
```
1. A seed ensures that the random numbers generated by the program are always the same. This helps to be able to reproduce our work.
2. This is how many times we want to repeat the experiment.
3. This is where we are going to store the results of our tosses. It creates an empty list for us.
4. Here we are going to loop over the experiments, one for each run.
5. This is our coin toss. We are going to sample 1 from either 'H' or 'T'
6. If the coin toss is heads, then we add a 1 to the list. Otherwise, we add a zero to the list.
7. Return the mean of all of the tosses.
:::
::: {.content-visible when-format="html"}
Consider the following code block and this time actually see if you can update the sample sizes, or change the generation process at all. There is no harm in trying: anything you do can be undone simply be reloading the page! 
```{webr-r}
# A seed ensures that the random numbers generated by the program 
# are always the same. This helps to be able to reproduce our work.
set.seed(3141592) 

# This is how many times we want to repeat the experiment.
number_of_runs <- 1000 

# This is where we are going to store the results of our tosses. 
# It creates an empty list for us.
tosses <- c() 

# Here we are going to loop over the experiments, one for each run.
for(idx in 1:number_of_runs){ 
    # This is our coin toss. We are going to sample 1 from either 
    # 'H' or 'T'
    toss <- sample(x = c("H","T"), size = 1) 

    # If the coin toss is heads, then we add a 1 to the list. 
    # Otherwise, we add a zero to the list.
    if(toss == "H"){ 
        tosses <- c(tosses, 1) 
    } else { 
        tosses <- c(tosses, 0) 
    }
}

# Return the mean of all of the tosses.
mean(tosses) 
```
:::

It is worth adjusting some of the parameters within the simulation, and seeing what happens. What if you ran the experiment only 5 times? Ten thousand times? What if instead of counting the number of heads, we wanted to count the number of tails? What if we wanted to count the number of times that a six-sided die rolled a 4? All of these settings can be investigated with simple modifications to the provided script.

{"title":"Probailities with More than One Event","markdown":{"headingText":"Probailities with More than One Event","containsRefs":false,"markdown":"## Conditional Probability (and related theorems)\nRegardless of whether we are using the equally likely outcome model, or not, there are several properties of probability which can be derived that are either of direct interest themselves, or else are useful for manipulating probability expressions. These rules apply to any probability model, and greatly increase the flexibility of working with these models. \n\nUp until this point we have primarily focused on assigning probabilities to particular events. If we have some event of interest, $A$, then $P(A)$ is the probability that $A$ occurs in any manner. If we are using our equally likely probability model, then $P(A) = \\frac{N_A}{N}$. This is the probability of the event $A$ where nothing else is known at all. If we smooth over anything which could alter the likelihood, if we have no additional information, if we want the best guess for the likelihood of occurence in a vaccuum, this is the probability of interest. We refer to such quantities as **marginal probabilities**. \n\nWhile marginal probabilities are often of interest, and frequently are the best tool for summarizing the overall state of the world (or our knowledge regarding the state of the world), in pracctice we know that sometimes information that we have will change our understanding of the probability of an event. Suppose the event $A$ corresponds to the event that it snows tomorrow, in some particular city. It is possible to think about how often it snows on average, and report a value related to that as $P(A)$. Now, what if we know that it is currently the middle of summer? In this case, while $P(A)$ does not shrink to $0$, it becomes far less likely than if we did not have that information. Similarly, if we know that it is winter, the likelihood that it snows tomorrow increases. \n\nIn order to formall capture this we can introduce the idea of **conditional probability**. Unlike in the cas of marginal probabilities, conditional probabilities allow us to *condition* on extra pieces of information. Instead of asking \"what is the porbability of this event\", they instead ask, \"given that we know this piece of information, what is the probability of this event?\" The subtle distinction becomes quite powerful, both in terms of manipulating and working with probabilities, but also in terms  of expressing the correct events of interest for ourselves.\n\nTo make use of conditional probabilities,  we will think of the process of **conditioning** on (one or more) events. That is, we will talk of the probability of $A$ conditional on $B$,  where $A$ and $B$ are two events of interest. We write this quantity as $P(A|B)$, and typically will read this as \"the probability of $A$ given $B$.\" Intuitively,  this it the probability of $A$ happening, supposing that we know that $B$ has  already happened. \n\nTODO: Include some examples.\n\nRecall that $A$ and $B$, as  events, are merely subsets of the sample space, $\\mathcal{S}$. Each item in either $A$ or $B$ is one of the possible outcomes from the experiment or process that we are observing. Suppose that we know that $B$ has occurred. What this  means is that, one of the outcomes in the set $B$ was the observed outcome from the experiment. Now, if we want to know $P(A|B)$, we want  to know the probability, working from the assumption that $B$ has happened, that $A$ also happens. That is, knowing that $B$ has happened, what is the probability that $A$ and $B$ happen. \n\nThinking back to our discussion of events, $A$ and $B$ is denoted by $A \\cap B$, and it corresponds to the set of events inside the set $B$, which also belong to the  set $A$. Now, instead of considering the event $P(A\\cap B)$ directly, we need to acknowledge that for $A|B$, only the events in $B$ were possible. That is, instead of being divided by the whole space, we can only divide by the space of $B$. In some sense, we can view conditioning on $B$ as treating $B$ as though it is the full sample space, and finding probabilities within that. In general, $B$ will be smaller than  $\\mathcal{S}$, and so $P(B) < 1$. Instead of the conditional probability being \"out of\" $1$, it will instead be \"out of\" $P(B)$, which gives $P(A|B) = \\frac{P(A\\cap B)}{P(B)}$.\n\nTo make this more clear, let's consider a simple example. Suppose that we take $A$ to be the event that a $2$ is rolled on a fair, six-sided die, and $B$ to be the event that an even number was rolled. This is an equal probability model, and so each outcome gets $\\frac{1}{6}$ probability. The original sample space is  $\\mathcal{S} = \\{1,2,3,4,5,6\\}$, the event $A$ is $\\{2\\}$, and the event $B$ is $\\{2,4,6\\}$. In order for both $A$ and $B$ to occur, we note that we need $A \\cap B = \\{2\\}$. If we know that $B$ has occured, then we know that either a $2$, $4$, or $6$ has been rolled, with equal probability for each. Thus, intuitively,  we can view $B$ as the new sample space, and say that rolling a $2$ has a $\\frac{1}{3}$ probability, given that there are $3$ outcomes and $1$ of them is the event  of interest. Another way to consider this is to note that $P(B) = \\frac{1}{2}$, and so we need to scale each event by $\\frac{1}{1/2}$ in order to make sure that the total probability of our reduced sample space equals $1$. Then $P(A\\cap B) = \\frac{1}{6}$, so $$P(A|B) = \\frac{1/6}{1/2} = \\frac{1}{3}.$$\n\nTODO: try rewriting this a bit.\n\nSuppose that, instead of a fair die, it was  weighted so that $6$ comes up more frequently than the other options, coming up with probability $0.5$, and the other fives each coming up with probability $0.10$. If $A$ and $B$ are the same events as above, then $P(B) = 0.7$ now. If we know that $B$ has occurred, then the new sample space is $\\{2,4,6\\}$ where $P(2) = \\frac{0.1}{0.7} = \\frac{1}{7}$, $P(4) = \\frac{0.1}{0.7}$, and $P(6) = \\frac{0.5}{0.7} = \\frac{5}{7}$. Note that these three probabilities sum to $1$ still, which constitutes a valid probability model, and so $P(A|B) = \\frac{1}{7}$.\n\nTODO: Add example about this.\n\nUltimately, the conditional probability of $A$ given $B$ is gtgiven the definition of $$P(A|B) = \\frac{PA\\cap B}{P(B)}.$$ This is defined only for when $P(B) > 0$. The numerator in the expression, $P(A \\cap B)$ is called the **joint probability** of $A$ and $B$, and may also be written as $P(A,B)$. Sometimes, we wish to condition on more than one event. To do so, the same process extends naturally. For instance, suppose we  want to know the probability of $A$ given $B$ and $C$. This would be written $$P(A|B,C) = \\frac{P(A\\cap B\\cap C)}{P(B \\cap C)} = \\frac{P(A,B,C)}{P(B,C)}.$$ Moving beyond two  events occurs in the expected way.\n\nConditional probability is a mechanism for capturing our knowledge of the world, and using that to update our sense of the uncertainties at play. For instance, suppose that we are interested in drawing a random card from a deck of $52$, and we want to know the probability that it is a heart. Without any additional knowledge, the probability of this event is $\\frac{1}{4}$. Now, suppose that you know that it is a red card. In this case, we now know that itis either a heart or a diamond, and there are equal numbes of each, meaning that the new probability is $0.5$. We can work this out directly $$P(\\text{Heart}|\\text{Red}) = \\frac{P(\\text{Heart},\\text{Red})}{P(\\text{Red})} = \\frac{P(\\text{Heart})}{1/2} = \\frac{1/4}{1/2} = 0.5.$$ Suppose instaead that wwe had been told that the card was an ace. Here we now know that there are four possible outcomes that correspond to an ace, and only one of these is a heart, meaning the probability is $\\frac{1}{4}$. In this case, $P(A|B) = P(A)$, and our beliefs did not update.\n\nWhat if instead we had considered the second event to be \"the card was a spade.\" In this case if we want to know $P(A|B)$ then, given a spade being drawn, we know that the probability of drawing a heart is  $0$. \n\nTODO: Add further examples of conditional probabilities. \n\nWhile sometimes we will want to work out the  conditional probability, knowing the joint and marginal probabilities, there are other times where it is easier to determine the conditional probability directly, but where  we  wish to understand the marginal probability. That is, we may be able to know $P(A|B)$, but we want ot make statements regarding $P(A)$ or $P(A,B)$.\n\nIn this case, we can simply rearrange the defining relationship of conditional probability, to solve for the quantities of interest. Because of the importance of this procedure, we actually give the most straightforward rearrangment a special name. Note that, by multiplying both sides of the definition of $P(A|B)$ by $P(B)$, wwe get that $$P(A\\cap B) = P(A|B)P(B).$$ This is known as the **multiplication rule**. In particular, it states that we can solve for the joint probability of $A$ and $B$ by multiplying the conditional probability of $A$ given $B$, by the marginal probability of $B$. Note that this is symmetric in $A$ and $B$ so that $$P(A\\cap B) = P(B|A)P(A).$$  This is useful as  sometimes it is easier to determine $B$ given  $A$. \n\nTODO: Example regarding the multiplication rule.\n\nWhile the multiplication rule gives us the capacity to solve for joint probabilities, often  we  wish to make statements regarding marginal probabilities. Fortunately, we can extend this process outlined in the multiplication rule to solve directly for marginal probabilities as well. To do so, we first introduce the concept of a **partition**.\n\nA **partition** is simply a collection of sets which divide up the sample space such that all of the sets are disjoint from one another, and there is no overlap between any of the sets. For instance, if the sample space were all the positive integers, we could partition this space into all the even numbers as one set and all the odd numbers as a second. We could also partition this into the set of numbers which are less than $10$, the set of numnbers that are greater than $10$, and then $10$. In both examples we have sets that form the full sample space with no overlap. In notation, if our partition is formed of $B_1, B_2, B_3, \\dots$, then we require $\\bigcup_i B_i = \\mathcal{S}$ and that $B_i \\cap B_\\ell = \\emptyset$. Note that we could not partition the set into multiples of $2$ and multiples of $3$, since (i) not all values are contained between these two sets, and (ii) there is overlap between these two sets. \n\nTODO: Include further partition examples\n\nGiven  a partition, $B_1, B_2, \\dots$, suppose that we can work out both $P(B_i)$ for all $i$, and $P(A|B_i)$ for all $i$. In this case we can work out the marginal probability of $A$ using **the law of total probability**. To do so we take $$P(A) = \\sum_i P(A|B_i)P(B_i).$$ Intuitively, since the whole sample space is divided into the different $B_i$s, this rule breaks  down the calculation of $A$ happening into manageable chunks. Each term in the summation is \"the probability that $A$ happens,  given $B_i$ happening\" weighted by how likely it is that $B_i$ happens. Then by summing over all possible $B_i$, we know that we must be capturing all possible ways that $A$ can occur since all parts of the sample space are contained in exactly one of the sets of our partition.\n\nThe law of total probability is  an  indispensible tool for computing probabilities in practice. Suppose that an online retailer uses two different services to send out their packages, the first one delivers later with probability $0.05$ and the second delivers late with probability $0.1$. If the chances that a customer selects the first supplier are $0.75$, then with this information we can work out the probability that any randomly selected package will be late.  First note that in this case our sample space is comprised of $\\mathcal{S} = \\{(L,A),(L,B),(O,A),(O,B)\\}$ where $L$ stands for late, $O$ for on time, $A$ for the first compnay,  and $B$ for the second. We want to know $P(L)$, where $L = \\{(L,A),(L,B)\\}$. Note that we can partition  the space into those sent by $A$ and those sent by $B$, and we know that $P(A) = 0.75$ and $P(B) = 0.25$. Moreover, we know that $P(L|A) = 0.05$ and $P(L|B) = 0.10$ and so combining this all together we get $$P(L) = P(L|A)P(A) + P(L|B)P(B) = (0.05)(0.75) + (0.10)(0.25) = 0.0625.$$\n\nTODO: Write another example on the utility of L.o.T.P.\n\nWe have seen the direct computation of marginal probabilities (while using an equally likely outcome model), the computation of conditional probabilities, the use of the multiplication rule for joint probabilities, and the use of the law of total probability to indirectly calculate marginal probabilities through conditioning arguments. Throughout these discussions we have been primarily concerned with keeping events $A$ and $B$ arbitrary. Everything that we have indicated for $P(A)$ holds for $P(B)$, as does $P(A|B)$ and $P(B|A)$. In reality, it will often be the case that conditioning on one of the events will be natural, while conditioning on  the other will be more tricky. In these events, it can be useful to be able to transform statements regarding $P(A|B)$ into statements regarding $P(B|A)$, and vice versa.\n\nNote that the symmetry of definitions gives the fact that $$P(A|B)P(B) = P(A,B) = P(B|A)P(A).$$ This is an application of the multiplication rule in two different orientations. If we divide both sides of the equality by $P(B)$, assuming that it is not $0$,  then we get $$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}.$$ Now, if we form a partition, say $A,A_2,A_3,\\cdots$, then we can rewrite $P(B)$ using the law of total probability as $$P(B) =  P(B|A)P(A) + P(B|A_2)P(A_2) + \\cdots,$$ which can replace $P(B)$ in the expression. Taken together, we refer to this relationship as **Bayes' Theorem**, and it is typically expressed as $$P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + \\sum_i P(B|A_i)P(A_i)}.$$\n\nBayes' Theorem allows us to convert statements regarding $P(B|A)$ into statements regarding $P(A|B)$. Note that, as we derived above, Bayes' Theorem is really just an application of the multiplication rule and an application of the law of total probability. Sometimes we may have $P(B)$ directly, rendering the law of total probability in the denominator unnecessary. Often, the natural  partition to select when we do need the law of total probability is to take $A,A^C$. Note that any set with its complement forms a partition, since by definition they occupy the entire space and are non-overlapping. When this is  done we get the slightly more compact relationship of $$P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^C)P(A^C)}.$$\n\nUnlike our previous relationships, Bayes' Theorem allows us to translate one set of conditional knowledge into another. The most common example application is in  medical testing. Suppose that we know the performance characteristics of a particular medical test:  it is $99\\%$ accurate for positive cases, and $95\\%$ accurate for negative cases. That is,  wit h probability $0.99$ it correctly returns positive when an individual is infected, and with probability $0.95$ it returns negative when an individual is not infected. These are both statements of conditional probability. If  we take $A$ to be the event that the test returns positive, and $B$ to be the event that the patient is infected, then we are saying that $P(A|B) = 0.99$ and $P(A^C|B^C) = 0.95$ which means that $P(A|B^C) = 0.05$. Suppose that we know that, across the entire population, one in a thousand individuals is likely to be infected. This means that $P(B) = 0.001$. Now suppose that a random individual goes into a doctor's appointment, and tests positive for the disease. How likely are they to actually be infected?\n\nNote that in  this case we want to know the probability of them being infected given that they have tested positive. In notation, this is $P(B|A)$. We do not know this quantity directly, but given a simple application of Bayes' Theorem, we can work it out. Here using the natural partition of $B,B^C$, we get $$P(B|A) = \\frac{P(A|B)P(A)}{P(A|B)P(B)+P(A|B^C)P(B^C)} = \\frac{(0.99)(0.001)}{(0.99)(0.001)+(0.05)(0.999)} \\approx 0.019.$$ That is, despite the fact that this test is exceptionally effective at detecting this disease, a positive test still means that an individual has a probability of only $0.019$ of actually having the illness. This counter intuitive fact was an intensely frustrating reality for statisticians everywhere during the height of the COVID-19 pandemic, when politicians and the population at large turned away from testing owing to its perceived ineffectiveness. \n\nTODO: Add another exmaple regarding Bayes' Theorem\n\nBayes' Theorem highlights  a key lesson  when considering conditional probabilities, and it's a common mistake to make which  should be avoided at all costs. Namely,  we cannot interchange  $P(A|B)$ with $P(B|A)$. These probabilities are not necessarily highly correlated  with one another, and it is important to distinguish clearly which is the probability of interest. Mixing up $P(B|A)$ and $P(A|B)$ is often called \"confusion of the inverse,\" and it can lead to very faulty conclusions when ignored. In the medical testing example above, it is important to not confuse \"the probability that the test returns positive, assuming you  have theillness\" with \"the probability that you have the illness, given that the test returns positive.\" The stakes of these types of confusion can be quite high, and it is tremendously important to ensure that you are conditioning on the correct events. Fortunately, Bayes' Theorem allows us to translate ebtween events for conditioning, giving a mecchanism from translating between the two.\n\nTODO: Include Another Example (perhaps Smoking vs. lung cancer)\n\nTODO: Fix Bayes' to Bayes'\n\nTODO: Add in subsections\n\n## Independence\nWe have seen that, in most cases,  when we have conditioned on an event it has  changed the probability of that event.  For instance, if we want to know the probability it is raining, if we condition on knowing that it is a day full of gray skies, then the probability likely increases. By considering how tehse probabilities change when we condition, we are in effect indicating a dependence of  the events on one another. In terms of probability, this dependence is captured by an influence on the degree of uncertainty present, depending on what we know.\n\nIt is of course, totally possible that two  events do not influence one another. The weeather outside today is likely not influenced by your favourite sports team's performance last night, for instance. In this case, we would be suggest that $P(A|B) = P(A)$. We saw an example where this was concerned previously when we wanted to know the probability of a randomly selected card being a heart ($A$), given  that it was  an  ace ($B$).  We found tat this was  $\\frac{1}{4}$, exactly the same as the probability if we did not know that it was an ace. Thus here we have $P(A|B)=P(A)$. Note that in this case,  we could have also  said that $P(B|A)=P(B)=\\frac{1}{13}$.\n\nThe symmetry of these events makes it somewhat more convenient to express this relationship differently.  If we multiply both sides of the $P(A|B)=P(A)$, by $P(B)$, then applying the multiplication rule gives  $$P(A,B) = P(A)P(B).$$ Any two events that satisfy this relationship are said to be **independent**. If $A$ and $B$ are independent we write $A\\perp B$. \n\nNote that independence is always a symmetric property: if $A$ is independent of $B$, then $B$ is independent of $A$. To check whether two evetns are independent, we simply need to check whether their joint probability (that is, the proobability of $A$ and $B$) is equal to the product of their marginal probabilities. If $P(A)\\neq 0$, then we can divide both sides by $P(A)$ to gives $P(B|A) = P(B)$. Similarly, if $P(B)\\neq 0$, then we can divide both sides by $P(B)$ to give $P(A|B)=P(A)$.\n\nThis expression in terms of conditional probabilities is the more intuitive expression of independence. It directly captures the idea that \"knowing $B$ does not change our beleif about $A$\", which is how independence is best thought of. However, we must be careful. This conditional argument is only valid when the event that is being conditioned on is not probability $0$, where the defining relationship, $P(A,B) = P(A)P(B)$, will hold for any events.\n\nTODO: Example assessing the independence of several events.\n\nWhen two events are not independent, we say that they are dependent, and may sometimes write $A\\not\\perp B$. Note that we saw that, in general, $P(A\\cap B) =  P(A)+P(B)-P(A\\cup B)$. It is only when assuming independence that this simplifies further. \n\nImportantly,  if $A\\perp B$, then $A\\cap B\\neq \\emptyset$ unless either $A=\\emptyset$ or $B=\\emptyset$ or both. To see this recall that $P(\\emptyset) = 0$, and so if $A\\cap B = \\emptyset$ then $P(A\\cap B) = P(A)P(B) = P(\\emptyset) = 0$. This only holds if either $P(A) = 0$ or $P(B) = 0$. This may seem to be a rather technical point, however, it is the source of much confusion regarding independence. In particular, it is common to mistake independent events for **mutually exclusive events**.\n\nSuppose that my partner and I always eat dinner together. It will either be the case that I cook at home, that they cook at home, that we order in, or that we go out to eat. If we take these to be four events, $A$, $B$, $C$, and $D$, then it reasonable to suggest that only one of these can happen on any given night. Whenever this is the case, we refer to the events as being mututally exclusive: if one happens, we know that the others did not. Mutually exclusive events are always dependent since knowing that $A$ occurs dramatically shifts our belief about $B$,$C$, and $D$ (namely, we now know that they are impossible).\n\nThe primary concern with mututally exclusive and independent events is a linguistic one. We often use words like independent to mean unrelated, and in a sense, mututally exclusive events are unrelated in that one has nothing to do with another. However, in statistics and probability, when we discuss independence, it is not an independence of the events themselves but rather an independence relating to our beliefs regarding the uncertainty associated with the events. In this sense, mutually exclusive events are very informative regarding the uncertainty associated with them.\n\nTODO: Write example regarding mututally exclusive and independent events.\n\n## Contingency Tables\nThrough to this point we have discussed probabilities in the abstract, either through an  enumeration of equally likely outcomes, or else by directly specifying the likelihood of various events. While these are useful in many regards, we are often looking for more concise manners of summarizing information of interest. One tool for accomplishing this is a **contingency table**. A contingency table summarizes teh frequency of occurence of a variable (or multiple variables) across a population of interest.\n\nFor instance, you could form a contingency table relaying the frequency with which undergraduate students are enrolled in various faculties at a particular university. This tells you, of the whole population of students at the university, what is the faculty breakdown. By dividing the number in each faculty by the total number of students, you  convert the frequencies to proportions,  and these proportions can be viewed as probabilities.\n\nTODO: write-up frequency and prop table.\n\nThe interpretation of proportions as probabilities implies a very specific statistical experiment. In particular, the proportion represents the probability that an individual selected at random from the entire population has the given trait. This is frequently a probability of interest, which makes these summary tables a useful tool. Often, when a single trait is displayed we will not refer to them as contingency tables but rather as frequency tables or frequency distributions.  A contingency table will typically plot two or more traits on the same table, with each cell representing the frequency of both traits occurring simultaneously in the population. For instance, we may further include the student's years to see the breakdown of both faculty and year of study, in one table.\n\nTODO: Include updated table\n\nBy including two (or more) factors on the table we are able to capture not only the marginal probabilities for the population, but also the joint probabilities for the population,  and in turn, the conditional probabilities for the population. Being able to concisely summarize all of these concepts regarding traits in a population of interest renders contingency tables immensely useful in the study  of uncertainty broadly.\n\nTODO: Include example(s) of contingency tables.\n\nSuppose we consider a two-way contingency table. Each cell  consists of frequnecy with which a combination of two traits occurs in the population. If we take events corresponding to each of the levels of the two varaibles of interest, then these central cells represent the frequncy of joint events. That is, each interior cell gives the total number of observations with a set level for variable one AND a set level for variable two. Each row is summed, with the total number following into the corresponding row recorded in the right hand margin.  each column is summed, with the total number corresponding to the given column recorded in the bottom margin. Then the margin totals are summed and the total is recorded in the lower right margin space. \n\nTODO: Include graphic with summation of margins\n\nWhether the rows or columns are summed, they should sum to the same total, which is the total of the population under consideeartion. This is the same as  simply adding all of the observed interior frequencies. To turn a frequency into a probability, you need only divide the correct frquency by the correct total.\n\nFor the standard joint probabilities, you take the interior cell count and divide by the population total. Here we are saying that some fixed number, $m$, of the $N$ total individuals have both traits under consideration. If instead you wish to find a marginal probability, you have to consider the value in the corresponding margin: this is the total number of individuals with the given trait, ignoring the level of the other variable. These marginal values are also divided by the total population size. For conditional probbailities, we restrict our focus to either only one row, or one column. Then, we can take teh joint cell  and divide by the value in the margin, which gives the conditional probability of interest.\n\nTODO: show probability caluclations for the various scenarios that we saw.\n\nNote that these procedures are exactl in line with what we had seen before. The conditional probability is defined as the joint probability divided by the marginal probability. The process of computing the marginal can be seen as an application of the law of total probability. As a result, contingency tables can be a useful, tangible tool for investigating the techniques we have been discussing: they are not a substitutefor direct manipulation of the mathematical objedts, but they can present insight into the underlying processes where it may be hard to derive that insight otherwise.\n\nTODO: show example of conditional probability derivation.\n\nTODO: show example of law of total probabilitty derivation.\n\nIt is important to note that there is redundant information within a contingency table. For instance, the margins need not be listed explicit, as they can be directly  calculated from the interior points. Same goes for interior points,  given the margins of the table (assuming other interior points are also presented). TThis can be useful for a campact representation of the information, and manipulating these tables, finding the required information in many places, should become second anture as you continue to work with them more and more.\n\nTODO: include fill-in-the-blank contingency table\n\nIt is also important to recognize that independence and mutually exclusive events can be codified via the table as  well. Zeros on the  interior points indicate events which are mutually exclusive: if we know that one of them occurred, we also know that the other one did not. For independence, it requires a degree of solving proportions. We can  either check that the joint probability ($N_{A,B}/N$) is equal to the product of the two marginal probabilities, ($N_AN_B/N^2$), or else (assuming that the events are all non-zero), that the conditional probability ($N_{A,B}/N_A$) equals the marginal probability $N_B/N$. Either way this is represented by $N\\times N_{A,B} = N_AN_B$, and when this holds, we can  conclude that the events are independent.\n\nTODO: Example on mututally exclusive / independent events on a contingency table.","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["webr"],"output-file":"chapter3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.533","bibliography":["../references.bib"],"theme":["cosmo","../custom.scss"],"code-annotations":"select"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","filters":["webr"],"output-file":"chapter3.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["../references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}